{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e66885f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.conda\\lib\\site-packages\\gcsfs\\core.py:313: UserWarning: GCS project not set - cannot list or create buckets\n",
      "  warnings.warn(\"GCS project not set - cannot list or create buckets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains time from 2022-01-01T00:00:00.000000000 to 2022-12-31T18:00:00.000000000\n",
      "Global 2023 0.25 degree ZArr → ERA5_2022_6h_5VAR_0.25.zarr\n",
      "Global 2023 0.25 degree NetCDF4 → ERA5_2022_6h_5VAR_0.25.nc\n",
      "Global 2023 1.0 degree ZArr → ERA5_2022_6h_5VAR_1.0.zarr\n",
      "Global 2023 1.0 degree NetCDF4 → ERA5_2022_6h_5VAR_1.0.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import gcsfs\n",
    "import numpy as np\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "GCS_URI_6H_13L = \"gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr\"\n",
    "OUT_6H_13L_ZARR = \"ERA5_2020-2022_6h_5VAR_0.25.zarr\"\n",
    "OUT_6H_13L_NC = \"ERA5_2020-2022_6h_5VAR_0.25.nc\"\n",
    "OUT_6H_13L_1deg_ZARR = \"ERA5_2020-2022_6h_5VAR_1.0.zarr\"\n",
    "OUT_6H_13L_1deg_NC = \"ERA5_2020-2022_6h_5VAR_1.0.nc\"\n",
    "\n",
    "# set start and end dates for data range. yyyy-mm-dd\n",
    "start = \"2020-01-01\"\n",
    "end = \"2022-12-31\"\n",
    "\n",
    "VARS = [\n",
    "    \"2m_temperature\",\n",
    "    \"mean_sea_level_pressure\",\n",
    "    \"10m_u_component_of_wind\",\n",
    "    \"10m_v_component_of_wind\",\n",
    "    \"total_precipitation_6hr\"\n",
    "]\n",
    "\n",
    "def open_gcs_zarr(uri: str, project: str = None):\n",
    "    fs = gcsfs.GCSFileSystem(project=project)\n",
    "    return xr.open_zarr(fs.get_mapper(uri), consolidated=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1) load & time‐subset 6h/13l for 2020-2022\n",
    "ds = open_gcs_zarr(GCS_URI_6H_13L)\n",
    "ds2020_22 = ds.sel(time=slice(start, end))\n",
    "\n",
    "# 1b) keep only the five surface variables\n",
    "ds2020_22 = ds2020_22[VARS]\n",
    "\n",
    "times = ds2020_22.time.values\n",
    "start = np.min(times)\n",
    "end   = np.max(times)\n",
    "print(f\"Dataset contains time from {start} to {end}\")\n",
    "\n",
    "# use the actual coord names in your ds:\n",
    "lon_coord = \"longitude\"    if \"longitude\"    in ds2020_22.coords else \"lon\"\n",
    "lat_coord = \"latitude\"     if \"latitude\"     in ds2020_22.coords else \"lat\"\n",
    "\n",
    "# only save US subset\n",
    "us_lon_min, us_lon_max = 235.0, 294.0   # [0–360] U.S. longitudes\n",
    "us_lat_min, us_lat_max =  24.0,  50.0   # U.S. latitudes\n",
    "ds2020_22 = ds2020_22.assign_coords(lon=((ds.lon + 360) % 360)).sortby(\"lon\").sortby(\"lat\").sel(lon=slice(us_lon_min, us_lon_max),lat=slice(us_lat_min, us_lat_max))\n",
    "\n",
    "\n",
    "\n",
    "# 2) write out the U.S. subsets\n",
    "\n",
    "ds2020_22.to_zarr(OUT_6H_13L_ZARR, mode=\"w\")\n",
    "print(\"US 2020-2022 0.25 degree ZArr →\", OUT_6H_13L_ZARR)\n",
    "ds2020_22.to_netcdf(OUT_6H_13L_NC)\n",
    "print(\"US 2020-2022 0.25 degree NetCDF4 →\", OUT_6H_13L_NC)\n",
    "\n",
    "ds2020_22_1deg = ds2020_22.coarsen(\n",
    "    {lat_coord: 4, lon_coord: 4},\n",
    "    boundary=\"trim\").mean()\n",
    "\n",
    "\n",
    "ds2020_22_1deg.to_zarr(OUT_6H_13L_1deg_ZARR, mode=\"w\")\n",
    "print(\"US 2020-2022 1.0 degree ZArr →\", OUT_6H_13L_1deg_ZARR)\n",
    "ds2020_22_1deg.to_netcdf(OUT_6H_13L_1deg_NC)\n",
    "print(\"US 2020-2022 1.0 degree NetCDF4 →\", OUT_6H_13L_1deg_NC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf6835",
   "metadata": {},
   "source": [
    "Then gather arrival data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a107b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ERA5 DATASET SUMMARY ===\n",
      "<xarray.Dataset> Size: 2GB\n",
      "Dimensions:                  (time: 1460, latitude: 180, longitude: 360)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 720B 89.62 88.62 ... -89.38\n",
      "  * longitude                (longitude) float32 1kB 0.375 1.375 ... 358.4 359.4\n",
      "  * time                     (time) datetime64[ns] 12kB 2022-01-01 ... 2022-1...\n",
      "Data variables:\n",
      "    10m_u_component_of_wind  (time, latitude, longitude) float32 378MB dask.array<chunksize=(1, 180, 360), meta=np.ndarray>\n",
      "    10m_v_component_of_wind  (time, latitude, longitude) float32 378MB dask.array<chunksize=(1, 180, 360), meta=np.ndarray>\n",
      "    2m_temperature           (time, latitude, longitude) float32 378MB dask.array<chunksize=(1, 180, 360), meta=np.ndarray>\n",
      "    mean_sea_level_pressure  (time, latitude, longitude) float32 378MB dask.array<chunksize=(1, 180, 360), meta=np.ndarray>\n",
      "    total_precipitation_6hr  (time, latitude, longitude) float32 378MB dask.array<chunksize=(1, 180, 360), meta=np.ndarray>\n",
      "\n",
      "Variables in ds.data_vars:\n",
      " • 10m_u_component_of_wind        dims=('time', 'latitude', 'longitude')   shape=(1460, 180, 360)\n",
      " • 10m_v_component_of_wind        dims=('time', 'latitude', 'longitude')   shape=(1460, 180, 360)\n",
      " • 2m_temperature                 dims=('time', 'latitude', 'longitude')   shape=(1460, 180, 360)\n",
      " • mean_sea_level_pressure        dims=('time', 'latitude', 'longitude')   shape=(1460, 180, 360)\n",
      " • total_precipitation_6hr        dims=('time', 'latitude', 'longitude')   shape=(1460, 180, 360)\n",
      "\n",
      "Coordinates in ds.coords:\n",
      " • latitude = [89.625 88.625 87.625 86.625 85.625] …\n",
      " • longitude = [0.375 1.375 2.375 3.375 4.375] …\n",
      " • time = ['2022-01-01T00:00:00.000000000' '2022-01-01T06:00:00.000000000'\n",
      " '2022-01-01T12:00:00.000000000' '2022-01-01T18:00:00.000000000'\n",
      " '2022-01-02T00:00:00.000000000'] …\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'Datasets/Arrival_With_Weather/2022'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m ARRIVAL_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasets/Arrival_With_Weather/2022\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# pick the first CSV in the folder\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mARRIVAL_DIR\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     23\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ARRIVAL_DIR, fn), parse_dates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlightDate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== SAMPLE ARRIVAL CSV:\u001b[39m\u001b[38;5;124m\"\u001b[39m, fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Datasets/Arrival_With_Weather/2022'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# ─── 1) Inspect your ERA5 dataset ──────────────────────────────────────────────\n",
    "ERA5_STORE = \"ERA5_2020-2022_6h_5VAR_1.0.zarr\"\n",
    "ds = xr.open_zarr(ERA5_STORE)\n",
    "\n",
    "print(\"=== ERA5 DATASET SUMMARY ===\")\n",
    "print(ds)                        # full summary: dims, coords, data_vars\n",
    "print(\"\\nVariables in ds.data_vars:\")\n",
    "for name, var in ds.data_vars.items():\n",
    "    print(f\" • {name:30s} dims={var.dims}   shape={tuple(var.shape)}\")\n",
    "\n",
    "print(\"\\nCoordinates in ds.coords:\")\n",
    "for coord in ds.coords:\n",
    "    print(\" •\", coord, \"=\", ds.coords[coord].values[:5], \"…\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf520d4",
   "metadata": {},
   "source": [
    "augment arrival data with weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e20363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dims before subsetting: FrozenMappingWarningOnValuesAccess({'time': 1460, 'lat': 721, 'lon': 1440})\n",
      "dims after subsetting: FrozenMappingWarningOnValuesAccess({'time': 1460, 'lat': 105, 'lon': 237})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "#!pip install pyproj\n",
    "from pyproj import geod\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "ERA5_STORE   = \"ERA5_2020-2022_6h_5VAR_0.25.zarr\"\n",
    "MASTER_COORD = \"Datasets/T_MASTER_CORD.csv\"\n",
    "ARRIVAL_DIR  = \"Datasets/Arrival_Statistics/2020-2022\"\n",
    "OUTPUT_DIR   = \"Datasets/Arrival_With_Weather/2020-2022\"\n",
    "\n",
    "VARS = [\n",
    "    \"2m_temperature\",\n",
    "    \"mean_sea_level_pressure\",\n",
    "    \"10m_u_component_of_wind\",\n",
    "    \"10m_v_component_of_wind\",\n",
    "    \"total_precipitation_6hr\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ─── 1) load ERA5 & rename dims ───────────────────────────────────────────────\n",
    "ds = xr.open_zarr(ERA5_STORE, consolidated=True)\n",
    "\n",
    "if \"latitude\" in ds.dims and \"longitude\" in ds.dims:\n",
    "    ds = ds.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n",
    "\n",
    "print(\"dims before subsetting:\", ds.dims)\n",
    "\n",
    "# ─── 1b) wrap & sort lon/lat, then slice just over the U.S. ────────────────\n",
    "# Only do this slice if flight data contains only Domestic U.S. Flights and weather data contains the world.\n",
    "#us_lon_min, us_lon_max = 235.0, 294.0   # [0–360] U.S. longitudes\n",
    "#us_lat_min, us_lat_max =  24.0,  50.0   # U.S. latitudes\n",
    "\n",
    "#ds = (ds.assign_coords(lon=((ds.lon + 360) % 360)).sortby(\"lon\").sortby(\"lat\").sel(lon=slice(us_lon_min, us_lon_max),lat=slice(us_lat_min, us_lat_max)))\n",
    "#print(\"dims after subsetting:\", ds.dims)\n",
    "\n",
    "# ─── 2) build your fast lookup arrays ────────────────────────────────────────\n",
    "time_index = ds[\"time\"].to_index()\n",
    "lat_vals    = ds[\"lat\"].values\n",
    "lon_vals    = ds[\"lon\"].values\n",
    "var_arrays = {var: ds[var].data.compute()  for var in VARS}\n",
    "\n",
    "\n",
    "# ─── 3) load master coords ───────────────────────────────────────────────────\n",
    "mc = (\n",
    "    pd.read_csv(MASTER_COORD, dtype=str)\n",
    "      .set_index(\"AIRPORT_SEQ_ID\")[[\"LATITUDE\",\"LONGITUDE\"]]\n",
    "      .astype(float)\n",
    ")\n",
    "\n",
    "# ─── 4) helpers ─────────────────────────────────────────────\n",
    "def parse_hhmm(x):\n",
    "    if pd.isna(x) or not str(x).strip():\n",
    "        return pd.NaT\n",
    "    s = str(int(float(x))).zfill(4)\n",
    "    return pd.Timedelta(hours=int(s[:2]), minutes=int(s[2:]))\n",
    "\n",
    "\n",
    "\n",
    "# ─── then your make_weather_lookup becomes ─────────────────────────────────\n",
    "def make_weather_lookup(keys: pd.DataFrame) -> pd.DataFrame:\n",
    "    sub = keys.copy()\n",
    "    sub[\"lat\"] = sub[\"AirportSeqID\"].map(mc[\"LATITUDE\"])\n",
    "    sub[\"lon\"] = sub[\"AirportSeqID\"].map(mc[\"LONGITUDE\"]) % 360.0\n",
    "    sub = sub.dropna(subset=[\"Datetime\",\"lat\",\"lon\"])\n",
    "    if sub.empty:\n",
    "        return pd.DataFrame(columns=VARS,\n",
    "                            index=pd.MultiIndex.from_arrays([[],[]],\n",
    "                                                           names=[\"AirportSeqID\",\"Datetime\"]))\n",
    "\n",
    "    # 1) nearest‐time\n",
    "    t_idx = time_index.get_indexer(sub[\"Datetime\"], method=\"nearest\")\n",
    "\n",
    "    # 2) nearest‐lat/lon\n",
    "    sub_lat = sub[\"lat\"].to_numpy()\n",
    "    sub_lon = sub[\"lon\"].to_numpy()\n",
    "    l_idx = np.abs(lat_vals[None,:] - sub_lat[:,None]).argmin(axis=1)\n",
    "    o_idx = np.abs(lon_vals[None,:] - sub_lon[:,None]).argmin(axis=1)\n",
    "\n",
    "    # 3) pull out each VAR from our preloaded var_arrays\n",
    "    out = {}\n",
    "    for var in VARS:\n",
    "        arr = var_arrays[var]   # pure numpy now\n",
    "        out[var] = arr[t_idx, l_idx, o_idx]\n",
    "\n",
    "    # 4) assemble a DataFrame and re‑index\n",
    "    df_lkp = pd.DataFrame(out, index=sub.index)\n",
    "    df_lkp.index = pd.MultiIndex.from_frame(\n",
    "        sub[[\"AirportSeqID\",\"Datetime\"]],\n",
    "        names=[\"AirportSeqID\",\"Datetime\"]\n",
    "    )\n",
    "    return df_lkp\n",
    "\n",
    "\n",
    "def make_route_lookup(df: pd.DataFrame, spacing_miles=100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each flight (row of df), sample points every ~spacing_miles along the\n",
    "    great‐circle from origin→dest at the departure time. Then compute\n",
    "    mean/min/max for each VAR along that route.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for idx, row in df.iterrows():\n",
    "        orig = row[\"OriginAirportSeqID\"]\n",
    "        dest = row[\"DestAirportSeqID\"]\n",
    "        t0   = row[\"DepDatetime\"]\n",
    "\n",
    "        lat0, lon0 = mc.loc[orig]\n",
    "        lat1, lon1 = mc.loc[dest]\n",
    "        # total distance in meters:\n",
    "        _,_,dist_m = geod.inv(lon0, lat0, lon1, lat1)\n",
    "        # number of segments\n",
    "        nseg = max(1, int(dist_m / (spacing_miles*1609.34)))\n",
    "        # intermediate points (excluding endpoints)\n",
    "        pts = geod.npts(lon0, lat0, lon1, lat1, nseg-1)\n",
    "        # build full list including endpoints\n",
    "        all_pts = [(lat0,lon0)] + [(lat,lon) for lon,lat in pts] + [(lat1,lon1)]\n",
    "\n",
    "        t_idx = time_index.get_indexer([t0], method=\"nearest\")[0]\n",
    "        vals = {var: [] for var in VARS}\n",
    "\n",
    "        for lat, lon in all_pts:\n",
    "            # nearest grid\n",
    "            i_lat = np.abs(lat_vals - lat).argmin()\n",
    "            i_lon = np.abs(lon_vals - (lon%360)).argmin()\n",
    "            for var in VARS:\n",
    "                vals[var].append(var_arrays[var][t_idx, i_lat, i_lon])\n",
    "\n",
    "        # compute summaries\n",
    "        rec = {\"AirportSeqID\": orig, \"Datetime\": t0}\n",
    "        for var, arr in vals.items():\n",
    "            rec[f\"Route_{var}_mean\"] = np.nanmean(arr)\n",
    "            rec[f\"Route_{var}_min\"]  = np.nanmin(arr)\n",
    "            rec[f\"Route_{var}_max\"]  = np.nanmax(arr)\n",
    "        records.append(rec)\n",
    "\n",
    "    if not records:\n",
    "        return pd.DataFrame([], columns=[f\"Route_{v}_{s}\" for v in VARS for s in (\"mean\",\"min\",\"max\")],\n",
    "                            index=pd.MultiIndex(levels=[[],[]], codes=[[],[]],\n",
    "                                                names=[\"AirportSeqID\",\"Datetime\"]))\n",
    "\n",
    "    df_route = pd.DataFrame(records)\n",
    "    idx = pd.MultiIndex.from_frame(df_route[[\"AirportSeqID\",\"Datetime\"]],\n",
    "                                   names=[\"AirportSeqID\",\"Datetime\"])\n",
    "    df_route.index = idx\n",
    "    return df_route.drop(columns=[\"AirportSeqID\",\"Datetime\"])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "657fd9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_32956\\3010907270.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_1.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_32956\\3010907270.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_10.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_11.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_32956\\3010907270.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_11.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_32956\\3010907270.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_12.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_32956\\3010907270.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_2.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_32956\\3010907270.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_3.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_32956\\3010907270.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_4.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_32956\\3010907270.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_5.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_32956\\3010907270.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_6.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_32956\\3010907270.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_7.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_32956\\3010907270.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_8.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_9.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_32956\\3010907270.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_9.csv\n"
     ]
    }
   ],
   "source": [
    "for fn in sorted(os.listdir(ARRIVAL_DIR)):\n",
    "    if not fn.lower().endswith(\".csv\"):\n",
    "        continue\n",
    "    print(\"→ augmenting\", fn)\n",
    "    df = pd.read_csv(os.path.join(ARRIVAL_DIR, fn), dtype=str)\n",
    "\n",
    "    # build timestamps\n",
    "    df[\"FlightDate\"]   = pd.to_datetime(df[\"FlightDate\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "    df[\"DepDelta\"]     = df[\"DepTime\"].apply(parse_hhmm)\n",
    "    df[\"ArrDelta\"]     = df[\"ArrTime\"].apply(parse_hhmm)\n",
    "    df[\"DepDatetime\"]  = df[\"FlightDate\"] + df[\"DepDelta\"]\n",
    "    df[\"ArrDatetime\"]  = df[\"FlightDate\"] + df[\"ArrDelta\"]\n",
    "\n",
    "    # origin & dest lookups\n",
    "    orig = ( df[[\"OriginAirportSeqID\",\"DepDatetime\"]]\n",
    "             .dropna().drop_duplicates()\n",
    "             .rename(columns={\"OriginAirportSeqID\":\"AirportSeqID\",\"DepDatetime\":\"Datetime\"}) )\n",
    "    dest = ( df[[\"DestAirportSeqID\",\"ArrDatetime\"]]\n",
    "             .dropna().drop_duplicates()\n",
    "             .rename(columns={\"DestAirportSeqID\":\"AirportSeqID\",\"ArrDatetime\":\"Datetime\"}) )\n",
    "\n",
    "    orig_lkp  = make_weather_lookup(orig).rename(columns=lambda c: f\"Origin_{c}\")\n",
    "    dest_lkp  = make_weather_lookup(dest).rename(columns=lambda c: f\"Dest_{c}\")\n",
    "\n",
    "    # route lookups\n",
    "    #route_df  = df[[\"OriginAirportSeqID\",\"DepDatetime\"]].dropna().drop_duplicates()\n",
    "    #route_df  = route_df.rename(columns={\"OriginAirportSeqID\":\"AirportSeqID\",\"DepDatetime\":\"Datetime\"})\n",
    "    #route_lkp = make_route_lookup(route_df, spacing_miles=100)\n",
    "\n",
    "    # merge everything back\n",
    "    df = (df\n",
    "          .merge(orig_lkp,  left_on=[\"OriginAirportSeqID\",\"DepDatetime\"], right_index=True, how=\"left\")\n",
    "          .merge(dest_lkp,  left_on=[\"DestAirportSeqID\",\"ArrDatetime\"],   right_index=True, how=\"left\")\n",
    "          #.merge(route_lkp, left_on=[\"OriginAirportSeqID\",\"DepDatetime\"], right_index=True, how=\"left\")\n",
    "        )\n",
    "\n",
    "    # cleanup & save\n",
    "    df.drop(columns=[\"DepDelta\",\"ArrDelta\",\"DepDatetime\",\"ArrDatetime\"], errors=\"ignore\", inplace=True)\n",
    "    df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "    df.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "    out = os.path.join(OUTPUT_DIR, fn)\n",
    "    df.to_csv(out, index=False)\n",
    "    print(\"   saved →\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a834100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 2) Inspect one of your cleaned arrivals CSVs ──────────────────────────────\n",
    "ARRIVAL_DIR = \"Datasets/Arrival_With_Weather/2020-2022\"\n",
    "# pick the first CSV in the folder\n",
    "fn = sorted([f for f in os.listdir(ARRIVAL_DIR) if f.lower().endswith(\".csv\")])[0]\n",
    "df = pd.read_csv(os.path.join(ARRIVAL_DIR, fn), parse_dates=[\"FlightDate\"])\n",
    "\n",
    "print(\"\\n=== SAMPLE ARRIVAL CSV:\", fn, \"===\\n\")\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f535d20",
   "metadata": {},
   "source": [
    "Now run preprocessing to merge and format the arrival data for training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
