{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e66885f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global 2023 → ERA5_2023_6h_13lvl_1440x721.zarr\n",
      "Global 2023 → ERA5_2023_6h_13lvl_1440x721.nc\n",
      "  U.S. 2023 → ERA5_2023_6h_13l_US.zarr\n",
      "  U.S. 2023 → ERA5_2023_6h_13l_US.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import gcsfs\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "GCS_URI_6H_13L       = \"gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr\"\n",
    "OUT_6H_13L_ZARR      = \"ERA5_2023_6h_13lvl_1440x721.zarr\"\n",
    "OUT_6H_13L_NC        = \"ERA5_2023_6h_13lvl_1440x721.nc\"\n",
    "OUT_6H_13L_US_ZARR   = \"ERA5_2023_6h_13l_US.zarr\"\n",
    "OUT_6H_13L_US_NC     = \"ERA5_2023_6h_13l_US.nc\"\n",
    "\n",
    "def open_gcs_zarr(uri: str, project: str = None):\n",
    "    fs = gcsfs.GCSFileSystem(project=project)\n",
    "    return xr.open_zarr(fs.get_mapper(uri), consolidated=True)\n",
    "\n",
    "# 1) load & time‐subset 6h/13l for 2023\n",
    "ds = open_gcs_zarr(GCS_URI_6H_13L)\n",
    "ds2023 = ds.sel(time=slice(\"2023-01-01\",\"2023-12-31\"))\n",
    "\n",
    "# ─── spatially subset to continental U.S. ────────────────────────────────────\n",
    "# your lon runs 0→360\n",
    "us_lon_min, us_lon_max = -125 + 360, -66 + 360   # → 235, 294\n",
    "us_lat_min, us_lat_max = 24.0, 50.0              # 24N → 50N\n",
    "\n",
    "# use the actual coord names in your ds:\n",
    "lon_coord = \"longitude\"    if \"longitude\"    in ds2023.coords else \"lon\"\n",
    "lat_coord = \"latitude\"     if \"latitude\"     in ds2023.coords else \"lat\"\n",
    "\n",
    "ds2023_us = ds2023.sel(\n",
    "    { lon_coord: slice(us_lon_min, us_lon_max),\n",
    "      lat_coord: slice(us_lat_min, us_lat_max) }\n",
    ")\n",
    "\n",
    "# 2) write out both the global 2023 slice and the U.S. subset\n",
    "ds2023 .to_zarr(OUT_6H_13L_ZARR,    mode=\"w\")\n",
    "ds2023 .to_netcdf(OUT_6H_13L_NC)\n",
    "ds2023_us.to_zarr(OUT_6H_13L_US_ZARR, mode=\"w\")\n",
    "ds2023_us.to_netcdf(OUT_6H_13L_US_NC)\n",
    "\n",
    "print(\"Global 2023 →\", OUT_6H_13L_ZARR)\n",
    "print(\"Global 2023 →\", OUT_6H_13L_NC)\n",
    "print(\"  U.S. 2023 →\", OUT_6H_13L_US_ZARR)\n",
    "print(\"  U.S. 2023 →\", OUT_6H_13L_US_NC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4a107b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ERA5 DATASET SUMMARY ===\n",
      "<xarray.Dataset> Size: 38GB\n",
      "Dimensions:                                           (time: 40, latitude: 721,\n",
      "                                                       longitude: 1440,\n",
      "                                                       level: 13)\n",
      "Coordinates:\n",
      "  * latitude                                          (latitude) float32 3kB ...\n",
      "  * level                                             (level) int64 104B 50 ....\n",
      "  * longitude                                         (longitude) float32 6kB ...\n",
      "  * time                                              (time) datetime64[ns] 320B ...\n",
      "Data variables: (12/62)\n",
      "    10m_u_component_of_wind                           (time, latitude, longitude) float32 166MB dask.array<chunksize=(1, 721, 1440), meta=np.ndarray>\n",
      "    10m_v_component_of_wind                           (time, latitude, longitude) float32 166MB dask.array<chunksize=(1, 721, 1440), meta=np.ndarray>\n",
      "    10m_wind_speed                                    (time, latitude, longitude) float32 166MB dask.array<chunksize=(1, 721, 1440), meta=np.ndarray>\n",
      "    2m_dewpoint_temperature                           (time, latitude, longitude) float32 166MB dask.array<chunksize=(1, 721, 1440), meta=np.ndarray>\n",
      "    2m_temperature                                    (time, latitude, longitude) float32 166MB dask.array<chunksize=(1, 721, 1440), meta=np.ndarray>\n",
      "    above_ground                                      (time, level, latitude, longitude) float32 2GB dask.array<chunksize=(1, 13, 721, 1440), meta=np.ndarray>\n",
      "    ...                                                ...\n",
      "    volumetric_soil_water_layer_1                     (time, latitude, longitude) float32 166MB dask.array<chunksize=(1, 721, 1440), meta=np.ndarray>\n",
      "    volumetric_soil_water_layer_2                     (time, latitude, longitude) float32 166MB dask.array<chunksize=(1, 721, 1440), meta=np.ndarray>\n",
      "    volumetric_soil_water_layer_3                     (time, latitude, longitude) float32 166MB dask.array<chunksize=(1, 721, 1440), meta=np.ndarray>\n",
      "    volumetric_soil_water_layer_4                     (time, latitude, longitude) float32 166MB dask.array<chunksize=(1, 721, 1440), meta=np.ndarray>\n",
      "    vorticity                                         (time, level, latitude, longitude) float32 2GB dask.array<chunksize=(1, 13, 721, 1440), meta=np.ndarray>\n",
      "    wind_speed                                        (time, level, latitude, longitude) float32 2GB dask.array<chunksize=(1, 13, 721, 1440), meta=np.ndarray>\n",
      "\n",
      "Variables in ds.data_vars:\n",
      " • 10m_u_component_of_wind        dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • 10m_v_component_of_wind        dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • 10m_wind_speed                 dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • 2m_dewpoint_temperature        dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • 2m_temperature                 dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • above_ground                   dims=('time', 'level', 'latitude', 'longitude')   shape=(40, 13, 721, 1440)\n",
      " • ageostrophic_wind_speed        dims=('time', 'level', 'latitude', 'longitude')   shape=(40, 13, 721, 1440)\n",
      " • angle_of_sub_gridscale_orography dims=('latitude', 'longitude')   shape=(721, 1440)\n",
      " • anisotropy_of_sub_gridscale_orography dims=('latitude', 'longitude')   shape=(721, 1440)\n",
      " • boundary_layer_height          dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • divergence                     dims=('time', 'level', 'latitude', 'longitude')   shape=(40, 13, 721, 1440)\n",
      " • eddy_kinetic_energy            dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • geopotential                   dims=('time', 'level', 'latitude', 'longitude')   shape=(40, 13, 721, 1440)\n",
      " • geopotential_at_surface        dims=('latitude', 'longitude')   shape=(721, 1440)\n",
      " • geostrophic_wind_speed         dims=('time', 'level', 'latitude', 'longitude')   shape=(40, 13, 721, 1440)\n",
      " • high_vegetation_cover          dims=('latitude', 'longitude')   shape=(721, 1440)\n",
      " • integrated_vapor_transport     dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • lake_cover                     dims=('latitude', 'longitude')   shape=(721, 1440)\n",
      " • land_sea_mask                  dims=('latitude', 'longitude')   shape=(721, 1440)\n",
      " • lapse_rate                     dims=('time', 'level', 'latitude', 'longitude')   shape=(40, 13, 721, 1440)\n",
      " • leaf_area_index_high_vegetation dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • leaf_area_index_low_vegetation dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • low_vegetation_cover           dims=('latitude', 'longitude')   shape=(721, 1440)\n",
      " • mean_sea_level_pressure        dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • mean_surface_latent_heat_flux  dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • mean_surface_net_long_wave_radiation_flux dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • mean_surface_net_short_wave_radiation_flux dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • mean_surface_sensible_heat_flux dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • mean_top_downward_short_wave_radiation_flux dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • mean_top_net_long_wave_radiation_flux dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • mean_top_net_short_wave_radiation_flux dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • mean_vertically_integrated_moisture_divergence dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • potential_vorticity            dims=('time', 'level', 'latitude', 'longitude')   shape=(40, 13, 721, 1440)\n",
      " • relative_humidity              dims=('time', 'level', 'latitude', 'longitude')   shape=(40, 13, 721, 1440)\n",
      " • sea_ice_cover                  dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • sea_surface_temperature        dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • slope_of_sub_gridscale_orography dims=('latitude', 'longitude')   shape=(721, 1440)\n",
      " • snow_depth                     dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • soil_type                      dims=('latitude', 'longitude')   shape=(721, 1440)\n",
      " • specific_humidity              dims=('time', 'level', 'latitude', 'longitude')   shape=(40, 13, 721, 1440)\n",
      " • standard_deviation_of_filtered_subgrid_orography dims=('latitude', 'longitude')   shape=(721, 1440)\n",
      " • standard_deviation_of_orography dims=('latitude', 'longitude')   shape=(721, 1440)\n",
      " • surface_pressure               dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • temperature                    dims=('time', 'level', 'latitude', 'longitude')   shape=(40, 13, 721, 1440)\n",
      " • total_cloud_cover              dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • total_column_vapor             dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • total_column_water             dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • total_column_water_vapour      dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • total_precipitation_12hr       dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • total_precipitation_24hr       dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • total_precipitation_6hr        dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • type_of_high_vegetation        dims=('latitude', 'longitude')   shape=(721, 1440)\n",
      " • type_of_low_vegetation         dims=('latitude', 'longitude')   shape=(721, 1440)\n",
      " • u_component_of_wind            dims=('time', 'level', 'latitude', 'longitude')   shape=(40, 13, 721, 1440)\n",
      " • v_component_of_wind            dims=('time', 'level', 'latitude', 'longitude')   shape=(40, 13, 721, 1440)\n",
      " • vertical_velocity              dims=('time', 'level', 'latitude', 'longitude')   shape=(40, 13, 721, 1440)\n",
      " • volumetric_soil_water_layer_1  dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • volumetric_soil_water_layer_2  dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • volumetric_soil_water_layer_3  dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • volumetric_soil_water_layer_4  dims=('time', 'latitude', 'longitude')   shape=(40, 721, 1440)\n",
      " • vorticity                      dims=('time', 'level', 'latitude', 'longitude')   shape=(40, 13, 721, 1440)\n",
      " • wind_speed                     dims=('time', 'level', 'latitude', 'longitude')   shape=(40, 13, 721, 1440)\n",
      "\n",
      "Coordinates in ds.coords:\n",
      " • latitude = [90.   89.75 89.5  89.25 89.  ] …\n",
      " • level = [ 50 100 150 200 250] …\n",
      " • longitude = [0.   0.25 0.5  0.75 1.  ] …\n",
      " • time = ['2023-01-01T00:00:00.000000000' '2023-01-01T06:00:00.000000000'\n",
      " '2023-01-01T12:00:00.000000000' '2023-01-01T18:00:00.000000000'\n",
      " '2023-01-02T00:00:00.000000000'] …\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m ARRIVAL_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasets/Arrival_With_Weather/2023\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# pick the first CSV in the folder\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mARRIVAL_DIR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     23\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ARRIVAL_DIR, fn), parse_dates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlightDate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== SAMPLE ARRIVAL CSV:\u001b[39m\u001b[38;5;124m\"\u001b[39m, fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# ─── 1) Inspect your ERA5 dataset ──────────────────────────────────────────────\n",
    "ERA5_STORE = \"ERA5_2023_6h_13lvl_1440x721.zarr\"\n",
    "ds = xr.open_zarr(ERA5_STORE)\n",
    "\n",
    "print(\"=== ERA5 DATASET SUMMARY ===\")\n",
    "print(ds)                        # full summary: dims, coords, data_vars\n",
    "print(\"\\nVariables in ds.data_vars:\")\n",
    "for name, var in ds.data_vars.items():\n",
    "    print(f\" • {name:30s} dims={var.dims}   shape={tuple(var.shape)}\")\n",
    "\n",
    "print(\"\\nCoordinates in ds.coords:\")\n",
    "for coord in ds.coords:\n",
    "    print(\" •\", coord, \"=\", ds.coords[coord].values[:5], \"…\")\n",
    "\n",
    "# ─── 2) Inspect one of your cleaned arrivals CSVs ──────────────────────────────\n",
    "ARRIVAL_DIR = \"Datasets/Arrival_With_Weather/2023\"\n",
    "# pick the first CSV in the folder\n",
    "fn = sorted([f for f in os.listdir(ARRIVAL_DIR) if f.lower().endswith(\".csv\")])[0]\n",
    "df = pd.read_csv(os.path.join(ARRIVAL_DIR, fn), parse_dates=[\"FlightDate\"])\n",
    "\n",
    "print(\"\\n=== SAMPLE ARRIVAL CSV:\", fn, \"===\\n\")\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5e20363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dims before subsetting: FrozenMappingWarningOnValuesAccess({'time': 40, 'lat': 721, 'lon': 1440, 'level': 13})\n",
      "dims after subsetting: FrozenMappingWarningOnValuesAccess({'time': 40, 'lat': 105, 'lon': 237, 'level': 13})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "#ERA5_STORE   = \"ERA5_2023_6h_13l_US.zarr\"\n",
    "ERA5_STORE   = \"ERA5_2023_6h_13lvl_1440x721.zarr\"\n",
    "MASTER_COORD = \"Datasets/T_MASTER_CORD.csv\"\n",
    "ARRIVAL_DIR  = \"Datasets/Arrival_Statistics/2023\"\n",
    "OUTPUT_DIR   = \"Datasets/Arrival_With_Weather/2023\"\n",
    "\n",
    "VARS = [\n",
    "    \"2m_temperature\",\n",
    "    \"mean_sea_level_pressure\",\n",
    "    \"10m_u_component_of_wind\",\n",
    "    \"10m_v_component_of_wind\",\n",
    "    \"total_precipitation_6hr\",\n",
    "    \"mean_top_downward_short_wave_radiation_flux\",\n",
    "]\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ─── 1) load ERA5 & rename dims ───────────────────────────────────────────────\n",
    "ds = xr.open_zarr(ERA5_STORE, consolidated=True)\n",
    "\n",
    "if \"latitude\" in ds.dims and \"longitude\" in ds.dims:\n",
    "    ds = ds.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n",
    "\n",
    "print(\"dims before subsetting:\", ds.dims)\n",
    "\n",
    "# ─── 1b) wrap & sort lon/lat, then slice just over the U.S. ────────────────\n",
    "# (drop this block if ERA5_STORE already points to your US‑only zarr)\n",
    "us_lon_min, us_lon_max = 235.0, 294.0   # [0–360] U.S. longitudes\n",
    "us_lat_min, us_lat_max =  24.0,  50.0   # U.S. latitudes\n",
    "\n",
    "ds = (\n",
    "    ds\n",
    "    .assign_coords(lon=((ds.lon + 360) % 360))\n",
    "    .sortby(\"lon\")\n",
    "    .sortby(\"lat\")\n",
    "    .sel(lon=slice(us_lon_min, us_lon_max),\n",
    "         lat=slice(us_lat_min, us_lat_max))\n",
    ")\n",
    "print(\"dims after subsetting:\", ds.dims)\n",
    "\n",
    "# ─── 2) build your fast lookup arrays ────────────────────────────────────────\n",
    "time_index = ds[\"time\"].to_index()\n",
    "lat_vals    = ds[\"lat\"].values\n",
    "lon_vals    = ds[\"lon\"].values\n",
    "\n",
    "# ─── 3) load master coords ───────────────────────────────────────────────────\n",
    "mc = (\n",
    "    pd.read_csv(MASTER_COORD, dtype=str)\n",
    "      .set_index(\"AIRPORT_SEQ_ID\")[[\"LATITUDE\",\"LONGITUDE\"]]\n",
    "      .astype(float)\n",
    ")\n",
    "\n",
    "# ─── 4) hhmm → Timedelta helper ─────────────────────────────────────────────\n",
    "def parse_hhmm(x):\n",
    "    if pd.isna(x) or not str(x).strip():\n",
    "        return pd.NaT\n",
    "    s = str(int(float(x))).zfill(4)\n",
    "    return pd.Timedelta(hours=int(s[:2]), minutes=int(s[2:]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ─── right after your subsetting block, before you build time_index, etc ───\n",
    "# Pre‑load all the VARS into memory as NumPy arrays:\n",
    "var_arrays = {}\n",
    "for var in VARS:\n",
    "    # this will compute the Dask array into a np.ndarray of shape (time, lat, lon)\n",
    "    var_arrays[var] = ds[var].data.compute()\n",
    "# Now var_arrays[var][t_idx, l_idx, o_idx] is pure NumPy fancy‑indexing\n",
    "\n",
    "# ─── then your make_weather_lookup becomes ─────────────────────────────────\n",
    "def make_weather_lookup(keys: pd.DataFrame) -> pd.DataFrame:\n",
    "    sub = keys.copy()\n",
    "    sub[\"lat\"] = sub[\"AirportSeqID\"].map(mc[\"LATITUDE\"])\n",
    "    sub[\"lon\"] = sub[\"AirportSeqID\"].map(mc[\"LONGITUDE\"]) % 360.0\n",
    "    sub = sub.dropna(subset=[\"Datetime\",\"lat\",\"lon\"])\n",
    "    if sub.empty:\n",
    "        return pd.DataFrame(columns=VARS,\n",
    "                            index=pd.MultiIndex.from_arrays([[],[]],\n",
    "                                                           names=[\"AirportSeqID\",\"Datetime\"]))\n",
    "\n",
    "    # 1) nearest‐time\n",
    "    t_idx = time_index.get_indexer(sub[\"Datetime\"], method=\"nearest\")\n",
    "\n",
    "    # 2) nearest‐lat/lon\n",
    "    sub_lat = sub[\"lat\"].to_numpy()\n",
    "    sub_lon = sub[\"lon\"].to_numpy()\n",
    "    l_idx = np.abs(lat_vals[None,:] - sub_lat[:,None]).argmin(axis=1)\n",
    "    o_idx = np.abs(lon_vals[None,:] - sub_lon[:,None]).argmin(axis=1)\n",
    "\n",
    "    # 3) pull out each VAR from our preloaded var_arrays\n",
    "    out = {}\n",
    "    for var in VARS:\n",
    "        arr = var_arrays[var]   # pure numpy now\n",
    "        out[var] = arr[t_idx, l_idx, o_idx]\n",
    "\n",
    "    # 4) assemble a DataFrame and re‑index\n",
    "    df_lkp = pd.DataFrame(out, index=sub.index)\n",
    "    df_lkp.index = pd.MultiIndex.from_frame(\n",
    "        sub[[\"AirportSeqID\",\"Datetime\"]],\n",
    "        names=[\"AirportSeqID\",\"Datetime\"]\n",
    "    )\n",
    "    return df_lkp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4027164e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_42080\\1839012808.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2023\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_1.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_42080\\1839012808.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2023\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_10.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_11.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_42080\\1839012808.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2023\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_11.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_42080\\1839012808.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2023\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_12.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_42080\\1839012808.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2023\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_2.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_42080\\1839012808.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2023\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_3.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_42080\\1839012808.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2023\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_4.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_42080\\1839012808.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2023\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_5.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_42080\\1839012808.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2023\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_6.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_42080\\1839012808.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2023\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_7.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_42080\\1839012808.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2023\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_8.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_9.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_42080\\1839012808.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   saved → Datasets/Arrival_With_Weather/2023\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_9.csv\n"
     ]
    }
   ],
   "source": [
    "for fn in sorted(os.listdir(ARRIVAL_DIR)):\n",
    "    if not fn.lower().endswith(\".csv\"):\n",
    "        continue\n",
    "    print(\"→ augmenting\", fn)\n",
    "    df = pd.read_csv(os.path.join(ARRIVAL_DIR, fn), dtype=str, parse_dates=False)\n",
    "\n",
    "    # 1) build true timestamps\n",
    "    df[\"FlightDate\"] = (\n",
    "        pd.to_datetime(df[\"FlightDate\"].astype(str),\n",
    "                       format=\"%Y-%m-%d\",\n",
    "                       errors=\"coerce\")\n",
    "          .dt.normalize()\n",
    "    )\n",
    "    df[\"DepDelta\"] = df[\"DepTime\"].apply(parse_hhmm)\n",
    "    df[\"ArrDelta\"] = df[\"ArrTime\"].apply(parse_hhmm)\n",
    "    df[\"DepDatetime\"] = df[\"FlightDate\"] + df[\"DepDelta\"]\n",
    "    df[\"ArrDatetime\"] = df[\"FlightDate\"] + df[\"ArrDelta\"]\n",
    "\n",
    "    # 2) vectorized lookups\n",
    "    orig = (\n",
    "      df[[\"OriginAirportSeqID\",\"DepDatetime\"]]\n",
    "        .dropna().drop_duplicates()\n",
    "        .rename(columns={\"OriginAirportSeqID\":\"AirportSeqID\",\n",
    "                         \"DepDatetime\":\"Datetime\"})\n",
    "    )\n",
    "    dest = (\n",
    "      df[[\"DestAirportSeqID\",\"ArrDatetime\"]]\n",
    "        .dropna().drop_duplicates()\n",
    "        .rename(columns={\"DestAirportSeqID\":\"AirportSeqID\",\n",
    "                         \"ArrDatetime\":\"Datetime\"})\n",
    "    )\n",
    "    orig_lkp = make_weather_lookup(orig)\n",
    "    dest_lkp = make_weather_lookup(dest)\n",
    "\n",
    "    # 3) stitch back on\n",
    "    df = (\n",
    "      df\n",
    "      .merge(orig_lkp.rename(columns=lambda c: f\"Origin_{c}\"),\n",
    "             left_on=[\"OriginAirportSeqID\",\"DepDatetime\"],\n",
    "             right_index=True, how=\"left\")\n",
    "      .merge(dest_lkp.rename(columns=lambda c: f\"Dest_{c}\"),\n",
    "             left_on=[\"DestAirportSeqID\",\"ArrDatetime\"],\n",
    "             right_index=True, how=\"left\")\n",
    "    )\n",
    "\n",
    "    # 4) drop your temps\n",
    "    df.drop(columns=[\"DepDelta\",\"ArrDelta\",\"DepDatetime\",\"ArrDatetime\"],\n",
    "            errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # 5) now drop *any* column with no actual data:\n",
    "    #    first turn blank‐strings into NaN\n",
    "    df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "    #    then drop columns that are all-NaN\n",
    "    df.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "    # 6) save\n",
    "    out = os.path.join(OUTPUT_DIR, fn)\n",
    "    df.to_csv(out, index=False)\n",
    "    print(\"   saved →\", out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f535d20",
   "metadata": {},
   "source": [
    "Now run preprocessing to merge and format the arrival data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f777d5",
   "metadata": {},
   "source": [
    "then take data and create grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117ef357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ddb11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe67e92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2023_1.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 104\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m VARS:\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m         val \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnearest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlon\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m         val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.conda\\lib\\site-packages\\xarray\\core\\dataarray.py:1690\u001b[0m, in \u001b[0;36mDataArray.sel\u001b[1;34m(self, indexers, method, tolerance, drop, **indexers_kwargs)\u001b[0m\n\u001b[0;32m   1574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msel\u001b[39m(\n\u001b[0;32m   1575\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1576\u001b[0m     indexers: Mapping[Any, Any] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1580\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mindexers_kwargs: Any,\n\u001b[0;32m   1581\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   1582\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a new DataArray whose data is given by selecting index\u001b[39;00m\n\u001b[0;32m   1583\u001b[0m \u001b[38;5;124;03m    labels along the specified dimension(s).\u001b[39;00m\n\u001b[0;32m   1584\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1688\u001b[0m \u001b[38;5;124;03m    Dimensions without coordinates: points\u001b[39;00m\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1690\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_temp_dataset()\u001b[38;5;241m.\u001b[39msel(\n\u001b[0;32m   1691\u001b[0m         indexers\u001b[38;5;241m=\u001b[39mindexers,\n\u001b[0;32m   1692\u001b[0m         drop\u001b[38;5;241m=\u001b[39mdrop,\n\u001b[0;32m   1693\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m   1694\u001b[0m         tolerance\u001b[38;5;241m=\u001b[39mtolerance,\n\u001b[0;32m   1695\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mindexers_kwargs,\n\u001b[0;32m   1696\u001b[0m     )\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_temp_dataset(ds)\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.conda\\lib\\site-packages\\xarray\\core\\dataset.py:2890\u001b[0m, in \u001b[0;36mDataset.sel\u001b[1;34m(self, indexers, method, tolerance, drop, **indexers_kwargs)\u001b[0m\n\u001b[0;32m   2822\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a new dataset with each array indexed by tick labels\u001b[39;00m\n\u001b[0;32m   2823\u001b[0m \u001b[38;5;124;03malong the specified dimension(s).\u001b[39;00m\n\u001b[0;32m   2824\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2887\u001b[0m \n\u001b[0;32m   2888\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2889\u001b[0m indexers \u001b[38;5;241m=\u001b[39m either_dict_or_kwargs(indexers, indexers_kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2890\u001b[0m query_results \u001b[38;5;241m=\u001b[39m \u001b[43mmap_index_queries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2891\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\n\u001b[0;32m   2892\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m drop:\n\u001b[0;32m   2895\u001b[0m     no_scalar_variables \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.conda\\lib\\site-packages\\xarray\\core\\indexing.py:196\u001b[0m, in \u001b[0;36mmap_index_queries\u001b[1;34m(obj, indexers, method, tolerance, **indexers_kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(IndexSelResult(labels))\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(index\u001b[38;5;241m.\u001b[39msel(labels, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions))\n\u001b[0;32m    198\u001b[0m merged \u001b[38;5;241m=\u001b[39m merge_sel_results(results)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;66;03m# drop dimension coordinates found in dimension indexers\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# (also drop multi-index if any)\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# (.sel() already ensures alignment)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.conda\\lib\\site-packages\\xarray\\core\\indexes.py:783\u001b[0m, in \u001b[0;36mPandasIndex.sel\u001b[1;34m(self, labels, method, tolerance)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 783\u001b[0m         indexer \u001b[38;5;241m=\u001b[39m \u001b[43mget_indexer_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    786\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(indexer \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    787\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m    788\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot all values found in index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoord_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    789\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.conda\\lib\\site-packages\\xarray\\core\\indexes.py:564\u001b[0m, in \u001b[0;36mget_indexer_nd\u001b[1;34m(index, labels, method, tolerance)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flat_labels\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    563\u001b[0m     flat_labels \u001b[38;5;241m=\u001b[39m flat_labels\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 564\u001b[0m flat_indexer \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    565\u001b[0m indexer \u001b[38;5;241m=\u001b[39m flat_indexer\u001b[38;5;241m.\u001b[39mreshape(labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexer\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.conda\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3953\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   3948\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   3949\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m this\u001b[38;5;241m.\u001b[39m_get_indexer(\n\u001b[0;32m   3950\u001b[0m         target, method\u001b[38;5;241m=\u001b[39mmethod, limit\u001b[38;5;241m=\u001b[39mlimit, tolerance\u001b[38;5;241m=\u001b[39mtolerance\n\u001b[0;32m   3951\u001b[0m     )\n\u001b[1;32m-> 3953\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.conda\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3968\u001b[0m, in \u001b[0;36mIndex._get_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   3966\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_fill_indexer(target, method, limit, tolerance)\n\u001b[0;32m   3967\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 3968\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_nearest_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39m_is_multi \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_multi:\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.conda\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4139\u001b[0m, in \u001b[0;36mIndex._get_nearest_indexer\u001b[1;34m(self, target, limit, tolerance)\u001b[0m\n\u001b[0;32m   4136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   4137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_fill_indexer(target, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 4139\u001b[0m left_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4140\u001b[0m right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer(target, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackfill\u001b[39m\u001b[38;5;124m\"\u001b[39m, limit\u001b[38;5;241m=\u001b[39mlimit)\n\u001b[0;32m   4142\u001b[0m left_distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_difference_compat(target, left_indexer)\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.conda\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3953\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   3948\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   3949\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m this\u001b[38;5;241m.\u001b[39m_get_indexer(\n\u001b[0;32m   3950\u001b[0m         target, method\u001b[38;5;241m=\u001b[39mmethod, limit\u001b[38;5;241m=\u001b[39mlimit, tolerance\u001b[38;5;241m=\u001b[39mtolerance\n\u001b[0;32m   3951\u001b[0m     )\n\u001b[1;32m-> 3953\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.conda\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3966\u001b[0m, in \u001b[0;36mIndex._get_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   3963\u001b[0m     tolerance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_tolerance(tolerance, target)\n\u001b[0;32m   3965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackfill\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m-> 3966\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_fill_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3967\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   3968\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_nearest_indexer(target, limit, tolerance)\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.conda\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4088\u001b[0m, in \u001b[0;36mIndex._get_fill_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   4086\u001b[0m         indexer \u001b[38;5;241m=\u001b[39m libalgos\u001b[38;5;241m.\u001b[39mbackfill(own_values, target_values, limit\u001b[38;5;241m=\u001b[39mlimit)\n\u001b[0;32m   4087\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4088\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_fill_indexer_searchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tolerance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   4090\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_indexer_tolerance(target, indexer, tolerance)\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.conda\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4112\u001b[0m, in \u001b[0;36mIndex._get_fill_indexer_searchsorted\u001b[1;34m(self, target, method, limit)\u001b[0m\n\u001b[0;32m   4110\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer(target)\n\u001b[0;32m   4111\u001b[0m nonexact \u001b[38;5;241m=\u001b[39m indexer \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 4112\u001b[0m indexer[nonexact] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_searchsorted_monotonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnonexact\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m side \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   4114\u001b[0m     \u001b[38;5;66;03m# searchsorted returns \"indices into a sorted array such that,\u001b[39;00m\n\u001b[0;32m   4115\u001b[0m     \u001b[38;5;66;03m# if the corresponding elements in v were inserted before the\u001b[39;00m\n\u001b[0;32m   4116\u001b[0m     \u001b[38;5;66;03m# indices, the order of a would be preserved\".\u001b[39;00m\n\u001b[0;32m   4117\u001b[0m     \u001b[38;5;66;03m# Thus, we need to subtract 1 to find values to the left.\u001b[39;00m\n\u001b[0;32m   4118\u001b[0m     indexer[nonexact] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.conda\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6738\u001b[0m, in \u001b[0;36mIndex._searchsorted_monotonic\u001b[1;34m(self, label, side)\u001b[0m\n\u001b[0;32m   6733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearchsorted(label, side\u001b[38;5;241m=\u001b[39mside)\n\u001b[0;32m   6734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_monotonic_decreasing:\n\u001b[0;32m   6735\u001b[0m     \u001b[38;5;66;03m# np.searchsorted expects ascending sort order, have to reverse\u001b[39;00m\n\u001b[0;32m   6736\u001b[0m     \u001b[38;5;66;03m# everything for it to work (element ordering, search side and\u001b[39;00m\n\u001b[0;32m   6737\u001b[0m     \u001b[38;5;66;03m# resulting value).\u001b[39;00m\n\u001b[1;32m-> 6738\u001b[0m     pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   6739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mright\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   6740\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m pos\n\u001b[0;32m   6743\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex must be monotonic increasing or decreasing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.conda\\lib\\site-packages\\pandas\\core\\base.py:1352\u001b[0m, in \u001b[0;36mIndexOpsMixin.searchsorted\u001b[1;34m(self, value, side, sorter)\u001b[0m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m   1349\u001b[0m     \u001b[38;5;66;03m# Going through EA.searchsorted directly improves performance GH#38083\u001b[39;00m\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\u001b[38;5;241m.\u001b[39msearchsorted(value, side\u001b[38;5;241m=\u001b[39mside, sorter\u001b[38;5;241m=\u001b[39msorter)\n\u001b[1;32m-> 1352\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43msorter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1357\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.conda\\lib\\site-packages\\pandas\\core\\algorithms.py:1329\u001b[0m, in \u001b[0;36msearchsorted\u001b[1;34m(arr, value, side, sorter)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     arr \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(arr)\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;66;03m# Argument 1 to \"searchsorted\" of \"ndarray\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;66;03m# \"Union[NumpyValueArrayLike, ExtensionArray]\"; expected \"NumpyValueArrayLike\"\u001b[39;00m\n\u001b[1;32m-> 1329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "ERA5_STORE   = \"ERA5_2023_6h_13lvl_1440x721.zarr\"\n",
    "MASTER_COORD = \"Datasets/T_MASTER_CORD.csv\"\n",
    "ARRIVAL_DIR  = \"Datasets/Arrival_Statistics/2023\"\n",
    "OUTPUT_DIR   = \"Datasets/Arrival_With_Weather/2023\"\n",
    "\n",
    "# the ERA5 vars you want to pull out\n",
    "VARS = [\n",
    "    \"2m_temperature\",\n",
    "    \"mean_sea_level_pressure\",\n",
    "    \"10m_u_component_of_wind\",\n",
    "    \"10m_v_component_of_wind\",\n",
    "    \"total_precipitation_6hr\",\n",
    "    \"toa_incident_solar_radiation\",\n",
    "]\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ─── 1) load ERA5 & fix coords ─────────────────────────────────────────────────\n",
    "ds = xr.open_zarr(ERA5_STORE)\n",
    "# if your zarr came with 'latitude'/'longitude' dims, rename them:\n",
    "if \"latitude\" in ds.dims and \"longitude\" in ds.dims:\n",
    "    ds = ds.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n",
    "\n",
    "# ─── 2) load master coords ─────────────────────────────────────────────────────\n",
    "mc = (\n",
    "    pd.read_csv(MASTER_COORD, dtype=str)\n",
    "      .set_index(\"AIRPORT_SEQ_ID\")[[\"LATITUDE\", \"LONGITUDE\"]]\n",
    ")\n",
    "\n",
    "# ─── 3) helper to parse HHMM → Timedelta ────────────────────────────────────────\n",
    "def parse_hhmm(x):\n",
    "    if pd.isna(x) or not str(x).strip():\n",
    "        return pd.NaT\n",
    "    s = str(int(float(x))).zfill(4)\n",
    "    return pd.Timedelta(hours=int(s[:2]), minutes=int(s[2:]))\n",
    "\n",
    "# ─── 4) loop over each month’s arrival file ────────────────────────────────────\n",
    "for fn in sorted(os.listdir(ARRIVAL_DIR)):\n",
    "    if not fn.lower().endswith(\".csv\"):\n",
    "        continue\n",
    "    print(\"→ augmenting\", fn)\n",
    "    # --- read your CSV; FlightDate will come in as object/string because of dtype=str\n",
    "    df = pd.read_csv(\n",
    "        os.path.join(ARRIVAL_DIR, fn),\n",
    "        parse_dates=False,     # we’ll do it ourselves\n",
    "        dtype=str\n",
    "        )\n",
    "\n",
    "# 1) cast the FlightDate column to datetime64 (dropping any rogue time component):\n",
    "    df[\"FlightDate\"] = (\n",
    "        pd.to_datetime(df[\"FlightDate\"].astype(str),    # ensure it’s all string\n",
    "                       format=\"%Y-%m-%d\",\n",
    "                       errors=\"coerce\")\n",
    "          .dt.normalize()   # midnight, so FlightDate + Timedelta works nicely\n",
    "    )\n",
    "    \n",
    "\n",
    "    # now you can safely do .dt if you need it, but we don’t:\n",
    "    # 2) parse your HHMM into Timedeltas\n",
    "    df[\"DepDelta\"] = df[\"DepTime\"].apply(parse_hhmm)\n",
    "    df[\"ArrDelta\"] = df[\"ArrTime\"].apply(parse_hhmm)\n",
    "\n",
    "    # if FlightDate sometimes comes in as e.g. \"2023‑01‑08 00:00:00\", you can\n",
    "    # strip to just the date first, *after* it’s datetime:\n",
    "    df[\"FlightDate\"] = (\n",
    "        pd.to_datetime(df[\"FlightDate\"], errors=\"coerce\")\n",
    "          .dt.floor(\"D\")\n",
    "    )\n",
    "\n",
    "    # 3) form real timestamps\n",
    "    df[\"DepDatetime\"] = df[\"FlightDate\"] + df[\"DepDelta\"]\n",
    "    df[\"ArrDatetime\"] = df[\"FlightDate\"] + df[\"ArrDelta\"]\n",
    "\n",
    "    # … the rest of your augmentation loop follows …\n",
    "\n",
    "\n",
    "    # c) pre‑create your weather columns\n",
    "    for var in VARS:\n",
    "        df[f\"Origin_{var}\"] = np.nan\n",
    "        df[f\"Dest_{var}\"]   = np.nan\n",
    "\n",
    "    # d) fill them in\n",
    "    for i, row in df.iterrows():\n",
    "        for side, DTcol, AIDcol in [\n",
    "            (\"Origin\",\"DepDatetime\",\"OriginAirportSeqID\"),\n",
    "            (\"Dest\",  \"ArrDatetime\",\"DestAirportSeqID\")\n",
    "        ]:\n",
    "            t   = row[DTcol]\n",
    "            aid = str(row[AIDcol]).strip()\n",
    "            if pd.isna(t) or aid not in mc.index:\n",
    "                continue\n",
    "\n",
    "            lat = float(mc.at[aid, \"LATITUDE\"])\n",
    "            lon = float(mc.at[aid, \"LONGITUDE\"]) % 360\n",
    "\n",
    "            for var in VARS:\n",
    "                try:\n",
    "                    val = ds[var].sel(\n",
    "                        time=t,   method=\"nearest\",\n",
    "                        lat=lat, lon=lon\n",
    "                    ).item()\n",
    "                except Exception:\n",
    "                    val = np.nan\n",
    "                df.at[i, f\"{side}_{var}\"] = val\n",
    "\n",
    "    # e) drop just the temporaries\n",
    "    df.drop(columns=[\"DepDelta\",\"ArrDelta\",\"DepDatetime\",\"ArrDatetime\"],\n",
    "            inplace=True)\n",
    "\n",
    "    # f) drop _any_ column that's 100% NaN (that includes weather columns\n",
    "    #    for out‑of‑US or missing airports)\n",
    "    df.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "    # g) write it back out\n",
    "    out = os.path.join(OUTPUT_DIR, fn)\n",
    "    df.to_csv(out, index=False)\n",
    "    print(\"   saved →\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b9bf84",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 60\u001b[0m\n\u001b[0;32m     44\u001b[0m model_config \u001b[38;5;241m=\u001b[39m graphcast\u001b[38;5;241m.\u001b[39mModelConfig(\n\u001b[0;32m     45\u001b[0m     resolution\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m,\n\u001b[0;32m     46\u001b[0m     mesh_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m     radius_query_fraction_edge_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m\n\u001b[0;32m     51\u001b[0m )\n\u001b[0;32m     52\u001b[0m task_config \u001b[38;5;241m=\u001b[39m graphcast\u001b[38;5;241m.\u001b[39mTaskConfig(\n\u001b[0;32m     53\u001b[0m     input_variables   \u001b[38;5;241m=\u001b[39m graphcast\u001b[38;5;241m.\u001b[39mTASK\u001b[38;5;241m.\u001b[39minput_variables,\n\u001b[0;32m     54\u001b[0m     forcing_variables \u001b[38;5;241m=\u001b[39m graphcast\u001b[38;5;241m.\u001b[39mTASK\u001b[38;5;241m.\u001b[39mforcing_variables,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     input_duration    \u001b[38;5;241m=\u001b[39m graphcast\u001b[38;5;241m.\u001b[39mTASK\u001b[38;5;241m.\u001b[39minput_duration\n\u001b[0;32m     58\u001b[0m )\n\u001b[1;32m---> 60\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mgraphcast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraphCast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m rng   \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# ─── C) Prepare an empty output Zarr for predictions ──────────────────────────\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\graphcast\\graphcast.py:256\u001b[0m, in \u001b[0;36mGraphCast.__init__\u001b[1;34m(self, model_config, task_config)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spatial_features_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    246\u001b[0m     add_node_positions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    247\u001b[0m     add_node_latitude\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    251\u001b[0m     relative_latitude_local_coordinates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    252\u001b[0m )\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# Specification of the multimesh.\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meshes \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 256\u001b[0m     \u001b[43micosahedral_mesh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_hierarchy_of_triangular_meshes_for_sphere\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmesh_size\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Encoder, which moves data from the grid to the mesh with a single message\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;66;03m# passing step.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grid2mesh_gnn \u001b[38;5;241m=\u001b[39m deep_typed_graph_net\u001b[38;5;241m.\u001b[39mDeepTypedGraphNet(\n\u001b[0;32m    262\u001b[0m     embed_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Embed raw features of the grid and mesh nodes.\u001b[39;00m\n\u001b[0;32m    263\u001b[0m     embed_edges\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Embed raw features of the grid2mesh edges.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    276\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrid2mesh_gnn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    277\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\graphcast\\icosahedral_mesh.py:86\u001b[0m, in \u001b[0;36mget_hierarchy_of_triangular_meshes_for_sphere\u001b[1;34m(splits)\u001b[0m\n\u001b[0;32m     84\u001b[0m output_meshes \u001b[38;5;241m=\u001b[39m [current_mesh]\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(splits):\n\u001b[1;32m---> 86\u001b[0m   current_mesh \u001b[38;5;241m=\u001b[39m \u001b[43m_two_split_unit_sphere_triangle_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_mesh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m   output_meshes\u001b[38;5;241m.\u001b[39mappend(current_mesh)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_meshes\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\graphcast\\icosahedral_mesh.py:199\u001b[0m, in \u001b[0;36m_two_split_unit_sphere_triangle_faces\u001b[1;34m(triangular_mesh)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind1, ind2, ind3 \u001b[38;5;129;01min\u001b[39;00m triangular_mesh\u001b[38;5;241m.\u001b[39mfaces:\n\u001b[0;32m    185\u001b[0m   \u001b[38;5;66;03m# Transform each triangular face into 4 triangles,\u001b[39;00m\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;66;03m# preserving the orientation.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m   \u001b[38;5;66;03m#   /               \\    /              \\\u001b[39;00m\n\u001b[0;32m    197\u001b[0m   \u001b[38;5;66;03m# ind1 ------------ ind12 ------------ ind2\u001b[39;00m\n\u001b[0;32m    198\u001b[0m   ind12 \u001b[38;5;241m=\u001b[39m new_vertices_builder\u001b[38;5;241m.\u001b[39mget_new_child_vertex_index((ind1, ind2))\n\u001b[1;32m--> 199\u001b[0m   ind23 \u001b[38;5;241m=\u001b[39m \u001b[43mnew_vertices_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_new_child_vertex_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mind3\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m   ind31 \u001b[38;5;241m=\u001b[39m new_vertices_builder\u001b[38;5;241m.\u001b[39mget_new_child_vertex_index((ind3, ind1))\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;66;03m# Note how each of the 4 triangular new faces specifies the order of the\u001b[39;00m\n\u001b[0;32m    202\u001b[0m   \u001b[38;5;66;03m# vertices to preserve the orientation of the original face. As the input\u001b[39;00m\n\u001b[0;32m    203\u001b[0m   \u001b[38;5;66;03m# face should always be counter-clockwise as specified in the diagram,\u001b[39;00m\n\u001b[0;32m    204\u001b[0m   \u001b[38;5;66;03m# this means child faces should also be counter-clockwise.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\graphcast\\icosahedral_mesh.py:251\u001b[0m, in \u001b[0;36m_ChildVerticesBuilder.get_new_child_vertex_index\u001b[1;34m(self, parent_vertex_indices)\u001b[0m\n\u001b[0;32m    249\u001b[0m child_vertex_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_child_vertex_key(parent_vertex_indices)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m child_vertex_key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_child_vertices_index_mapping:\n\u001b[1;32m--> 251\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_child_vertex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent_vertex_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_child_vertices_index_mapping[child_vertex_key]\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\graphcast\\icosahedral_mesh.py:236\u001b[0m, in \u001b[0;36m_ChildVerticesBuilder._create_child_vertex\u001b[1;34m(self, parent_vertex_indices)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new vertex.\"\"\"\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# Position for new vertex is the middle point, between the parent points,\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# projected to unit sphere.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m child_vertex_position \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_vertices\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m--> 236\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparent_vertex_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m child_vertex_position \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(child_vertex_position)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# Add the vertex to the output list. The index for this new vertex will\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# match the length of the list before adding it.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.conda\\lib\\site-packages\\numpy\\_core\\_methods.py:118\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m um\u001b[38;5;241m.\u001b[39mclip(a, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_mean\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    119\u001b[0m     arr \u001b[38;5;241m=\u001b[39m asanyarray(a)\n\u001b[0;32m    121\u001b[0m     is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import jax\n",
    "import haiku as hk\n",
    "\n",
    "from graphcast import checkpoint\n",
    "from graphcast import data_utils\n",
    "from graphcast import graphcast\n",
    "\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "ERA5_STORE    = \"ERA5_2023_6h_13lvl_1440x721.zarr\"\n",
    "PRED_STORE    = \"Preds_2023_6h_13l.zarr\"\n",
    "CKPT_NPZ      = \"Models/graphcast_operational.npz\"\n",
    "ARR_DIR       = \"Datasets/Arrival_Statistics/2023\"\n",
    "OUT_DIR       = \"Datasets/Arrival_With_GNN/2023\"\n",
    "MASTER_COORD  = \"Datasets/T_MASTER_CORD.csv\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ─── A) Load & unflatten .npz params ───────────────────────────────────────────\n",
    "def unflatten_params(flat):\n",
    "    tree = {}\n",
    "    for path, arr in flat.items():\n",
    "        d = tree\n",
    "        *keys, leaf = path\n",
    "        for k in keys:\n",
    "            d = d.setdefault(k, {})\n",
    "        d[leaf] = arr\n",
    "    return tree\n",
    "\n",
    "npz = np.load(CKPT_NPZ, allow_pickle=True)\n",
    "flat = {\n",
    "    tuple(k.split(\"/\")): npz[k]\n",
    "    for k in npz.files if k.startswith(\"params:\")\n",
    "}\n",
    "nested = unflatten_params(flat)\n",
    "params = hk.data_structures.to_immutable_dict(nested)\n",
    "state  = {}  # no 'state:' in this archive\n",
    "\n",
    "# ─── B) Rebuild model & task configs ──────────────────────────────────────────\n",
    "model_config = graphcast.ModelConfig(\n",
    "    resolution=0.25,\n",
    "    mesh_size=20,\n",
    "    latent_size=64,\n",
    "    gnn_msg_steps=1,\n",
    "    hidden_layers=2,\n",
    "    radius_query_fraction_edge_length=0.6\n",
    ")\n",
    "task_config = graphcast.TaskConfig(\n",
    "    input_variables   = graphcast.TASK.input_variables,\n",
    "    forcing_variables = graphcast.TASK.forcing_variables,\n",
    "    target_variables  = graphcast.TASK.target_variables,\n",
    "    pressure_levels   = graphcast.PRESSURE_LEVELS[13],\n",
    "    input_duration    = graphcast.TASK.input_duration\n",
    ")\n",
    "\n",
    "model = graphcast.GraphCast(model_config, task_config)\n",
    "rng   = jax.random.PRNGKey(0)\n",
    "\n",
    "# ─── C) Prepare an empty output Zarr for predictions ──────────────────────────\n",
    "ds_in = xr.open_zarr(ERA5_STORE, chunks={\"time\": 1})\n",
    "template = xr.Dataset(\n",
    "    {\n",
    "     var: ((\"time\",\"lat\",\"lon\"),\n",
    "           np.full((len(ds_in.time), len(ds_in.lat), len(ds_in.lon)), np.nan))\n",
    "     for var in task_config.target_variables\n",
    "    },\n",
    "    coords={\"time\": ds_in.time, \"lat\": ds_in.lat, \"lon\": ds_in.lon}\n",
    ")\n",
    "template.to_zarr(PRED_STORE, mode=\"w\")\n",
    "\n",
    "# ─── D) Stream through each time-step, run GNN, and write slice ──────────────\n",
    "ds_pred = xr.open_zarr(PRED_STORE, consolidated=False, mode=\"a\")\n",
    "for ti in range(len(ds_in.time)):\n",
    "    t = ds_in.time.isel(time=ti)\n",
    "    di = ds_in[task_config.input_variables].isel(time=ti).expand_dims(batch=1, time=1)\n",
    "    df = ds_in[task_config.forcing_variables].isel(time=ti).expand_dims(batch=1, time=1)\n",
    "    dt = xr.zeros_like(ds_in[task_config.target_variables].isel(time=ti))\\\n",
    "           .expand_dims(batch=1, time=1)\n",
    "\n",
    "    preds, _ = model.apply(params, state, rng, di, dt, df, is_training=False)\n",
    "    p0 = preds.isel(batch=0, time=0)\n",
    "\n",
    "    for var in task_config.target_variables:\n",
    "        ds_pred[var][ti, :, :] = p0[var].data\n",
    "\n",
    "ds_pred.close()\n",
    "\n",
    "# ─── E) Load master coords ────────────────────────────────────────────────────\n",
    "mc = pd.read_csv(MASTER_COORD, dtype=str).set_index(\"AIRPORT_SEQ_ID\")\n",
    "mc[\"LAT\"] = mc[\"LATITUDE\"].astype(float)\n",
    "mc[\"LON\"] = (mc[\"LONGITUDE\"].astype(float) % 360)\n",
    "\n",
    "# ─── F) Augment arrival CSVs with GraphCast preds ─────────────────────────────\n",
    "def parse_hhmm(x):\n",
    "    if pd.isna(x) or str(x).strip()==\"\":\n",
    "        return pd.NaT\n",
    "    s = str(int(float(x))).zfill(4)\n",
    "    return pd.Timedelta(hours=int(s[:2]), minutes=int(s[2:]))\n",
    "\n",
    "preds_ds = xr.open_zarr(PRED_STORE, chunks={\"time\":1})\n",
    "\n",
    "for fn in os.listdir(ARR_DIR):\n",
    "    if not fn.lower().endswith(\".csv\"):\n",
    "        continue\n",
    "    df = pd.read_csv(os.path.join(ARR_DIR, fn), dtype=str,\n",
    "                     parse_dates=[\"FlightDate\"])\n",
    "    df[\"DepDT\"] = df[\"DepTime\"].apply(parse_hhmm) + df[\"FlightDate\"]\n",
    "    df[\"ArrDT\"] = df[\"ArrTime\"].apply(parse_hhmm) + df[\"FlightDate\"]\n",
    "\n",
    "    for var in task_config.target_variables:\n",
    "        df[f\"Origin_pred_{var}\"] = np.nan\n",
    "        df[f\"Dest_pred_{var}\"]   = np.nan\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        for side, dtcol, key in [\n",
    "            (\"Origin\",\"DepDT\",\"OriginAirportSeqID\"),\n",
    "            (\"Dest\",  \"ArrDT\",\"DestAirportSeqID\")\n",
    "        ]:\n",
    "            dt = row[dtcol]\n",
    "            aid = str(row.get(key,\"\"))\n",
    "            if pd.isna(dt) or aid not in mc.index:\n",
    "                continue\n",
    "            lat, lon = mc.at[aid,\"LAT\"], mc.at[aid,\"LON\"]\n",
    "            for var in task_config.target_variables:\n",
    "                try:\n",
    "                    val = preds_ds[var].sel(\n",
    "                        time=dt, method=\"nearest\",\n",
    "                        lat=lat, method=\"nearest\",\n",
    "                        lon=lon, method=\"nearest\"\n",
    "                    ).item()\n",
    "                except Exception:\n",
    "                    val = np.nan\n",
    "                df.at[i, f\"{side}_pred_{var}\"] = val\n",
    "\n",
    "    df.drop(columns=[\"DepDT\",\"ArrDT\"], inplace=True)\n",
    "    df.dropna(axis=1, how=\"all\", inplace=True)\n",
    "    out = os.path.join(OUT_DIR, fn)\n",
    "    df.to_csv(out, index=False)\n",
    "    print(\"Saved GNN‑augmented →\", out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aa629a",
   "metadata": {},
   "source": [
    "use gridded xarrays for GNN predictions and arrival data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b690cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import jax\n",
    "import haiku as hk\n",
    "import graphcast\n",
    "from graphcast.checkpoint import load as load_checkpoint\n",
    "from graphcast.model_utils import (\n",
    "    dataset_to_stacked,\n",
    "    stacked_to_dataset,\n",
    "    restore_leading_axes,\n",
    ")\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "CKPT_PATH     = \"Models/graphcast_operational.npz\"\n",
    "INPUT_STORE   = \"Datasets/Gridded_Station_Inputs_2024.nc\"   # your station‐gridded .nc\n",
    "ARRIVAL_DIR   = \"Datasets/Arrival_With_Weather/2024\"      # CSVs you want to augment\n",
    "OUTPUT_DIR    = \"Datasets/Arrival_With_GNN/2024\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ─── 1) load the checkpoint from your .npz ─────────────────────────────────────\n",
    "#    this will give you ckpt.params, ckpt.model_config, ckpt.task_config\n",
    "ckpt = load_checkpoint(CKPT_PATH, graphcast.CheckPoint)\n",
    "params      = ckpt.params\n",
    "model_cfg   = ckpt.model_config\n",
    "task_cfg    = ckpt.task_config\n",
    "\n",
    "# ─── 2) build your GraphCast model & JAX/Haiku wrappers ────────────────────────\n",
    "model = graphcast.GraphCast(model_cfg, task_cfg)\n",
    "\n",
    "# we need a pure‑function that takes a stacked input array and returns the stacked pred\n",
    "def _forward_fn(stacked_inputs):\n",
    "    # Haiku modules expect: apply(params, state, rng, *args)\n",
    "    return model(stacked_inputs)\n",
    "\n",
    "# wrap it into a pair of (init, apply) functions with state\n",
    "network = hk.without_apply_rng(hk.transform_with_state(_forward_fn))\n",
    "\n",
    "# ─── 3) load your pre‐gridded station dataset ──────────────────────────────────\n",
    "ds_grid = xr.open_dataset(INPUT_STORE)\n",
    "\n",
    "# slice out only the variables GraphCast needs\n",
    "vars_in  = list(task_cfg.input_variables) + list(task_cfg.forcing_variables)\n",
    "vars_out = list(task_cfg.target_variables)\n",
    "\n",
    "ds_in  = ds_grid[vars_in]\n",
    "ds_tmpl = ds_grid[vars_out]  # template for un‑stacking later\n",
    "\n",
    "# ─── 4) run the GNN once over the entire grid ─────────────────────────────────\n",
    "# stack everything into a single JAX array of shape [batch=1,time,lat,lon,channels]\n",
    "stacked = dataset_to_stacked(ds_in, sizes=ds_grid.sizes)\n",
    "stacked = stacked.expand_dims(\"batch\", 0)\n",
    "\n",
    "# apply the network\n",
    "(pred_flat, net_state), _ = network.apply(params, {}, None, stacked)\n",
    "\n",
    "# restore the axes so we have [batch,time,lat,lon,channels]\n",
    "pred_restored = restore_leading_axes(pred_flat)\n",
    "\n",
    "# unstack back into an xarray.Dataset with your target variable names\n",
    "ds_pred = stacked_to_dataset(pred_restored, template_dataset=ds_tmpl)\n",
    "\n",
    "# drop the dummy batch dim, now ds_pred dims = (time,lat,lon)\n",
    "ds_pred = ds_pred.isel(batch=0)\n",
    "\n",
    "\n",
    "# ─── 5) helper to turn HHMM → timedelta ───────────────────────────────────────\n",
    "def _parse_hhmm(x):\n",
    "    if pd.isna(x) or not str(x).strip():\n",
    "        return pd.NaT\n",
    "    s = str(int(float(x))).zfill(4)\n",
    "    hh, mm = int(s[:2]), int(s[2:])\n",
    "    return pd.Timedelta(hours=hh, minutes=mm)\n",
    "\n",
    "\n",
    "# ─── 6) loop over your arrival files and lookup the predictions ───────────────\n",
    "for fn in os.listdir(ARRIVAL_DIR):\n",
    "    if not fn.lower().endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    print(\"Augmenting\", fn)\n",
    "    df = pd.read_csv(os.path.join(ARRIVAL_DIR, fn),\n",
    "                     parse_dates=[\"FlightDate\"],\n",
    "                     dtype=str)\n",
    "\n",
    "    # build actual datetime columns for dep/arr\n",
    "    df[\"DepDelta\"] = df[\"DepTime\"].apply(_parse_hhmm)\n",
    "    df[\"ArrDelta\"] = df[\"ArrTime\"].apply(_parse_hhmm)\n",
    "    df[\"DepDatetime\"] = df[\"FlightDate\"] + df[\"DepDelta\"]\n",
    "    df[\"ArrDatetime\"] = df[\"FlightDate\"] + df[\"ArrDelta\"]\n",
    "\n",
    "    # prepare new prediction columns\n",
    "    for var in vars_out:\n",
    "        df[f\"Origin_pred_{var}\"] = np.nan\n",
    "        df[f\"Dest_pred_{var}\"]   = np.nan\n",
    "\n",
    "    # for each flight, do a nearest‑neighbor lookup in ds_pred\n",
    "    for side, dtcol, apicol, latcol, loncol in [\n",
    "        (\"Origin\", \"DepDatetime\", \"OriginAirportSeqID\", \"Origin_Lat\",  \"Origin_Lon\"),\n",
    "        (\"Dest\",   \"ArrDatetime\", \"DestAirportSeqID\",   \"Dest_Lat\",    \"Dest_Lon\"),\n",
    "    ]:\n",
    "        # you need to have pre‑computed Origin_Lat/Lon and Dest_Lat/Lon\n",
    "        # by merging in your master coord table earlier.\n",
    "        mask = ~df[dtcol].isna()\n",
    "        idx = df.loc[mask, [dtcol, apicol, latcol, loncol]].index\n",
    "        for i in idx:\n",
    "            t   = df.at[i, dtcol]\n",
    "            lat = float(df.at[i, latcol])\n",
    "            lon = float(df.at[i, loncol]) % 360.0\n",
    "            for var in vars_out:\n",
    "                try:\n",
    "                    v = ds_pred[var].sel(time=t, lat=lat, lon=lon,\n",
    "                                         method=\"nearest\").item()\n",
    "                except Exception:\n",
    "                    v = np.nan\n",
    "                df.at[i, f\"{side}_pred_{var}\"] = v\n",
    "\n",
    "    # drop helper cols\n",
    "    df.drop(columns=[\"DepDelta\",\"ArrDelta\"], inplace=True)\n",
    "\n",
    "    # write out\n",
    "    out_path = os.path.join(OUTPUT_DIR, fn)\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(\" → wrote\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c77056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c94edd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86c59ffb",
   "metadata": {},
   "source": [
    "or maybe augment historic and predicted at the same time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47d5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# assume weather_dfs is your dict of historic station‑interpolated DataFrames\n",
    "\n",
    "preds_ds = xr.open_zarr(PRED_STORE, chunks={\"time\":1})\n",
    "\n",
    "for fn in os.listdir(ARR_DIR):\n",
    "    if not fn.lower().endswith(\".csv\"): \n",
    "        continue\n",
    "\n",
    "    # 1) load the arrival + historic columns\n",
    "    df = pd.read_csv(os.path.join(ARR_DIR, fn), parse_dates=[\"FlightDate\"], dtype=str)\n",
    "    df[\"DepDT\"] = df[\"DepTime\"].apply(parse_hhmm) + df[\"FlightDate\"]\n",
    "    df[\"ArrDT\"] = df[\"ArrTime\"].apply(parse_hhmm) + df[\"FlightDate\"]\n",
    "\n",
    "    # 2) initialize predicted‐weather columns\n",
    "    for var in task_config.target_variables:\n",
    "        df[f\"Origin_pred_{var}\"] = np.nan\n",
    "        df[f\"Dest_pred_{var}\"]   = np.nan\n",
    "\n",
    "    # 3) fill both historic *and* predicted\n",
    "    for i, row in df.iterrows():\n",
    "        for side, dtcol, apcol in [\n",
    "            (\"Origin\",\"DepDT\",\"OriginAirportID\"),\n",
    "            (\"Dest\",  \"ArrDT\",\"DestAirportID\")\n",
    "        ]:\n",
    "            dt  = row[dtcol]\n",
    "            aid = str(row[apcol]).strip()\n",
    "\n",
    "            # (a) historic lookup (as before)\n",
    "            if not pd.isna(dt) and aid in weather_dfs:\n",
    "                rec = weather_dfs[aid].iloc[\n",
    "                    weather_dfs[aid].index.get_indexer([dt], method=\"nearest\")[0]\n",
    "                ]\n",
    "                for feat in FEATURES:\n",
    "                    df.at[i, f\"{side}_{feat}\"] = rec[feat]\n",
    "\n",
    "            # (b) predicted lookup\n",
    "            if not pd.isna(dt) and aid in mc.index:\n",
    "                lat, lon = mc.at[aid, \"LAT\"], mc.at[aid, \"LON\"]\n",
    "                for var in task_config.target_variables:\n",
    "                    try:\n",
    "                        val = preds_ds[var].sel(\n",
    "                            time=dt, method=\"nearest\",\n",
    "                            lat=lat, method=\"nearest\",\n",
    "                            lon=lon, method=\"nearest\"\n",
    "                        ).item()\n",
    "                    except KeyError:\n",
    "                        val = np.nan\n",
    "                    df.at[i, f\"{side}_pred_{var}\"] = val\n",
    "\n",
    "    # 4) clean up & save\n",
    "    df.drop(columns=[\"DepDT\",\"ArrDT\"], inplace=True)\n",
    "    df.dropna(axis=1, how=\"all\", inplace=True)\n",
    "    out = os.path.join(OUT_DIR, fn)\n",
    "    df.to_csv(out, index=False)\n",
    "    print(\"Saved both historic + predicted →\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45ae254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
