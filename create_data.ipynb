{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e66885f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.conda\\lib\\site-packages\\gcsfs\\core.py:313: UserWarning: GCS project not set - cannot list or create buckets\n",
      "  warnings.warn(\"GCS project not set - cannot list or create buckets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains time from 2020-01-01T00:00:00.000000000 to 2022-12-31T18:00:00.000000000\n",
      "US 2020-2022 0.25 degree ZArr → ERA5_2020-2022_6h_5VAR_0.25.zarr\n",
      "US 2020-2022 0.25 degree NetCDF4 → ERA5_2020-2022_6h_5VAR_0.25.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import gcsfs\n",
    "import numpy as np\n",
    "\n",
    "# CONFIG\n",
    "GCS_URI_6H_13L = \"gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr\"\n",
    "OUT_6H_13L_ZARR = \"Datasets/ERA5_2020-2022_6h_5VAR_0.25.zarr\"\n",
    "\n",
    "# set start and end dates for data range. yyyy-mm-dd\n",
    "start = \"2020-01-01\"\n",
    "end = \"2022-12-31\"\n",
    "\n",
    "VARS = [\n",
    "    \"2m_temperature\",\n",
    "    \"mean_sea_level_pressure\",\n",
    "    \"10m_u_component_of_wind\",\n",
    "    \"10m_v_component_of_wind\",\n",
    "    \"total_precipitation_6hr\"\n",
    "]\n",
    "\n",
    "def open_gcs_zarr(uri: str, project: str = None):\n",
    "    fs = gcsfs.GCSFileSystem(project=project)\n",
    "    return xr.open_zarr(fs.get_mapper(uri), consolidated=True)\n",
    "\n",
    "\n",
    "\n",
    "# load & time‐subset 6h/13l for 2020-2022\n",
    "ds = open_gcs_zarr(GCS_URI_6H_13L)\n",
    "ds2020_22 = ds.sel(time=slice(start, end))\n",
    "\n",
    "# keep only the five surface variables\n",
    "ds2020_22 = ds2020_22[VARS]\n",
    "\n",
    "times = ds2020_22.time.values\n",
    "start = np.min(times)\n",
    "end   = np.max(times)\n",
    "print(f\"Dataset contains time from {start} to {end}\")\n",
    "\n",
    "lon_coord = \"longitude\"    if \"longitude\"    in ds2020_22.coords else \"lon\"\n",
    "lat_coord = \"latitude\"     if \"latitude\"     in ds2020_22.coords else \"lat\"\n",
    "\n",
    "# only save US subset\n",
    "us_lon_min, us_lon_max = 235.0, 294.0   # [0–360] U.S. longitudes\n",
    "us_lat_min, us_lat_max =  24.0,  50.0   # U.S. latitudes\n",
    "ds2020_22 = ( ds2020_22.sortby(lon_coord)\n",
    "    .sortby(lat_coord)\n",
    "    .sel({lon_coord: slice(us_lon_min, us_lon_max), lat_coord: slice(us_lat_min, us_lat_max)}))\n",
    "\n",
    "\n",
    "# write out the U.S. subsets\n",
    "ds2020_22.to_zarr(OUT_6H_13L_ZARR, mode=\"w\")\n",
    "print(\"US 2020-2022 0.25 degree ZArr →\", OUT_6H_13L_ZARR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa6679",
   "metadata": {},
   "source": [
    "If you want to download the weather data zarr from drive, set weather_down = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00608e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting beautifulsoup4 (from gdown)\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ebror\\onedrive\\documents\\github\\project\\.conda\\lib\\site-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\ebror\\onedrive\\documents\\github\\project\\.conda\\lib\\site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ebror\\onedrive\\documents\\github\\project\\.conda\\lib\\site-packages (from gdown) (4.67.1)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->gdown)\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\ebror\\onedrive\\documents\\github\\project\\.conda\\lib\\site-packages (from beautifulsoup4->gdown) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ebror\\onedrive\\documents\\github\\project\\.conda\\lib\\site-packages (from requests[socks]->gdown) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ebror\\onedrive\\documents\\github\\project\\.conda\\lib\\site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ebror\\onedrive\\documents\\github\\project\\.conda\\lib\\site-packages (from requests[socks]->gdown) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ebror\\onedrive\\documents\\github\\project\\.conda\\lib\\site-packages (from requests[socks]->gdown) (2025.1.31)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ebror\\onedrive\\documents\\github\\project\\.conda\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, PySocks, beautifulsoup4, gdown\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.13.4 gdown-5.2.0 soupsieve-2.7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import gdown\n",
    "\n",
    "weather_down = True\n",
    "\n",
    "arrival_url = \"https://drive.google.com/drive/folders/1iuhGZFFPtB_2lBNFtX20IS9U6ZaSk51a?usp=sharing\"\n",
    "DOWNLOAD_DIR = \"./Datasets/Arrival_Statistics/\"\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cc39a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading all files from:\n",
      "  https://drive.google.com/drive/folders/1iuhGZFFPtB_2lBNFtX20IS9U6ZaSk51a?usp=sharing\n",
      "→ into ./Datasets/Arrival_Statistics//\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1WBUyDUvYDmmfM1vDxbcnS-wny6QsxFCZ 2020-2022.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1WBUyDUvYDmmfM1vDxbcnS-wny6QsxFCZ\n",
      "From (redirected): https://drive.google.com/uc?id=1WBUyDUvYDmmfM1vDxbcnS-wny6QsxFCZ&confirm=t&uuid=896a992c-514e-4a6e-bdb1-dbb29ca7298d\n",
      "To: c:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\Datasets\\Arrival_Statistics\\2020-2022.zip\n",
      "100%|██████████| 831M/831M [00:22<00:00, 37.7MB/s] \n",
      "Download completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for arrival csv .zip files to extract…\n",
      "\n",
      "Extracting 2020-2022.zip → ./Datasets/Arrival_Statistics//\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1ZUqpBIOUPNWE0Z1ezJi8GH73SRDUIzWE ERA5_2020-2022_6h_5VAR_0.25.zarr.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1ZUqpBIOUPNWE0Z1ezJi8GH73SRDUIzWE\n",
      "From (redirected): https://drive.google.com/uc?id=1ZUqpBIOUPNWE0Z1ezJi8GH73SRDUIzWE&confirm=t&uuid=28bf172c-c494-4ac9-8c4f-ca37ab6d7bb2\n",
      "To: c:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\ERA5_2020-2022_6h_5VAR_0.25.zarr.zip\n",
      "100%|██████████| 1.42G/1.42G [00:36<00:00, 39.0MB/s]\n",
      "Download completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for weather zarr .zip files to extract…\n",
      "\n",
      "Extracting ERA5_2020-2022_6h_5VAR_0.25.zarr.zip → ./\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nDownloading all files from:\\n  {arrival_url}\\n→ into {DOWNLOAD_DIR}/\\n\")\n",
    "gdown.download_folder(\n",
    "    url=arrival_url,\n",
    "    output=DOWNLOAD_DIR,\n",
    "    quiet=False,\n",
    "    use_cookies=False,\n",
    ")\n",
    "\n",
    "print(\"\\nLooking for arrival csv .zip files to extract…\\n\")\n",
    "for zippath in glob.glob(os.path.join(DOWNLOAD_DIR, \"*.zip\")):\n",
    "    print(f\"Extracting {os.path.basename(zippath)} → {DOWNLOAD_DIR}/\")\n",
    "    with zipfile.ZipFile(zippath, \"r\") as zf:\n",
    "        zf.extractall(DOWNLOAD_DIR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if weather_down == True:\n",
    "    weather_url = \"https://drive.google.com/drive/folders/1mQXRMo2jLqG3Zc1I4NvOaDNc6uheEBJN?usp=sharing\"\n",
    "    gdown.download_folder(\n",
    "        url=weather_url,\n",
    "        output=\"Datasets/\",\n",
    "        quiet=False,\n",
    "        use_cookies=False,\n",
    "    )\n",
    "    print(\"\\nLooking for weather zarr .zip files to extract…\\n\")\n",
    "    for zippath in glob.glob(os.path.join(\"Datasets/\", \"*.zip\")):\n",
    "        print(f\"Extracting {os.path.basename(zippath)} → ./\")\n",
    "        with zipfile.ZipFile(zippath, \"r\") as zf:\n",
    "            zf.extractall(\"Datasets/\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nDone!\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a107b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ERA5 DATASET SUMMARY ===\n",
      "<xarray.Dataset> Size: 2GB\n",
      "Dimensions:                  (time: 4384, latitude: 105, longitude: 237)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 420B 24.0 24.25 ... 49.75 50.0\n",
      "  * longitude                (longitude) float32 948B 235.0 235.2 ... 294.0\n",
      "  * time                     (time) datetime64[ns] 35kB 2020-01-01 ... 2022-1...\n",
      "Data variables:\n",
      "    10m_u_component_of_wind  (time, latitude, longitude) float32 436MB dask.array<chunksize=(1, 105, 237), meta=np.ndarray>\n",
      "    10m_v_component_of_wind  (time, latitude, longitude) float32 436MB dask.array<chunksize=(1, 105, 237), meta=np.ndarray>\n",
      "    2m_temperature           (time, latitude, longitude) float32 436MB dask.array<chunksize=(1, 105, 237), meta=np.ndarray>\n",
      "    mean_sea_level_pressure  (time, latitude, longitude) float32 436MB dask.array<chunksize=(1, 105, 237), meta=np.ndarray>\n",
      "    total_precipitation_6hr  (time, latitude, longitude) float32 436MB dask.array<chunksize=(1, 105, 237), meta=np.ndarray>\n",
      "\n",
      "Variables in ds.data_vars:\n",
      " • 10m_u_component_of_wind        dims=('time', 'latitude', 'longitude')   shape=(4384, 105, 237)\n",
      " • 10m_v_component_of_wind        dims=('time', 'latitude', 'longitude')   shape=(4384, 105, 237)\n",
      " • 2m_temperature                 dims=('time', 'latitude', 'longitude')   shape=(4384, 105, 237)\n",
      " • mean_sea_level_pressure        dims=('time', 'latitude', 'longitude')   shape=(4384, 105, 237)\n",
      " • total_precipitation_6hr        dims=('time', 'latitude', 'longitude')   shape=(4384, 105, 237)\n",
      "\n",
      "Coordinates in ds.coords:\n",
      " • latitude = [24.   24.25 24.5  24.75 25.  ] …\n",
      " • longitude = [235.   235.25 235.5  235.75 236.  ] …\n",
      " • time = ['2020-01-01T00:00:00.000000000' '2020-01-01T06:00:00.000000000'\n",
      " '2020-01-01T12:00:00.000000000' '2020-01-01T18:00:00.000000000'\n",
      " '2020-01-02T00:00:00.000000000'] …\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# Inspect the ERA5 dataset \n",
    "ERA5_STORE = \"Datasets/ERA5_2020-2022_6h_5VAR_0.25.zarr\"\n",
    "ds = xr.open_zarr(ERA5_STORE)\n",
    "\n",
    "print(\"=== ERA5 DATASET SUMMARY ===\")\n",
    "print(ds)\n",
    "print(\"\\nVariables in ds.data_vars:\")\n",
    "for name, var in ds.data_vars.items():\n",
    "    print(f\" • {name:30s} dims={var.dims}   shape={tuple(var.shape)}\")\n",
    "\n",
    "print(\"\\nCoordinates in ds.coords:\")\n",
    "for coord in ds.coords:\n",
    "    print(\" •\", coord, \"=\", ds.coords[coord].values[:5], \"…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf520d4",
   "metadata": {},
   "source": [
    "augment arrival data with weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e20363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dims before subsetting: FrozenMappingWarningOnValuesAccess({'time': 4384, 'lat': 105, 'lon': 237})\n"
     ]
    }
   ],
   "source": [
    "# CONFIG\n",
    "ERA5_STORE   = \"Datasets/ERA5_2020-2022_6h_5VAR_0.25.zarr\"\n",
    "MASTER_COORD = \"Datasets/T_MASTER_CORD.csv\"\n",
    "ARRIVAL_DIR  = \"Datasets/Arrival_Statistics/2020-2022\"\n",
    "OUTPUT_DIR   = \"Datasets/Arrival_With_Weather/2020-2022\"\n",
    "\n",
    "VARS = [\n",
    "    \"2m_temperature\",\n",
    "    \"mean_sea_level_pressure\",\n",
    "    \"10m_u_component_of_wind\",\n",
    "    \"10m_v_component_of_wind\",\n",
    "    \"total_precipitation_6hr\"\n",
    "]\n",
    "\n",
    "\n",
    "os.makedirs(ARRIVAL_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ─── 1) load ERA5 & rename dims ───────────────────────────────────────────────\n",
    "ds = xr.open_zarr(ERA5_STORE, consolidated=True)\n",
    "\n",
    "if \"latitude\" in ds.dims and \"longitude\" in ds.dims:\n",
    "    ds = ds.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n",
    "\n",
    "print(\"dims before subsetting:\", ds.dims)\n",
    "\n",
    "\n",
    "# ─── 2) build your fast lookup arrays ────────────────────────────────────────\n",
    "time_index = ds[\"time\"].to_index()\n",
    "lat_vals    = ds[\"lat\"].values\n",
    "lon_vals    = ds[\"lon\"].values\n",
    "var_arrays = { var: ds[var].values for var in VARS }\n",
    "\n",
    "\n",
    "# ─── 3) load master coords ───────────────────────────────────────────────────\n",
    "mc = (\n",
    "    pd.read_csv(MASTER_COORD, dtype=str)\n",
    "      .set_index(\"AIRPORT_SEQ_ID\")[[\"LATITUDE\",\"LONGITUDE\"]]\n",
    "      .astype(float)\n",
    ")\n",
    "\n",
    "# ─── 4) helpers ─────────────────────────────────────────────\n",
    "def parse_hhmm(x):\n",
    "    if pd.isna(x) or not str(x).strip():\n",
    "        return pd.NaT\n",
    "    s = str(int(float(x))).zfill(4)\n",
    "    return pd.Timedelta(hours=int(s[:2]), minutes=int(s[2:]))\n",
    "\n",
    "\n",
    "\n",
    "# ─── then your make_weather_lookup becomes ─────────────────────────────────\n",
    "def make_weather_lookup(keys: pd.DataFrame) -> pd.DataFrame:\n",
    "    sub = keys.copy()\n",
    "    sub[\"lat\"] = sub[\"AirportSeqID\"].map(mc[\"LATITUDE\"])\n",
    "    sub[\"lon\"] = sub[\"AirportSeqID\"].map(mc[\"LONGITUDE\"]) % 360.0\n",
    "    \n",
    "    \n",
    "    if sub.empty:\n",
    "        return pd.DataFrame(columns=VARS,\n",
    "                            index=pd.MultiIndex.from_arrays([[],[]],\n",
    "                                                           names=[\"AirportSeqID\",\"Datetime\"]))\n",
    "\n",
    "    # nearest‐time\n",
    "    t_idx = time_index.get_indexer(sub[\"Datetime\"], method=\"nearest\")\n",
    "\n",
    "    # nearest‐lat/lon\n",
    "    sub_lat = sub[\"lat\"].to_numpy()\n",
    "    sub_lon = sub[\"lon\"].to_numpy()\n",
    "    l_idx = np.abs(lat_vals[None,:] - sub_lat[:,None]).argmin(axis=1)\n",
    "    o_idx = np.abs(lon_vals[None,:] - sub_lon[:,None]).argmin(axis=1)\n",
    "\n",
    "    # pull out each VAR from our preloaded var_arrays\n",
    "    out = {}\n",
    "    for var in VARS:\n",
    "        arr = var_arrays[var]   # pure numpy now\n",
    "        out[var] = arr[t_idx, l_idx, o_idx]\n",
    "\n",
    "    # assemble a DataFrame and re‑index\n",
    "    df_lkp = pd.DataFrame(out, index=sub.index)\n",
    "    df_lkp.index = pd.MultiIndex.from_frame(\n",
    "        sub[[\"AirportSeqID\",\"Datetime\"]],\n",
    "        names=[\"AirportSeqID\",\"Datetime\"]\n",
    "    )\n",
    "    return df_lkp\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657fd9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_1.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_1.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_10.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_10.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_11.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_11.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_12.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_12.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_2.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_2.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_3.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_3.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_4.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_4.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_5.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_5.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_6.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_6.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_7.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_7.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_8.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_8.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_9.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_9.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_1.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_1.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_10.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_10.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_11.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_11.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_12.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_12.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_2.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_2.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_3.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_3.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_4.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_4.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_5.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_5.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_6.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_6.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_7.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_7.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_8.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_8.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_9.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2021_9.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_1.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_1.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_10.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_10.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_11.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_11.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_12.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_12.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_2.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_2.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_3.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_3.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_4.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_4.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_5.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_5.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_6.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_6.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_7.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_7.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_8.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_8.csv\n",
      "→ augmenting On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_9.csv\n",
      "   saved → Datasets/Arrival_With_Weather/2020-2022\\On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_9.csv\n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"future.no_silent_downcasting\", True)\n",
    "for fn in sorted(os.listdir(ARRIVAL_DIR)):\n",
    "    if not fn.lower().endswith(\".csv\"):\n",
    "        continue\n",
    "    print(\"→ augmenting\", fn)\n",
    "    df = pd.read_csv(os.path.join(ARRIVAL_DIR, fn), dtype=str)\n",
    "\n",
    "    # build timestamps\n",
    "    df[\"FlightDate\"]   = pd.to_datetime(df[\"FlightDate\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "    df[\"DepDelta\"]     = df[\"DepTime\"].apply(parse_hhmm)\n",
    "    df[\"ArrDelta\"]     = df[\"ArrTime\"].apply(parse_hhmm)\n",
    "    df[\"DepDatetime\"]  = df[\"FlightDate\"] + df[\"DepDelta\"]\n",
    "    df[\"ArrDatetime\"]  = df[\"FlightDate\"] + df[\"ArrDelta\"]\n",
    "\n",
    "    # origin & dest lookups\n",
    "    orig = ( df[[\"OriginAirportSeqID\",\"DepDatetime\"]]\n",
    "             .dropna().drop_duplicates()\n",
    "             .rename(columns={\"OriginAirportSeqID\":\"AirportSeqID\",\"DepDatetime\":\"Datetime\"}) )\n",
    "    dest = ( df[[\"DestAirportSeqID\",\"ArrDatetime\"]]\n",
    "             .dropna().drop_duplicates()\n",
    "             .rename(columns={\"DestAirportSeqID\":\"AirportSeqID\",\"ArrDatetime\":\"Datetime\"}) )\n",
    "\n",
    "    orig_lkp  = make_weather_lookup(orig).rename(columns=lambda c: f\"Origin_{c}\")\n",
    "    dest_lkp  = make_weather_lookup(dest).rename(columns=lambda c: f\"Dest_{c}\")\n",
    "\n",
    "\n",
    "    # merge everything back\n",
    "    df = (df\n",
    "          .merge(orig_lkp,  left_on=[\"OriginAirportSeqID\",\"DepDatetime\"], right_index=True, how=\"left\")\n",
    "          .merge(dest_lkp,  left_on=[\"DestAirportSeqID\",\"ArrDatetime\"],   right_index=True, how=\"left\")\n",
    "        )\n",
    "\n",
    "    # cleanup & save\n",
    "    df.drop(columns=[\"DepDelta\",\"ArrDelta\",\"DepDatetime\",\"ArrDatetime\"], errors=\"ignore\", inplace=True)\n",
    "    df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "    df.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "    out = os.path.join(OUTPUT_DIR, fn)\n",
    "    df.to_csv(out, index=False)\n",
    "    print(\"   saved →\", out)\n",
    "    \n",
    "os.makedirs(\"./data/old_data/\", exist_ok=True)\n",
    "os.makedirs(\"./models/results/figures/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a834100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebror\\AppData\\Local\\Temp\\ipykernel_19340\\3669659073.py:5: DtypeWarning: Columns (48,69,76,77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(ARRIVAL_DIR, fn), parse_dates=[\"FlightDate\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SAMPLE ARRIVAL CSV: On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2020_1.csv ===\n",
      "\n",
      "Columns: ['Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek', 'FlightDate', 'Reporting_Airline', 'DOT_ID_Reporting_Airline', 'IATA_CODE_Reporting_Airline', 'Tail_Number', 'Flight_Number_Reporting_Airline', 'OriginAirportID', 'OriginAirportSeqID', 'OriginCityMarketID', 'Origin', 'OriginCityName', 'OriginState', 'OriginStateFips', 'OriginStateName', 'OriginWac', 'DestAirportID', 'DestAirportSeqID', 'DestCityMarketID', 'Dest', 'DestCityName', 'DestState', 'DestStateFips', 'DestStateName', 'DestWac', 'CRSDepTime', 'DepTime', 'DepDelay', 'DepDelayMinutes', 'DepDel15', 'DepartureDelayGroups', 'DepTimeBlk', 'TaxiOut', 'WheelsOff', 'WheelsOn', 'TaxiIn', 'CRSArrTime', 'ArrTime', 'ArrDelay', 'ArrDelayMinutes', 'ArrDel15', 'ArrivalDelayGroups', 'ArrTimeBlk', 'Cancelled', 'CancellationCode', 'Diverted', 'CRSElapsedTime', 'ActualElapsedTime', 'AirTime', 'Flights', 'Distance', 'DistanceGroup', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay', 'FirstDepTime', 'TotalAddGTime', 'LongestAddGTime', 'DivAirportLandings', 'DivReachedDest', 'DivActualElapsedTime', 'DivArrDelay', 'DivDistance', 'Div1Airport', 'Div1AirportID', 'Div1AirportSeqID', 'Div1WheelsOn', 'Div1TotalGTime', 'Div1LongestGTime', 'Div1WheelsOff', 'Div1TailNum', 'Div2Airport', 'Div2AirportID', 'Div2AirportSeqID', 'Div2WheelsOn', 'Div2TotalGTime', 'Div2LongestGTime', 'Div2WheelsOff', 'Div2TailNum', 'Origin_2m_temperature', 'Origin_mean_sea_level_pressure', 'Origin_10m_u_component_of_wind', 'Origin_10m_v_component_of_wind', 'Origin_total_precipitation_6hr', 'Dest_2m_temperature', 'Dest_mean_sea_level_pressure', 'Dest_10m_u_component_of_wind', 'Dest_10m_v_component_of_wind', 'Dest_total_precipitation_6hr']\n",
      "\n",
      "First 5 rows:\n",
      "   Year  Quarter  Month  DayofMonth  DayOfWeek FlightDate Reporting_Airline  \\\n",
      "0  2020        1      1          17          5 2020-01-17                B6   \n",
      "1  2020        1      1          18          6 2020-01-18                B6   \n",
      "2  2020        1      1          19          7 2020-01-19                B6   \n",
      "3  2020        1      1          20          1 2020-01-20                B6   \n",
      "4  2020        1      1          21          2 2020-01-21                B6   \n",
      "\n",
      "   DOT_ID_Reporting_Airline IATA_CODE_Reporting_Airline Tail_Number  ...  \\\n",
      "0                     20409                          B6      N661JB  ...   \n",
      "1                     20409                          B6      N584JB  ...   \n",
      "2                     20409                          B6      N662JB  ...   \n",
      "3                     20409                          B6      N784JB  ...   \n",
      "4                     20409                          B6      N784JB  ...   \n",
      "\n",
      "   Origin_2m_temperature  Origin_mean_sea_level_pressure  \\\n",
      "0              297.41843                      102555.180   \n",
      "1              296.14220                      102541.640   \n",
      "2              298.19740                      101954.280   \n",
      "3              293.29083                      101960.960   \n",
      "4              290.89350                      101853.195   \n",
      "\n",
      "   Origin_10m_u_component_of_wind  Origin_10m_v_component_of_wind  \\\n",
      "0                       -7.572047                       -4.783078   \n",
      "1                       -6.024319                        3.832024   \n",
      "2                        1.425548                        3.954334   \n",
      "3                        1.265657                       -6.558300   \n",
      "4                        5.317981                       -7.284566   \n",
      "\n",
      "  Origin_total_precipitation_6hr Dest_2m_temperature  \\\n",
      "0                   6.001405e-04           264.76760   \n",
      "1                   3.824383e-05           267.59805   \n",
      "2                  -1.117587e-08           272.92520   \n",
      "3                   1.746695e-04           265.67078   \n",
      "4                   3.297813e-05           261.80753   \n",
      "\n",
      "  Dest_mean_sea_level_pressure  Dest_10m_u_component_of_wind  \\\n",
      "0                    103905.90                      2.523050   \n",
      "1                    101774.24                     -1.546868   \n",
      "2                    101062.31                      3.529461   \n",
      "3                    102721.19                      2.869700   \n",
      "4                    103107.73                      2.684846   \n",
      "\n",
      "  Dest_10m_v_component_of_wind  Dest_total_precipitation_6hr  \n",
      "0                    -2.797639                 -1.862645e-09  \n",
      "1                     3.089462                  5.056867e-03  \n",
      "2                    -2.081129                  6.651506e-06  \n",
      "3                    -2.142799                 -9.313226e-09  \n",
      "4                    -1.938694                  0.000000e+00  \n",
      "\n",
      "[5 rows x 95 columns]\n"
     ]
    }
   ],
   "source": [
    "# Inspect one of the cleaned arrivals CSVs\n",
    "ARRIVAL_DIR = \"Datasets/Arrival_With_Weather/2020-2022\"\n",
    "# pick the first CSV in the folder\n",
    "fn = sorted([f for f in os.listdir(ARRIVAL_DIR) if f.lower().endswith(\".csv\")])[0]\n",
    "df = pd.read_csv(os.path.join(ARRIVAL_DIR, fn), parse_dates=[\"FlightDate\"])\n",
    "\n",
    "print(\"\\n=== SAMPLE ARRIVAL CSV:\", fn, \"===\\n\")\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f535d20",
   "metadata": {},
   "source": [
    "Now run preprocessing to merge and format the arrival data for training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
