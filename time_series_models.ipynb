{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Bidirectional, Conv1D, MaxPooling1D, Flatten, Concatenate, TimeDistributed, Reshape, Activation, Lambda, Layer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug = pd.read_csv(\"./data/X_train_lstm.csv\")\n",
    "X_test_aug = pd.read_csv(\"./data/X_test_lstm.csv\")\n",
    "\n",
    "Y_train_aug = pd.read_csv(\"./data/Y_train_lstm.csv\")\n",
    "Y_test_aug = pd.read_csv(\"./data/Y_test_lstm.csv\")\n",
    "\n",
    "# Only using total delay\n",
    "Y_train_aug = Y_train_aug[['ARR_DELAY']]\n",
    "Y_test_aug = Y_test_aug[['ARR_DELAY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"./data/old_data/X_train_lstm.csv\")\n",
    "X_test = pd.read_csv(\"./data/old_data/X_test_lstm.csv\")\n",
    "\n",
    "Y_train = pd.read_csv(\"./data/old_data/Y_train_lstm.csv\")\n",
    "Y_test = pd.read_csv(\"./data/old_data/Y_test_lstm.csv\")\n",
    "\n",
    "# Only using total delay\n",
    "Y_train = Y_train[['ARR_DELAY']]\n",
    "Y_test = Y_test[['ARR_DELAY']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m10882/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1640.3092 - mae: 21.1990\n",
      "Epoch 1: val_loss improved from inf to 828.77429, saving model to ./models/lstm_model_aug.keras\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 29ms/step - loss: 1640.6317 - mae: 21.1990 - val_loss: 828.7743 - val_mae: 19.4323\n",
      "Epoch 2/50\n",
      "\u001b[1m10881/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 3444.7246 - mae: 21.4990\n",
      "Epoch 2: val_loss improved from 828.77429 to 481.08832, saving model to ./models/lstm_model_aug.keras\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 26ms/step - loss: 3444.2175 - mae: 21.4982 - val_loss: 481.0883 - val_mae: 17.1260\n",
      "Epoch 3/50\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 755497.0000 - mae: 67.8682\n",
      "Epoch 3: val_loss did not improve from 481.08832\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 25ms/step - loss: 755489.6250 - mae: 67.8682 - val_loss: 542.9799 - val_mae: 16.3463\n",
      "Epoch 4/50\n",
      "\u001b[1m10881/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 294254.0938 - mae: 48.6834\n",
      "Epoch 4: val_loss did not improve from 481.08832\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 24ms/step - loss: 294333.2500 - mae: 48.6864 - val_loss: 568.6693 - val_mae: 16.3542\n",
      "Epoch 5/50\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 24388.4375 - mae: 27.1823\n",
      "Epoch 5: val_loss did not improve from 481.08832\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 25ms/step - loss: 24388.8789 - mae: 27.1821 - val_loss: 509.6695 - val_mae: 16.5938\n",
      "Epoch 6/50\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 776851.0000 - mae: 73.6863\n",
      "Epoch 6: val_loss did not improve from 481.08832\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 24ms/step - loss: 776834.0625 - mae: 73.6845 - val_loss: 503.3231 - val_mae: 16.4104\n",
      "Epoch 7/50\n",
      "\u001b[1m10881/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 670840.7500 - mae: 36.2881\n",
      "Epoch 7: val_loss did not improve from 481.08832\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 23ms/step - loss: 670827.7500 - mae: 36.2898 - val_loss: 535.3212 - val_mae: 18.4925\n",
      "Epoch 7: early stopping\n",
      "\u001b[1m37549/37549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 3ms/step - loss: 548.7997 - mae: 18.7043\n",
      "Test Mean Absolute Error: 18.459857940673828\n",
      "\u001b[1m37549/37549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 2ms/step\n",
      "Mean Absolute Error for each column:\n",
      "18.460128784179688\n"
     ]
    }
   ],
   "source": [
    "# Defining callbacks\n",
    "checkpoint_aug = ModelCheckpoint(\"./models/lstm_model_aug.keras\", monitor='val_loss', save_best_only=True, verbose=1, save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define LSTM model\n",
    "# 2 layers of LSTM and number of hidden units were hand tuned\n",
    "lstm_model_aug = Sequential([\n",
    "    Input(shape=(X_train_aug.shape[1], 1)),\n",
    "    LSTM(units=64, activation='relu', recurrent_dropout=0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Use MSE for loss because we want to emphasize the \"wrongest\" guesses the most. MAE is an interpretable metric\n",
    "lstm_model_aug.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model w/ early stopping\n",
    "history_aug = lstm_model_aug.fit(X_train_aug, Y_train_aug, epochs=50, batch_size=265, validation_split=0.2, callbacks=[checkpoint_aug, early_stopping])\n",
    "\n",
    "\n",
    "loss, mae = lstm_model_aug.evaluate(X_test_aug, Y_test_aug)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred_aug = lstm_model_aug.predict(X_test_aug)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test_aug, Y_pred_aug)\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)\n",
    "\n",
    "pd.DataFrame(history_aug.history).to_csv(\"./models/results/lstm_test_aug.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m10881/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 449.2684 - mae: 16.0787\n",
      "Epoch 1: val_loss improved from inf to 447.40070, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 20ms/step - loss: 449.2621 - mae: 16.0786 - val_loss: 447.4007 - val_mae: 16.1171\n",
      "Epoch 2/50\n",
      "\u001b[1m10880/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 409.4044 - mae: 15.3388\n",
      "Epoch 2: val_loss did not improve from 447.40070\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 17ms/step - loss: 409.4041 - mae: 15.3388 - val_loss: 448.2163 - val_mae: 16.1392\n",
      "Epoch 3/50\n",
      "\u001b[1m10882/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 406.1967 - mae: 15.2418\n",
      "Epoch 3: val_loss did not improve from 447.40070\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 17ms/step - loss: 406.1965 - mae: 15.2418 - val_loss: 448.1986 - val_mae: 15.9978\n",
      "Epoch 4/50\n",
      "\u001b[1m10881/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 404.2705 - mae: 15.1895\n",
      "Epoch 4: val_loss did not improve from 447.40070\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 17ms/step - loss: 404.2704 - mae: 15.1895 - val_loss: 449.8622 - val_mae: 15.9220\n",
      "Epoch 5/50\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 402.5240 - mae: 15.1463\n",
      "Epoch 5: val_loss did not improve from 447.40070\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 17ms/step - loss: 402.5240 - mae: 15.1463 - val_loss: 451.0342 - val_mae: 15.8428\n",
      "Epoch 6/50\n",
      "\u001b[1m10882/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 405.5446 - mae: 15.1658\n",
      "Epoch 6: val_loss did not improve from 447.40070\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 18ms/step - loss: 405.5465 - mae: 15.1658 - val_loss: 458.8492 - val_mae: 16.0689\n",
      "Epoch 6: early stopping\n",
      "\u001b[1m37549/37549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 2ms/step - loss: 477.0460 - mae: 16.3935\n",
      "Test Mean Absolute Error: 16.03424072265625\n",
      "\u001b[1m37549/37549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2ms/step\n",
      "Mean Absolute Error for each column:\n",
      "16.03421401977539\n"
     ]
    }
   ],
   "source": [
    "# Defining callbacks\n",
    "checkpoint = ModelCheckpoint(\"./models/lstm_model.keras\", monitor='val_loss', save_best_only=True, verbose=1, save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define LSTM model\n",
    "# Up to 2 layers of LSTM and number of hidden units were hand tuned to determine this as the optimum model\n",
    "lstm_model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], 1)),\n",
    "    LSTM(units=64, activation='relu', recurrent_dropout=0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Use MSE for loss because we want to emphasize the \"wrongest\" guesses the most. MAE is an interpretable metric\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model w/ early stopping\n",
    "# Batch size is the average number of flights per day\n",
    "history = lstm_model.fit(X_train, Y_train, epochs=50, batch_size=265, validation_split=0.2, callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "\n",
    "loss, mae = lstm_model.evaluate(X_test, Y_test)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred = lstm_model.predict(X_test)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test, Y_pred)#, multioutput='raw_values')\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)\n",
    "\n",
    "pd.DataFrame(history.history).to_csv(\"./models/results/lstm_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m10882/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 641.9301 - mae: 17.5752\n",
      "Epoch 1: val_loss improved from inf to 448.36588, saving model to ./models/bilstm_model_aug.keras\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 36ms/step - loss: 641.9041 - mae: 17.5749 - val_loss: 448.3659 - val_mae: 16.4557\n",
      "Epoch 2/50\n",
      "\u001b[1m10881/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 499.4506 - mae: 16.1421\n",
      "Epoch 2: val_loss did not improve from 448.36588\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 31ms/step - loss: 499.5045 - mae: 16.1422 - val_loss: 452.3696 - val_mae: 15.8583\n",
      "Epoch 3/50\n",
      "\u001b[1m10881/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 18286.4531 - mae: 26.6757\n",
      "Epoch 3: val_loss did not improve from 448.36588\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 30ms/step - loss: 18298.2969 - mae: 26.6771 - val_loss: 2393.3933 - val_mae: 41.6816\n",
      "Epoch 4/50\n",
      "\u001b[1m10881/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 51445.6211 - mae: 37.2663\n",
      "Epoch 4: val_loss did not improve from 448.36588\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 31ms/step - loss: 51447.6641 - mae: 37.2656 - val_loss: 568.9623 - val_mae: 16.5754\n",
      "Epoch 5/50\n",
      "\u001b[1m10881/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 154416.5312 - mae: 55.6439\n",
      "Epoch 5: val_loss did not improve from 448.36588\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 31ms/step - loss: 154400.4219 - mae: 55.6403 - val_loss: 524.3457 - val_mae: 17.1987\n",
      "Epoch 6/50\n",
      "\u001b[1m 1591/10883\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:32\u001b[0m 29ms/step - loss: 34575.9141 - mae: 31.2875"
     ]
    }
   ],
   "source": [
    "# Defining callbacks\n",
    "checkpoint_aug = ModelCheckpoint(\"./models/bilstm_model_aug.keras\", monitor='val_loss', save_best_only=True, verbose=1, save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define LSTM model\n",
    "# Up to 2 layers of LSTM and number of hidden units were hand tuned to determine this as the optimum model\n",
    "bilstm_model_aug = Sequential([\n",
    "    Input(shape=(X_train.shape[1], 1)),\n",
    "    Bidirectional(\n",
    "        LSTM(units=64, activation='relu', recurrent_dropout=0.2)\n",
    "    ),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Use MSE for loss because we want to emphasize the \"wrongest\" guesses the most. MAE is an interpretable metric\n",
    "bilstm_model_aug.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model w/ early stopping\n",
    "# Batch size is the average number of flights per day\n",
    "history_aug = bilstm_model_aug.fit(X_train_aug, Y_train_aug, epochs=50, batch_size=265, validation_split=0.2, callbacks=[checkpoint_aug, early_stopping])\n",
    "\n",
    "\n",
    "loss, mae = bilstm_model_aug.evaluate(X_test_aug, Y_test_aug)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred_aug = bilstm_model_aug.predict(X_test_aug)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test_aug, Y_pred_aug)#, multioutput='raw_values')\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)\n",
    "\n",
    "pd.DataFrame(history_aug.history).to_csv(\"./models/results/bilstm_test_aug.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2874/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 688.5102 - mae: 15.7008\n",
      "Epoch 1: val_loss improved from inf to 394.46857, saving model to ./models/bilstm_model.weights.h5\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 9ms/step - loss: 688.2634 - mae: 15.6969 - val_loss: 394.4686 - val_mae: 10.4897\n",
      "Epoch 2/50\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 373.3483 - mae: 10.3712\n",
      "Epoch 2: val_loss improved from 394.46857 to 391.52213, saving model to ./models/bilstm_model.weights.h5\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 373.3479 - mae: 10.3712 - val_loss: 391.5221 - val_mae: 10.3779\n",
      "Epoch 3/50\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 370.1462 - mae: 10.2577\n",
      "Epoch 3: val_loss improved from 391.52213 to 388.73044, saving model to ./models/bilstm_model.weights.h5\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 370.1461 - mae: 10.2577 - val_loss: 388.7304 - val_mae: 10.2749\n",
      "Epoch 4/50\n",
      "\u001b[1m2870/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 368.1946 - mae: 10.1849\n",
      "Epoch 4: val_loss improved from 388.73044 to 387.65872, saving model to ./models/bilstm_model.weights.h5\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 368.1941 - mae: 10.1849 - val_loss: 387.6587 - val_mae: 10.3326\n",
      "Epoch 5/50\n",
      "\u001b[1m2870/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 367.3254 - mae: 10.1543\n",
      "Epoch 5: val_loss improved from 387.65872 to 386.34100, saving model to ./models/bilstm_model.weights.h5\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 367.3245 - mae: 10.1543 - val_loss: 386.3410 - val_mae: 10.4844\n",
      "Epoch 6/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 366.7675 - mae: 10.1336\n",
      "Epoch 6: val_loss improved from 386.34100 to 385.13205, saving model to ./models/bilstm_model.weights.h5\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 366.7674 - mae: 10.1336 - val_loss: 385.1320 - val_mae: 10.4026\n",
      "Epoch 7/50\n",
      "\u001b[1m2874/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 366.1919 - mae: 10.1153\n",
      "Epoch 7: val_loss did not improve from 385.13205\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 366.1920 - mae: 10.1153 - val_loss: 386.1089 - val_mae: 10.1988\n",
      "Epoch 8/50\n",
      "\u001b[1m2872/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 366.9864 - mae: 10.1520\n",
      "Epoch 8: val_loss improved from 385.13205 to 384.34317, saving model to ./models/bilstm_model.weights.h5\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 366.9850 - mae: 10.1519 - val_loss: 384.3432 - val_mae: 10.1838\n",
      "Epoch 9/50\n",
      "\u001b[1m2870/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 365.3218 - mae: 10.0816\n",
      "Epoch 9: val_loss improved from 384.34317 to 384.09750, saving model to ./models/bilstm_model.weights.h5\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 365.3262 - mae: 10.0818 - val_loss: 384.0975 - val_mae: 10.5063\n",
      "Epoch 10/50\n",
      "\u001b[1m2872/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 366.7005 - mae: 10.1153\n",
      "Epoch 10: val_loss improved from 384.09750 to 383.72894, saving model to ./models/bilstm_model.weights.h5\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 366.7039 - mae: 10.1154 - val_loss: 383.7289 - val_mae: 10.3337\n",
      "Epoch 11/50\n",
      "\u001b[1m2873/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 367.8284 - mae: 10.1420\n",
      "Epoch 11: val_loss improved from 383.72894 to 382.83881, saving model to ./models/bilstm_model.weights.h5\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 367.8295 - mae: 10.1421 - val_loss: 382.8388 - val_mae: 10.3879\n",
      "Epoch 12/50\n",
      "\u001b[1m2875/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 365.1812 - mae: 10.0884\n",
      "Epoch 12: val_loss did not improve from 382.83881\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 365.1815 - mae: 10.0884 - val_loss: 388.1347 - val_mae: 10.9019\n",
      "Epoch 13/50\n",
      "\u001b[1m2875/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 371.8855 - mae: 10.3247\n",
      "Epoch 13: val_loss improved from 382.83881 to 382.47076, saving model to ./models/bilstm_model.weights.h5\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 371.8837 - mae: 10.3246 - val_loss: 382.4708 - val_mae: 10.2188\n",
      "Epoch 14/50\n",
      "\u001b[1m2873/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 366.2829 - mae: 10.1468\n",
      "Epoch 14: val_loss improved from 382.47076 to 380.96884, saving model to ./models/bilstm_model.weights.h5\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 366.2820 - mae: 10.1467 - val_loss: 380.9688 - val_mae: 10.2652\n",
      "Epoch 15/50\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 364.7329 - mae: 10.1000\n",
      "Epoch 15: val_loss improved from 380.96884 to 380.47397, saving model to ./models/bilstm_model.weights.h5\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10ms/step - loss: 364.7334 - mae: 10.1001 - val_loss: 380.4740 - val_mae: 10.2079\n",
      "Epoch 16/50\n",
      "\u001b[1m2873/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 363.3489 - mae: 10.0602\n",
      "Epoch 16: val_loss improved from 380.47397 to 380.00598, saving model to ./models/bilstm_model.weights.h5\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 363.3491 - mae: 10.0602 - val_loss: 380.0060 - val_mae: 10.1862\n",
      "Epoch 17/50\n",
      "\u001b[1m2873/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 376.2947 - mae: 10.4097\n",
      "Epoch 17: val_loss did not improve from 380.00598\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 376.3020 - mae: 10.4098 - val_loss: 380.5334 - val_mae: 10.3969\n",
      "Epoch 18/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 367.6858 - mae: 10.1650\n",
      "Epoch 18: val_loss did not improve from 380.00598\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 367.6904 - mae: 10.1652 - val_loss: 382.3242 - val_mae: 10.1317\n",
      "Epoch 19/50\n",
      "\u001b[1m2874/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 370.6357 - mae: 10.3226\n",
      "Epoch 19: val_loss improved from 380.00598 to 377.37982, saving model to ./models/bilstm_model.weights.h5\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 10ms/step - loss: 370.6333 - mae: 10.3225 - val_loss: 377.3798 - val_mae: 10.1134\n",
      "Epoch 20/50\n",
      "\u001b[1m2873/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 361.8726 - mae: 10.0314\n",
      "Epoch 20: val_loss did not improve from 377.37982\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 361.8722 - mae: 10.0314 - val_loss: 387.6001 - val_mae: 10.4616\n",
      "Epoch 21/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 362.4624 - mae: 10.0836\n",
      "Epoch 21: val_loss did not improve from 377.37982\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 362.4608 - mae: 10.0835 - val_loss: 379.1719 - val_mae: 10.1466\n",
      "Epoch 22/50\n",
      "\u001b[1m2875/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 361.7965 - mae: 10.0595\n",
      "Epoch 22: val_loss improved from 377.37982 to 376.89182, saving model to ./models/bilstm_model.weights.h5\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10ms/step - loss: 361.7961 - mae: 10.0594 - val_loss: 376.8918 - val_mae: 10.2680\n",
      "Epoch 23/50\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 360.7395 - mae: 10.0182\n",
      "Epoch 23: val_loss did not improve from 376.89182\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10ms/step - loss: 360.7395 - mae: 10.0182 - val_loss: 379.2889 - val_mae: 10.3835\n",
      "Epoch 24/50\n",
      "\u001b[1m2873/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 361.2630 - mae: 10.0034\n",
      "Epoch 24: val_loss did not improve from 376.89182\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10ms/step - loss: 361.2647 - mae: 10.0035 - val_loss: 381.2879 - val_mae: 10.3576\n",
      "Epoch 25/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 375.1035 - mae: 10.1798\n",
      "Epoch 25: val_loss improved from 376.89182 to 375.81744, saving model to ./models/bilstm_model.weights.h5\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10ms/step - loss: 375.1033 - mae: 10.1798 - val_loss: 375.8174 - val_mae: 10.3443\n",
      "Epoch 26/50\n",
      "\u001b[1m2870/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 360.8430 - mae: 9.9543\n",
      "Epoch 26: val_loss did not improve from 375.81744\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 10ms/step - loss: 360.8479 - mae: 9.9544 - val_loss: 385.4972 - val_mae: 10.9011\n",
      "Epoch 27/50\n",
      "\u001b[1m2874/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 362.7053 - mae: 10.0573\n",
      "Epoch 27: val_loss did not improve from 375.81744\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 362.7033 - mae: 10.0573 - val_loss: 375.9123 - val_mae: 10.2500\n",
      "Epoch 28/50\n",
      "\u001b[1m2872/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 362.3343 - mae: 10.0328\n",
      "Epoch 28: val_loss did not improve from 375.81744\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 362.3358 - mae: 10.0328 - val_loss: 378.2432 - val_mae: 9.9999\n",
      "Epoch 29/50\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 365.3603 - mae: 10.0654\n",
      "Epoch 29: val_loss improved from 375.81744 to 375.13556, saving model to ./models/bilstm_model.weights.h5\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 365.3616 - mae: 10.0654 - val_loss: 375.1356 - val_mae: 10.0438\n",
      "Epoch 30/50\n",
      "\u001b[1m2872/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 367.9583 - mae: 10.1173\n",
      "Epoch 30: val_loss did not improve from 375.13556\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 367.9668 - mae: 10.1175 - val_loss: 375.7191 - val_mae: 10.2358\n",
      "Epoch 31/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 383.7763 - mae: 10.5422\n",
      "Epoch 31: val_loss did not improve from 375.13556\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 383.7723 - mae: 10.5421 - val_loss: 376.2931 - val_mae: 10.2710\n",
      "Epoch 32/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 358.4061 - mae: 9.8962\n",
      "Epoch 32: val_loss did not improve from 375.13556\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 358.4076 - mae: 9.8963 - val_loss: 376.2912 - val_mae: 10.2346\n",
      "Epoch 33/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 361.0536 - mae: 9.9623\n",
      "Epoch 33: val_loss did not improve from 375.13556\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 361.0508 - mae: 9.9623 - val_loss: 375.8452 - val_mae: 10.0266\n",
      "Epoch 34/50\n",
      "\u001b[1m2870/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 360.4542 - mae: 9.9410\n",
      "Epoch 34: val_loss improved from 375.13556 to 375.00430, saving model to ./models/bilstm_model.weights.h5\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 8ms/step - loss: 360.4664 - mae: 9.9413 - val_loss: 375.0043 - val_mae: 10.2473\n",
      "Epoch 35/50\n",
      "\u001b[1m2873/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 361.6716 - mae: 9.9909\n",
      "Epoch 35: val_loss did not improve from 375.00430\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 8ms/step - loss: 361.6697 - mae: 9.9909 - val_loss: 375.0657 - val_mae: 10.2249\n",
      "Epoch 36/50\n",
      "\u001b[1m2874/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 361.2196 - mae: 10.0237\n",
      "Epoch 36: val_loss did not improve from 375.00430\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - loss: 361.2188 - mae: 10.0237 - val_loss: 375.3562 - val_mae: 10.2225\n",
      "Epoch 37/50\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 362.2070 - mae: 10.0639\n",
      "Epoch 37: val_loss did not improve from 375.00430\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 8ms/step - loss: 362.2072 - mae: 10.0639 - val_loss: 378.7737 - val_mae: 10.6367\n",
      "Epoch 38/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 359.1739 - mae: 9.9496\n",
      "Epoch 38: val_loss did not improve from 375.00430\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - loss: 359.1813 - mae: 9.9497 - val_loss: 377.9796 - val_mae: 10.5909\n",
      "Epoch 39/50\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 359.3078 - mae: 9.9127\n",
      "Epoch 39: val_loss did not improve from 375.00430\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 359.3085 - mae: 9.9127 - val_loss: 375.0139 - val_mae: 10.1457\n",
      "Epoch 39: early stopping\n",
      "\u001b[1m9924/9924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 340.4415 - mae: 9.8000\n",
      "Test Mean Absolute Error: 9.620270729064941\n",
      "\u001b[1m9924/9924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1ms/step\n",
      "Mean Absolute Error for each column:\n",
      "[16.327892   3.0245533  0.7545036  8.037462  19.956173 ]\n"
     ]
    }
   ],
   "source": [
    "# Defining callbacks\n",
    "checkpoint = ModelCheckpoint(\"./models/bilstm_model.keras\", monitor='val_loss', save_best_only=True, verbose=1, save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define LSTM model\n",
    "# Up to 2 layers of LSTM and number of hidden units were hand tuned to determine this as the optimum model\n",
    "bilstm_model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], 1)),\n",
    "    Bidirectional(\n",
    "        LSTM(units=64, activation='relu', recurrent_dropout=0.2)\n",
    "    ),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Use MSE for loss because we want to emphasize the \"wrongest\" guesses the most. MAE is an interpretable metric\n",
    "bilstm_model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model w/ early stopping\n",
    "# Batch size is the average number of flights per day\n",
    "history = bilstm_model.fit(X_train, Y_train, epochs=50, batch_size=265, validation_split=0.2, callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "\n",
    "loss, mae = bilstm_model.evaluate(X_test, Y_test)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred = bilstm_model.predict(X_test)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test, Y_pred)#, multioutput='raw_values')\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)\n",
    "\n",
    "pd.DataFrame(history.history).to_csv(\"./models/results/bilstm_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN + LSTM Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m3031/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 373.2918 - mae: 10.6417\n",
      "Epoch 1: val_loss improved from inf to 345.11807, saving model to ./models/hybrid_model_aug.weights.h5\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 22ms/step - loss: 373.2748 - mae: 10.6412 - val_loss: 345.1181 - val_mae: 9.8558\n",
      "Epoch 2/50\n",
      "\u001b[1m3030/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 342.4810 - mae: 9.6945\n",
      "Epoch 2: val_loss improved from 345.11807 to 343.27127, saving model to ./models/hybrid_model_aug.weights.h5\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 22ms/step - loss: 342.4802 - mae: 9.6944 - val_loss: 343.2713 - val_mae: 9.5489\n",
      "Epoch 3/50\n",
      "\u001b[1m3032/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 339.3009 - mae: 9.5892\n",
      "Epoch 3: val_loss improved from 343.27127 to 340.48566, saving model to ./models/hybrid_model_aug.weights.h5\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 22ms/step - loss: 339.3008 - mae: 9.5892 - val_loss: 340.4857 - val_mae: 9.5377\n",
      "Epoch 4/50\n",
      "\u001b[1m3030/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 337.5156 - mae: 9.5100\n",
      "Epoch 4: val_loss improved from 340.48566 to 340.32013, saving model to ./models/hybrid_model_aug.weights.h5\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 22ms/step - loss: 337.5161 - mae: 9.5100 - val_loss: 340.3201 - val_mae: 9.3204\n",
      "Epoch 5/50\n",
      "\u001b[1m3032/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 337.1588 - mae: 9.4705\n",
      "Epoch 5: val_loss improved from 340.32013 to 338.53180, saving model to ./models/hybrid_model_aug.weights.h5\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 23ms/step - loss: 337.1585 - mae: 9.4704 - val_loss: 338.5318 - val_mae: 9.3267\n",
      "Epoch 6/50\n",
      "\u001b[1m3032/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 335.7403 - mae: 9.4267\n",
      "Epoch 6: val_loss did not improve from 338.53180\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 22ms/step - loss: 335.7400 - mae: 9.4266 - val_loss: 339.0125 - val_mae: 9.2967\n",
      "Epoch 7/50\n",
      "\u001b[1m3031/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 334.5701 - mae: 9.3946\n",
      "Epoch 7: val_loss improved from 338.53180 to 337.90997, saving model to ./models/hybrid_model_aug.weights.h5\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 22ms/step - loss: 334.5706 - mae: 9.3947 - val_loss: 337.9100 - val_mae: 9.3302\n",
      "Epoch 8/50\n",
      "\u001b[1m3031/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 333.8059 - mae: 9.3732\n",
      "Epoch 8: val_loss did not improve from 337.90997\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 23ms/step - loss: 333.8056 - mae: 9.3732 - val_loss: 338.8559 - val_mae: 9.3170\n",
      "Epoch 9/50\n",
      "\u001b[1m3031/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 332.9903 - mae: 9.3516\n",
      "Epoch 9: val_loss did not improve from 337.90997\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 22ms/step - loss: 332.9915 - mae: 9.3517 - val_loss: 338.5595 - val_mae: 9.3860\n",
      "Epoch 10/50\n",
      "\u001b[1m3032/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 333.2134 - mae: 9.3608\n",
      "Epoch 10: val_loss improved from 337.90997 to 337.46664, saving model to ./models/hybrid_model_aug.weights.h5\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 22ms/step - loss: 333.2132 - mae: 9.3608 - val_loss: 337.4666 - val_mae: 9.3262\n",
      "Epoch 11/50\n",
      "\u001b[1m3031/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 331.7882 - mae: 9.3259\n",
      "Epoch 11: val_loss did not improve from 337.46664\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 22ms/step - loss: 331.7882 - mae: 9.3259 - val_loss: 338.3032 - val_mae: 9.2695\n",
      "Epoch 12/50\n",
      "\u001b[1m3032/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 331.0073 - mae: 9.3067\n",
      "Epoch 12: val_loss improved from 337.46664 to 337.29831, saving model to ./models/hybrid_model_aug.weights.h5\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 22ms/step - loss: 331.0074 - mae: 9.3067 - val_loss: 337.2983 - val_mae: 9.2750\n",
      "Epoch 13/50\n",
      "\u001b[1m3031/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 330.7180 - mae: 9.3009\n",
      "Epoch 13: val_loss did not improve from 337.29831\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 21ms/step - loss: 330.7181 - mae: 9.3009 - val_loss: 337.6226 - val_mae: 9.3032\n",
      "Epoch 14/50\n",
      "\u001b[1m3032/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 330.1245 - mae: 9.2867\n",
      "Epoch 14: val_loss did not improve from 337.29831\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 22ms/step - loss: 330.1246 - mae: 9.2867 - val_loss: 337.5863 - val_mae: 9.2899\n",
      "Epoch 15/50\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 331.7347 - mae: 9.3287\n",
      "Epoch 15: val_loss did not improve from 337.29831\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 22ms/step - loss: 331.7348 - mae: 9.3287 - val_loss: 338.7651 - val_mae: 9.3142\n",
      "Epoch 16/50\n",
      "\u001b[1m3032/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 330.7817 - mae: 9.2964\n",
      "Epoch 16: val_loss improved from 337.29831 to 336.68637, saving model to ./models/hybrid_model_aug.weights.h5\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 23ms/step - loss: 330.7816 - mae: 9.2964 - val_loss: 336.6864 - val_mae: 9.2939\n",
      "Epoch 17/50\n",
      "\u001b[1m3032/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 329.4750 - mae: 9.2688\n",
      "Epoch 17: val_loss did not improve from 336.68637\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 23ms/step - loss: 329.4750 - mae: 9.2688 - val_loss: 336.8571 - val_mae: 9.2949\n",
      "Epoch 18/50\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 328.9793 - mae: 9.2582\n",
      "Epoch 18: val_loss did not improve from 336.68637\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 24ms/step - loss: 328.9793 - mae: 9.2582 - val_loss: 336.8157 - val_mae: 9.2629\n",
      "Epoch 19/50\n",
      "\u001b[1m3031/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 408.8765 - mae: 9.7776\n",
      "Epoch 19: val_loss did not improve from 336.68637\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 24ms/step - loss: 408.9466 - mae: 9.7783 - val_loss: 385.9011 - val_mae: 10.4340\n",
      "Epoch 20/50\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 360.0663 - mae: 10.0740\n",
      "Epoch 20: val_loss did not improve from 336.68637\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 23ms/step - loss: 360.0652 - mae: 10.0739 - val_loss: 353.7241 - val_mae: 9.8388\n",
      "Epoch 21/50\n",
      "\u001b[1m3032/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 352.7124 - mae: 9.9201\n",
      "Epoch 21: val_loss did not improve from 336.68637\n",
      "\u001b[1m3033/3033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 24ms/step - loss: 352.7123 - mae: 9.9201 - val_loss: 352.6557 - val_mae: 9.8211\n",
      "Epoch 21: early stopping\n",
      "\u001b[1m20216/20216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step - loss: 358.7362 - mae: 9.9302\n",
      "Test Mean Absolute Error: 9.709922790527344\n",
      "\u001b[1m20216/20216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2ms/step\n",
      "Mean Absolute Error for each column:\n",
      "[17.46393     3.747185    0.31966054  8.338318   18.678913  ]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_aug = ModelCheckpoint(\"./models/hybrid_model_aug.keras\", monitor='val_loss', save_best_only=True, verbose=1, save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define input layer\n",
    "input_layer_aug = Input(shape=(X_train_aug.shape[1], 1))\n",
    "\n",
    "# CNN model\n",
    "conv_layer_aug = Conv1D(filters=32, kernel_size=3, activation='relu')(input_layer_aug)\n",
    "maxpool_layer_aug = MaxPooling1D(pool_size=2)(conv_layer_aug)\n",
    "flatten_layer_aug = Flatten()(maxpool_layer_aug)\n",
    "dense_cnn_aug = Dense(32, activation='relu')(flatten_layer_aug)\n",
    "\n",
    "# BiLSTM model\n",
    "lstm_layer_aug = LSTM(64, activation='relu')(input_layer_aug)\n",
    "# lstm_layer2 = LSTM(32, activation='relu', return_sequences=False)(lstm_layer)\n",
    "dense_lstm_aug = Dense(32, activation='relu')(lstm_layer_aug)\n",
    "\n",
    "# Concatenate CNN and BiLSTM outputs\n",
    "concatenated_aug = Concatenate()([dense_cnn_aug, dense_lstm_aug])\n",
    "\n",
    "# Output layer\n",
    "output_layer_aug = Dense(1)(concatenated_aug)\n",
    "\n",
    "# Create the ensemble model\n",
    "hybrid_model_aug = Model(inputs=input_layer_aug, outputs=output_layer_aug)\n",
    "\n",
    "hybrid_model_aug.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history_aug = hybrid_model_aug.fit(\n",
    "    X_train_aug,\n",
    "    Y_train_aug,\n",
    "    epochs=50,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint_aug, early_stopping]\n",
    ")\n",
    "\n",
    "loss, mae = hybrid_model_aug.evaluate(X_test_aug, Y_test_aug)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred_aug = hybrid_model_aug.predict(X_test_aug)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test_aug, Y_pred_aug)#, multioutput='raw_values')\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)\n",
    "\n",
    "pd.DataFrame(history_aug.history).to_csv(\"./models/results/cnn_lstm_test_aug.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1486/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 477.9755 - mae: 12.8623\n",
      "Epoch 1: val_loss improved from inf to 402.44519, saving model to ./models/hybrid_model.weights.h5\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 14ms/step - loss: 477.7948 - mae: 12.8587 - val_loss: 402.4452 - val_mae: 11.0491\n",
      "Epoch 2/50\n",
      "\u001b[1m1485/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 378.0799 - mae: 10.5672\n",
      "Epoch 2: val_loss improved from 402.44519 to 398.20496, saving model to ./models/hybrid_model.weights.h5\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 378.0763 - mae: 10.5671 - val_loss: 398.2050 - val_mae: 10.9517\n",
      "Epoch 3/50\n",
      "\u001b[1m1488/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 375.2344 - mae: 10.4778\n",
      "Epoch 3: val_loss improved from 398.20496 to 395.38043, saving model to ./models/hybrid_model.weights.h5\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - loss: 375.2339 - mae: 10.4778 - val_loss: 395.3804 - val_mae: 10.8343\n",
      "Epoch 4/50\n",
      "\u001b[1m1485/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 373.7253 - mae: 10.4090\n",
      "Epoch 4: val_loss improved from 395.38043 to 393.78564, saving model to ./models/hybrid_model.weights.h5\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - loss: 373.7242 - mae: 10.4089 - val_loss: 393.7856 - val_mae: 10.7565\n",
      "Epoch 5/50\n",
      "\u001b[1m1488/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 372.6652 - mae: 10.3638\n",
      "Epoch 5: val_loss improved from 393.78564 to 392.89673, saving model to ./models/hybrid_model.weights.h5\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - loss: 372.6653 - mae: 10.3638 - val_loss: 392.8967 - val_mae: 10.7624\n",
      "Epoch 6/50\n",
      "\u001b[1m1485/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 371.3696 - mae: 10.2874\n",
      "Epoch 6: val_loss improved from 392.89673 to 391.76321, saving model to ./models/hybrid_model.weights.h5\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - loss: 371.3690 - mae: 10.2874 - val_loss: 391.7632 - val_mae: 10.7057\n",
      "Epoch 7/50\n",
      "\u001b[1m1487/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 370.3830 - mae: 10.2449\n",
      "Epoch 7: val_loss improved from 391.76321 to 391.53320, saving model to ./models/hybrid_model.weights.h5\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - loss: 370.3825 - mae: 10.2449 - val_loss: 391.5332 - val_mae: 10.7416\n",
      "Epoch 8/50\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 369.3209 - mae: 10.2112\n",
      "Epoch 8: val_loss improved from 391.53320 to 390.97964, saving model to ./models/hybrid_model.weights.h5\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 369.3207 - mae: 10.2112 - val_loss: 390.9796 - val_mae: 10.7698\n",
      "Epoch 9/50\n",
      "\u001b[1m1487/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 367.9533 - mae: 10.1762\n",
      "Epoch 9: val_loss improved from 390.97964 to 385.94678, saving model to ./models/hybrid_model.weights.h5\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 367.9511 - mae: 10.1761 - val_loss: 385.9468 - val_mae: 10.6240\n",
      "Epoch 10/50\n",
      "\u001b[1m1488/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 361.8904 - mae: 10.0161\n",
      "Epoch 10: val_loss did not improve from 385.94678\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - loss: 361.8891 - mae: 10.0161 - val_loss: 386.0945 - val_mae: 10.5117\n",
      "Epoch 11/50\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 359.8569 - mae: 9.9467\n",
      "Epoch 11: val_loss improved from 385.94678 to 380.32843, saving model to ./models/hybrid_model.weights.h5\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - loss: 359.8565 - mae: 9.9467 - val_loss: 380.3284 - val_mae: 10.3826\n",
      "Epoch 12/50\n",
      "\u001b[1m1486/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 360.2470 - mae: 9.9408\n",
      "Epoch 12: val_loss did not improve from 380.32843\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - loss: 360.2526 - mae: 9.9409 - val_loss: 382.0291 - val_mae: 10.4981\n",
      "Epoch 13/50\n",
      "\u001b[1m1488/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 359.2847 - mae: 9.9137\n",
      "Epoch 13: val_loss improved from 380.32843 to 380.07233, saving model to ./models/hybrid_model.weights.h5\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - loss: 359.2836 - mae: 9.9137 - val_loss: 380.0723 - val_mae: 10.3907\n",
      "Epoch 14/50\n",
      "\u001b[1m1488/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 357.2916 - mae: 9.8537\n",
      "Epoch 14: val_loss improved from 380.07233 to 378.74994, saving model to ./models/hybrid_model.weights.h5\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 357.2915 - mae: 9.8537 - val_loss: 378.7499 - val_mae: 10.3403\n",
      "Epoch 15/50\n",
      "\u001b[1m1487/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 356.8081 - mae: 9.8376\n",
      "Epoch 15: val_loss did not improve from 378.74994\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 356.8076 - mae: 9.8376 - val_loss: 381.8014 - val_mae: 10.4576\n",
      "Epoch 16/50\n",
      "\u001b[1m1485/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 356.1474 - mae: 9.8209\n",
      "Epoch 16: val_loss improved from 378.74994 to 377.83887, saving model to ./models/hybrid_model.weights.h5\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 356.1465 - mae: 9.8209 - val_loss: 377.8389 - val_mae: 10.4003\n",
      "Epoch 17/50\n",
      "\u001b[1m1485/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 355.3870 - mae: 9.8016\n",
      "Epoch 17: val_loss improved from 377.83887 to 377.38168, saving model to ./models/hybrid_model.weights.h5\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 355.3866 - mae: 9.8016 - val_loss: 377.3817 - val_mae: 10.4112\n",
      "Epoch 18/50\n",
      "\u001b[1m1487/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 354.6856 - mae: 9.7831\n",
      "Epoch 18: val_loss improved from 377.38168 to 376.66116, saving model to ./models/hybrid_model.weights.h5\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 354.6853 - mae: 9.7831 - val_loss: 376.6612 - val_mae: 10.3971\n",
      "Epoch 19/50\n",
      "\u001b[1m1488/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 354.1785 - mae: 9.7692\n",
      "Epoch 19: val_loss did not improve from 376.66116\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 354.1788 - mae: 9.7692 - val_loss: 378.6074 - val_mae: 10.4276\n",
      "Epoch 20/50\n",
      "\u001b[1m1486/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 354.4172 - mae: 9.7728\n",
      "Epoch 20: val_loss improved from 376.66116 to 376.23465, saving model to ./models/hybrid_model.weights.h5\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 354.4158 - mae: 9.7727 - val_loss: 376.2346 - val_mae: 10.3766\n",
      "Epoch 21/50\n",
      "\u001b[1m1487/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 353.4160 - mae: 9.7521\n",
      "Epoch 21: val_loss improved from 376.23465 to 375.70267, saving model to ./models/hybrid_model.weights.h5\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 353.4158 - mae: 9.7521 - val_loss: 375.7027 - val_mae: 10.3450\n",
      "Epoch 22/50\n",
      "\u001b[1m1485/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 352.9393 - mae: 9.7407\n",
      "Epoch 22: val_loss did not improve from 375.70267\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 352.9388 - mae: 9.7407 - val_loss: 375.8767 - val_mae: 10.3476\n",
      "Epoch 23/50\n",
      "\u001b[1m1487/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 352.4446 - mae: 9.7303\n",
      "Epoch 23: val_loss did not improve from 375.70267\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 352.4443 - mae: 9.7303 - val_loss: 376.3724 - val_mae: 10.4455\n",
      "Epoch 24/50\n",
      "\u001b[1m1486/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 352.2191 - mae: 9.7238\n",
      "Epoch 24: val_loss did not improve from 375.70267\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 352.2184 - mae: 9.7238 - val_loss: 376.2823 - val_mae: 10.3537\n",
      "Epoch 25/50\n",
      "\u001b[1m1488/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 351.9005 - mae: 9.7151\n",
      "Epoch 25: val_loss did not improve from 375.70267\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - loss: 351.9002 - mae: 9.7151 - val_loss: 376.2032 - val_mae: 10.3860\n",
      "Epoch 26/50\n",
      "\u001b[1m1486/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 351.3416 - mae: 9.6995\n",
      "Epoch 26: val_loss did not improve from 375.70267\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 351.3416 - mae: 9.6995 - val_loss: 375.7722 - val_mae: 10.3142\n",
      "Epoch 26: early stopping\n",
      "\u001b[1m9924/9924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 342.4491 - mae: 9.9326\n",
      "Test Mean Absolute Error: 9.784661293029785\n",
      "\u001b[1m9924/9924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1ms/step\n",
      "Mean Absolute Error for each column:\n",
      "[16.978485    3.4189842   0.29223827  7.859515   20.37392   ]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"./models/hybrid_model.keras\", monitor='val_loss', save_best_only=True, verbose=1, save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define input layer\n",
    "input_layer = Input(shape=(X_train.shape[1], 1))\n",
    "\n",
    "# CNN model\n",
    "conv_layer = Conv1D(filters=32, kernel_size=3, activation='relu')(input_layer)\n",
    "maxpool_layer = MaxPooling1D(pool_size=2)(conv_layer)\n",
    "flatten_layer = Flatten()(maxpool_layer)\n",
    "dense_cnn = Dense(32, activation='relu')(flatten_layer)\n",
    "\n",
    "# BiLSTM model\n",
    "lstm_layer = LSTM(64, activation='relu')(input_layer)\n",
    "# lstm_layer2 = LSTM(32, activation='relu', return_sequences=False)(lstm_layer)\n",
    "dense_lstm = Dense(32, activation='relu')(lstm_layer)\n",
    "\n",
    "# Concatenate CNN and BiLSTM outputs\n",
    "concatenated = Concatenate()([dense_cnn, dense_lstm])\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(1)(concatenated)\n",
    "\n",
    "# Create the ensemble model\n",
    "hybrid_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "hybrid_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history = hybrid_model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=50,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint, early_stopping]\n",
    ")\n",
    "\n",
    "loss, mae = hybrid_model.evaluate(X_test, Y_test)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred = hybrid_model.predict(X_test)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test, Y_pred)#, multioutput='raw_values')\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)\n",
    "\n",
    "pd.DataFrame(history.history).to_csv(\"./models/results/cnn_lstm_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN + LSTM Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix shape: (19213, 19213)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────\n",
    "MASTER_COORD = \"./Datasets/T_MASTER_CORD.csv\"\n",
    "K_NEIGHBORS  = 5      # connect each airport to its 5 closest neighbors\n",
    "EARTH_R      = 6371.0 # km\n",
    "\n",
    "# ─── 1) load airport coordinates ────────────────────────────────\n",
    "mc = (\n",
    "    pd.read_csv(MASTER_COORD, dtype=str)\n",
    "      .set_index(\"AIRPORT_SEQ_ID\")[[\"LATITUDE\",\"LONGITUDE\"]]\n",
    "      .astype(float)\n",
    ")\n",
    "# list of IDs and arrays of lat/lon in radians\n",
    "ids  = mc.index.to_list()\n",
    "lats = np.deg2rad(mc[\"LATITUDE\"].values)\n",
    "lons = np.deg2rad(mc[\"LONGITUDE\"].values)\n",
    "N    = len(ids)\n",
    "\n",
    "# ─── 2) compute pairwise haversine distances ────────────────────\n",
    "# using the vectorized formula\n",
    "lat1 = lats[:, None]\n",
    "lat2 = lats[None, :]\n",
    "dlon = lons[None, :] - lons[:, None]\n",
    "\n",
    "dlat = lat2 - lat1\n",
    "a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "c = 2 * np.arcsin(np.sqrt(a))\n",
    "dist_km = EARTH_R * c  # shape (N, N)\n",
    "\n",
    "# ─── 3) build adjacency by k-nearest neighbors ───────────────────\n",
    "A = np.zeros((N, N), dtype=np.float32)\n",
    "\n",
    "for i in range(N):\n",
    "    # argsort returns i itself at position 0, so skip it\n",
    "    neighbors = np.argsort(dist_km[i])[1 : K_NEIGHBORS+1]\n",
    "    A[i, neighbors] = 1.0\n",
    "\n",
    "# symmetrize: if i→j or j→i, keep edge both ways\n",
    "A = np.maximum(A, A.T)\n",
    "\n",
    "# ─── 4) (Optional) row-normalize adjacency ──────────────────────\n",
    "# so each row sums to 1\n",
    "row_sums = A.sum(axis=1, keepdims=True)\n",
    "A_norm   = A / np.where(row_sums>0, row_sums, 1.0)\n",
    "\n",
    "# Now `A_norm` is your adjacency matrix to feed into the GNN\n",
    "print(\"Adjacency matrix shape:\", A_norm.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable(package=\"Custom\", name=\"SimpleGraphConv\")\n",
    "class SimpleGraphConv(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape = [(batch, N, F), (batch, N, N)]\n",
    "        F = input_shape[0][-1]\n",
    "        self.w = self.add_weight(\n",
    "            shape=(F, self.units),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            name=\"kernel\")\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X, A = inputs    # X: (batch, N, F), A: (batch, N, N)\n",
    "        return tf.matmul(A, tf.matmul(X, self.w))\n",
    "\n",
    "\n",
    "@register_keras_serializable(package=\"Custom\", name=\"GCNTimeDistributed\")\n",
    "class GCNTimeDistributed(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        # we’ll reuse this conv at each time slice\n",
    "        self.gcn = SimpleGraphConv(units)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape = [(batch, T, N, F), (batch, N, N)]\n",
    "        # we need to build the inner GCN on a flattened time‐slice shape:\n",
    "        # pretend batch' = None and time = 1 so shape = (None, N, F) & (None, N, N)\n",
    "        _, T, N, F = input_shape[0]\n",
    "        # call inner build:\n",
    "        self.gcn.build([(None, N, F), (None, N, N)])\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X, A = inputs\n",
    "        batch = tf.shape(X)[0]\n",
    "        T     = tf.shape(X)[1]\n",
    "        N     = tf.shape(X)[2]\n",
    "        F     = tf.shape(X)[3]\n",
    "\n",
    "        # 1) collapse time\n",
    "        Xr = tf.reshape(X, (batch * T, N, F))\n",
    "        # 2) tile adjacency\n",
    "        Aexp = tf.expand_dims(A, 1)               # (batch, 1, N, N)\n",
    "        Atil = tf.tile(Aexp, [1, T, 1, 1])         # (batch, T, N, N)\n",
    "        Ar   = tf.reshape(Atil, (batch * T, N, N))\n",
    "        # 3) apply GCN\n",
    "        Yr = self.gcn([Xr, Ar])                    # (batch*T, N, units)\n",
    "        # 4) restore time axis\n",
    "        return tf.reshape(Yr, (batch, T, N, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_mae\",\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build GNN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(952611, 1, 2, 5)\n"
     ]
    }
   ],
   "source": [
    "# 1) decide T, N, F\n",
    "T = 1       # one snapshot per flight\n",
    "N = 2       # origin + destination nodes\n",
    "F = 5       # the five weather vars you listed\n",
    "\n",
    "A_np = A_norm\n",
    "gcn_units = 32\n",
    "\n",
    "X_in = Input(shape=(T, N, F), name=\"node_features\")       # dynamic nodal time‐series\n",
    "A_in = Input(shape=(N, N),    name=\"adjacency_matrix\")    # can be broadcast\n",
    "\n",
    "weather_cols = X_train.columns[: N * F]  # first 10 columns\n",
    "assert len(weather_cols) == N * F\n",
    "\n",
    "# ─── C) extract and reshape ───────────────────────────────────────────────────\n",
    "weather_train = X_train[weather_cols].to_numpy()  # shape (n_samples, 10)\n",
    "weather_test  = X_test[ weather_cols].to_numpy()\n",
    "\n",
    "# reshape into (batch, T, N, F)\n",
    "X_train_gnn = weather_train.reshape(-1, T, N, F)\n",
    "X_test_gnn  = weather_test.reshape(-1, T, N, F)\n",
    "\n",
    "print(X_train_gnn.shape)  # → (952611, 1, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.conda\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── 2a) GCN‐over‐time layer 1\n",
    "g = GCNTimeDistributed(gcn_units, name=\"time_gcn\")([X_in, A_in])\n",
    "\n",
    "# ─── (optional) 2nd GCN‐over‐time\n",
    "g = GCNTimeDistributed(gcn_units, name=\"time_gcn2\")([g, A_in])\n",
    "\n",
    "# ─── 2b) flatten per‐time‐step but keep T\n",
    "\n",
    "g_flat = TimeDistributed(Flatten(), name=\"flatten_nodes\")(g)  \n",
    "# shape = (batch, T, N * gcn_units)\n",
    "\n",
    "# ─── 2c) LSTM\n",
    "h = LSTM(64, name=\"temporal_lstm\")(g_flat)  # now g_flat is 3D\n",
    "\n",
    "# ─── 2d) final Dense (make sure units=Y_train.shape[1])\n",
    "Y_dim = Y_train.shape[1]\n",
    "out = Dense(Y_dim, name=\"output\")(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"GNN_LSTM_Hybrid\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"GNN_LSTM_Hybrid\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ node_features       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ adjacency_matrix    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_gcn            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │ node_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GCNTimeDistribute…</span> │                   │            │ adjacency_matrix… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_gcn2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ time_gcn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GCNTimeDistribute…</span> │                   │            │ adjacency_matrix… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_nodes       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_gcn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ temporal_lstm       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ flatten_nodes[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)              │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │ temporal_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ node_features       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ adjacency_matrix    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_gcn            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │        \u001b[38;5;34m160\u001b[0m │ node_features[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mGCNTimeDistribute…\u001b[0m │                   │            │ adjacency_matrix… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_gcn2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │      \u001b[38;5;34m1,024\u001b[0m │ time_gcn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│ (\u001b[38;5;33mGCNTimeDistribute…\u001b[0m │                   │            │ adjacency_matrix… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_nodes       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ time_gcn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ temporal_lstm       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m33,024\u001b[0m │ flatten_nodes[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mLSTM\u001b[0m)              │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │        \u001b[38;5;34m325\u001b[0m │ temporal_lstm[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,533</span> (134.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,533\u001b[0m (134.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,533</span> (134.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,533\u001b[0m (134.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ─── 3) Compile, train & evaluate ────────────────────────────────────────────\n",
    "\n",
    "gnn_lstm = Model([X_in, A_in], out, name=\"GNN_LSTM_Hybrid\")\n",
    "gnn_lstm.summary()\n",
    "gnn_lstm.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=\"./models/gnn_lstm.keras\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    verbose=1, save_weights_only=False\n",
    ")\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_train shape: (952611, 2, 2)\n",
      "Epoch 1/50\n",
      "\u001b[1m23795/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 427.8394 - mae: 11.1616\n",
      "Epoch 1: val_loss improved from inf to 437.55191, saving model to ./models/gnn_lstm.weights.h5\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2ms/step - loss: 427.8296 - mae: 11.1618 - val_loss: 437.5519 - val_mae: 11.7252 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m23785/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5343 - mae: 11.4718\n",
      "Epoch 2: val_loss did not improve from 437.55191\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2ms/step - loss: 414.5339 - mae: 11.4718 - val_loss: 437.5519 - val_mae: 11.7252 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m23813/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5340 - mae: 11.4718\n",
      "Epoch 3: val_loss did not improve from 437.55191\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2ms/step - loss: 414.5339 - mae: 11.4718 - val_loss: 437.5519 - val_mae: 11.7252 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m23809/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5340 - mae: 11.4718\n",
      "Epoch 4: val_loss did not improve from 437.55191\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2ms/step - loss: 414.5339 - mae: 11.4718 - val_loss: 437.5519 - val_mae: 11.7252 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m23801/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5223 - mae: 11.4710\n",
      "Epoch 5: val_loss improved from 437.55191 to 437.46173, saving model to ./models/gnn_lstm.weights.h5\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2ms/step - loss: 414.5221 - mae: 11.4710 - val_loss: 437.4617 - val_mae: 11.7276 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m23790/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5221 - mae: 11.4715\n",
      "Epoch 6: val_loss did not improve from 437.46173\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2ms/step - loss: 414.5218 - mae: 11.4715 - val_loss: 437.4617 - val_mae: 11.7276 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m23799/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5220 - mae: 11.4715\n",
      "Epoch 7: val_loss improved from 437.46173 to 437.46167, saving model to ./models/gnn_lstm.weights.h5\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2ms/step - loss: 414.5218 - mae: 11.4715 - val_loss: 437.4617 - val_mae: 11.7276 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m23811/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5163 - mae: 11.4707\n",
      "Epoch 8: val_loss improved from 437.46167 to 437.41028, saving model to ./models/gnn_lstm.weights.h5\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step - loss: 414.5163 - mae: 11.4707 - val_loss: 437.4103 - val_mae: 11.7323 - learning_rate: 2.5000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5164 - mae: 11.4718\n",
      "Epoch 9: val_loss did not improve from 437.41028\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step - loss: 414.5164 - mae: 11.4718 - val_loss: 437.4103 - val_mae: 11.7323 - learning_rate: 2.5000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m23790/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5168 - mae: 11.4718\n",
      "Epoch 10: val_loss did not improve from 437.41028\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step - loss: 414.5164 - mae: 11.4718 - val_loss: 437.4103 - val_mae: 11.7323 - learning_rate: 2.5000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m23808/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5143 - mae: 11.4714\n",
      "Epoch 11: val_loss did not improve from 437.41028\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2ms/step - loss: 414.5142 - mae: 11.4714 - val_loss: 437.4149 - val_mae: 11.7334 - learning_rate: 1.2500e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m23800/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5140 - mae: 11.4723\n",
      "Epoch 12: val_loss did not improve from 437.41028\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2ms/step - loss: 414.5138 - mae: 11.4723 - val_loss: 437.4149 - val_mae: 11.7334 - learning_rate: 1.2500e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m23787/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5142 - mae: 11.4723\n",
      "Epoch 13: val_loss did not improve from 437.41028\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2ms/step - loss: 414.5138 - mae: 11.4723 - val_loss: 437.4149 - val_mae: 11.7334 - learning_rate: 1.2500e-04\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Test loss, MAE: [365.7703857421875, 10.890534400939941]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# For origin ↔ dest only:\n",
    "A2 = np.array([[0., 1.],\n",
    "               [1., 0.]], dtype=np.float32)\n",
    "\n",
    "# Tile it for every sample in the train/test set:\n",
    "A_train = np.tile(A2[None], (X_train_gnn.shape[0], 1, 1))  # (952611,2,2)\n",
    "A_test  = np.tile(A2[None], (X_test_gnn.shape[0],  1, 1))  # (… likewise)\n",
    "\n",
    "print(\"A_train shape:\", A_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Fit just like you did your LSTM models:\n",
    "history = gnn_lstm.fit(\n",
    "    [X_train_gnn, A_train],\n",
    "    Y_train,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr],\n",
    ")\n",
    "\n",
    "test_results = gnn_lstm.evaluate(\n",
    "    [X_test_gnn, A_test],\n",
    "    Y_test,\n",
    "    verbose=0,\n",
    ")\n",
    "print(\"Test loss, MAE:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GNN training data → ./data/\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(history.history).to_csv(\"./models/results/gnn_test.csv\")\n",
    "np.save(\"./data/X_train_gnn.npy\", X_train_gnn)\n",
    "np.save(\"./data/A_train.npy\",     A_train)\n",
    "np.save(\"./data/Y_train.npy\",     Y_train)\n",
    "\n",
    "np.save(\"./data/X_test_gnn.npy\", X_test_gnn)\n",
    "np.save(\"./data/A_test.npy\",     A_test)\n",
    "np.save(\"./data/Y_test.npy\",     Y_test)\n",
    "\n",
    "print(\"Saved GNN training data → ./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1940657, 1, 2, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) decide T, N, F\n",
    "T = 1       # one snapshot per flight\n",
    "N = 2       # origin + destination nodes\n",
    "F = 5       # the five weather vars you listed\n",
    "\n",
    "\n",
    "A_np_aug = A_norm\n",
    "gcn_units = 32\n",
    "\n",
    "# Inputs\n",
    "X_in_aug = Input(shape=(T, N, F), name=\"node_features\")       # dynamic nodal time‐series\n",
    "A_in_aug = Input(shape=(N, N),    name=\"adjacency_matrix\")    # can be broadcast\n",
    "\n",
    "\n",
    "# 2) pull out just the 10 weather columns from your 11\n",
    "#    (assumes X_train[:, :10] are [orig_var0..4, dest_var0..4])\n",
    "weather_cols_aug = X_train_aug.columns[: N * F]  # first 10 columns\n",
    "assert len(weather_cols_aug) == N * F\n",
    "\n",
    "# ─── C) extract and reshape ───────────────────────────────────────────────────\n",
    "weather_train_aug = X_train_aug[weather_cols_aug].to_numpy()  # shape (n_samples, 10)\n",
    "weather_test_aug  = X_test_aug[ weather_cols_aug].to_numpy()\n",
    "\n",
    "# reshape into (batch, T, N, F)\n",
    "X_train_gnn_aug = weather_train_aug.reshape(-1, T, N, F)\n",
    "X_test_gnn_aug  = weather_test_aug.reshape(-1, T, N, F)\n",
    "\n",
    "print(X_train_gnn_aug.shape)  # → (952611, 1, 2, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 2) Build the Graph-LSTM hybrid ───────────────────────────────────────────\n",
    "\n",
    "# ─── 2a) GCN‐over‐time layer 1\n",
    "g_aug = GCNTimeDistributed(gcn_units, name=\"time_gcn\")([X_in_aug, A_in_aug])\n",
    "\n",
    "# ─── (optional) 2nd GCN‐over‐time\n",
    "#g_aug = GCNTimeDistributed(gcn_units, name=\"time_gcn2\")([g_aug, A_in_aug])\n",
    "\n",
    "# ─── 2b) flatten per‐time‐step but keep T\n",
    "\n",
    "g_flat_aug = TimeDistributed(Flatten(), name=\"flatten_nodes\")(g_aug)  \n",
    "# shape = (batch, T, N * gcn_units)\n",
    "\n",
    "# ─── 2c) LSTM\n",
    "h_aug = LSTM(64, name=\"temporal_lstm\")(g_flat_aug)  # now g_flat is 3D\n",
    "\n",
    "# ─── 2d) final Dense (make sure units=Y_train.shape[1])\n",
    "Y_dim = Y_train_aug.shape[1]\n",
    "out_aug = Dense(Y_dim, name=\"output\")(h_aug)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"GNN_LSTM_Hybrid\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"GNN_LSTM_Hybrid\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ node_features       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ adjacency_matrix    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_gcn            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │ node_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GCNTimeDistribute…</span> │                   │            │ adjacency_matrix… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_nodes       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_gcn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ temporal_lstm       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ flatten_nodes[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)              │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │ temporal_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ node_features       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ adjacency_matrix    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_gcn            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │        \u001b[38;5;34m160\u001b[0m │ node_features[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mGCNTimeDistribute…\u001b[0m │                   │            │ adjacency_matrix… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_nodes       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ time_gcn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ temporal_lstm       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m33,024\u001b[0m │ flatten_nodes[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mLSTM\u001b[0m)              │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │        \u001b[38;5;34m325\u001b[0m │ temporal_lstm[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,509</span> (130.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m33,509\u001b[0m (130.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,509</span> (130.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m33,509\u001b[0m (130.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ─── 3) Compile, train & evaluate ────────────────────────────────────────────\n",
    "\n",
    "# Assemble\n",
    "gnn_lstm_aug = Model(inputs=[X_in_aug, A_in_aug], outputs=out_aug, name=\"GNN_LSTM_Hybrid\")\n",
    "gnn_lstm_aug.summary()\n",
    "\n",
    "\n",
    "gnn_lstm_aug.compile(optimizer=\"adam\",loss=\"mse\",metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=\"./models/gnn_lstm_aug.keras\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    verbose=1, save_weights_only=False\n",
    ")\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_train_aug shape: (1940657, 2, 2)\n",
      "Epoch 1/50\n",
      "\u001b[1m48500/48517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 401.7130 - mae: 11.0300\n",
      "Epoch 1: val_loss improved from inf to 395.76895, saving model to ./models/gnn_lstm_aug.weights.h5\n",
      "\u001b[1m48517/48517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 2ms/step - loss: 401.7108 - mae: 11.0300 - val_loss: 395.7690 - val_mae: 11.1188 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m48482/48517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 394.2573 - mae: 11.2336\n",
      "Epoch 2: val_loss did not improve from 395.76895\n",
      "\u001b[1m48517/48517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 1ms/step - loss: 394.2574 - mae: 11.2336 - val_loss: 395.7690 - val_mae: 11.1188 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m48512/48517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 394.2574 - mae: 11.2336\n",
      "Epoch 3: val_loss did not improve from 395.76895\n",
      "\u001b[1m48517/48517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 2ms/step - loss: 394.2574 - mae: 11.2336 - val_loss: 395.7690 - val_mae: 11.1188 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m48509/48517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 394.2574 - mae: 11.2336\n",
      "Epoch 4: val_loss did not improve from 395.76895\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m48517/48517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1ms/step - loss: 394.2574 - mae: 11.2336 - val_loss: 395.7690 - val_mae: 11.1188 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m48501/48517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 394.2483 - mae: 11.2344\n",
      "Epoch 5: val_loss did not improve from 395.76895\n",
      "\u001b[1m48517/48517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1ms/step - loss: 394.2484 - mae: 11.2344 - val_loss: 395.8989 - val_mae: 11.1172 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m48502/48517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 394.2474 - mae: 11.2336\n",
      "Epoch 6: val_loss did not improve from 395.76895\n",
      "\u001b[1m48517/48517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 1ms/step - loss: 394.2474 - mae: 11.2336 - val_loss: 395.8989 - val_mae: 11.1172 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Aug Test loss, MAE: [392.37664794921875, 11.129707336425781]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# For origin ↔ dest only:\n",
    "A2 = np.array([[0., 1.],\n",
    "               [1., 0.]], dtype=np.float32)\n",
    "\n",
    "# Tile it for every sample in the train/test set:\n",
    "A_train_aug = np.tile(A2[None], (X_train_gnn_aug.shape[0], 1, 1))  # (952611,2,2)\n",
    "A_test_aug  = np.tile(A2[None], (X_test_gnn_aug.shape[0],  1, 1))  # (… likewise)\n",
    "\n",
    "print(\"A_train_aug shape:\", A_train_aug.shape)\n",
    "\n",
    "\n",
    "# Fit just like you did your LSTM models:\n",
    "history_aug = gnn_lstm_aug.fit(\n",
    "    [X_train_gnn_aug, A_train_aug],\n",
    "    Y_train_aug,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr],\n",
    ")\n",
    "\n",
    "# Evaluate on test:\n",
    "test_results_aug = gnn_lstm_aug.evaluate(\n",
    "    [X_test_gnn_aug, A_test_aug],\n",
    "    Y_test_aug,\n",
    "    verbose=0,\n",
    ")\n",
    "print(\"Aug Test loss, MAE:\", test_results_aug)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GNN aug training data → ./data/\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(history_aug.history).to_csv(\"./models/results/gnn_test_aug.csv\")\n",
    "\n",
    "np.save(\"./data/X_train_gnn_aug.npy\", X_train_gnn_aug)\n",
    "np.save(\"./data/A_train_aug.npy\",     A_train_aug)\n",
    "np.save(\"./data/Y_train_aug.npy\",     Y_train_aug)\n",
    "\n",
    "np.save(\"./data/X_test_gnn_aug.npy\", X_test_gnn_aug)\n",
    "np.save(\"./data/A_test_aug.npy\",     A_test_aug)\n",
    "np.save(\"./data/Y_test_aug.npy\",     Y_test_aug)\n",
    "\n",
    "print(\"Saved GNN aug training data → ./data/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
