{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Bidirectional, Conv1D, MaxPooling1D, Flatten, Concatenate, TimeDistributed, Reshape, Activation, Lambda, Layer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug = pd.read_csv(\"./data/X_train_lstm.csv\")\n",
    "X_test_aug = pd.read_csv(\"./data/X_test_lstm.csv\")\n",
    "\n",
    "Y_train_aug = pd.read_csv(\"./data/Y_train_lstm.csv\")\n",
    "Y_test_aug = pd.read_csv(\"./data/Y_test_lstm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"./data/old_data/X_train_lstm.csv\")\n",
    "X_test = pd.read_csv(\"./data/old_data/X_test_lstm.csv\")\n",
    "\n",
    "Y_train = pd.read_csv(\"./data/old_data/Y_train_lstm.csv\")\n",
    "Y_test = pd.read_csv(\"./data/old_data/Y_test_lstm.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2867/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3423.3923 - mae: 25.2094\n",
      "Epoch 1: val_loss improved from inf to 479.73154, saving model to ./models/lstm_model_aug.keras\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 13ms/step - loss: 3422.0540 - mae: 25.2032 - val_loss: 479.7315 - val_mae: 9.9277\n",
      "Epoch 2/50\n",
      "\u001b[1m2867/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 533.6815 - mae: 11.1124\n",
      "Epoch 2: val_loss improved from 479.73154 to 434.94122, saving model to ./models/lstm_model_aug.keras\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 13ms/step - loss: 533.8452 - mae: 11.1133 - val_loss: 434.9412 - val_mae: 10.6739\n",
      "Epoch 3/50\n",
      "\u001b[1m2866/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 473.8381 - mae: 11.0616\n",
      "Epoch 3: val_loss did not improve from 434.94122\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 13ms/step - loss: 473.8531 - mae: 11.0620 - val_loss: 498.0321 - val_mae: 15.7319\n",
      "Epoch 4/50\n",
      "\u001b[1m2867/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 501.2985 - mae: 12.2704\n",
      "Epoch 4: val_loss did not improve from 434.94122\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10ms/step - loss: 501.2886 - mae: 12.2702 - val_loss: 459.2133 - val_mae: 10.6095\n",
      "Epoch 5/50\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 478.0035 - mae: 12.0937\n",
      "Epoch 5: val_loss improved from 434.94122 to 422.12466, saving model to ./models/lstm_model_aug.keras\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10ms/step - loss: 477.9950 - mae: 12.0935 - val_loss: 422.1247 - val_mae: 10.9306\n",
      "Epoch 6/50\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 493.4898 - mae: 11.7351\n",
      "Epoch 6: val_loss did not improve from 422.12466\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 493.5230 - mae: 11.7354 - val_loss: 424.0966 - val_mae: 11.3064\n",
      "Epoch 7/50\n",
      "\u001b[1m2867/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1421.2195 - mae: 13.2178\n",
      "Epoch 7: val_loss did not improve from 422.12466\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 1421.5508 - mae: 13.2189 - val_loss: 424.4998 - val_mae: 12.0476\n",
      "Epoch 8/50\n",
      "\u001b[1m2866/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 439.5095 - mae: 11.6526\n",
      "Epoch 8: val_loss improved from 422.12466 to 420.40213, saving model to ./models/lstm_model_aug.keras\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10ms/step - loss: 439.5124 - mae: 11.6525 - val_loss: 420.4021 - val_mae: 10.9590\n",
      "Epoch 9/50\n",
      "\u001b[1m2863/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 419.5861 - mae: 11.1939\n",
      "Epoch 9: val_loss did not improve from 420.40213\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10ms/step - loss: 419.6321 - mae: 11.1943 - val_loss: 442.4856 - val_mae: 11.8304\n",
      "Epoch 10/50\n",
      "\u001b[1m2867/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 486.2011 - mae: 11.6989\n",
      "Epoch 10: val_loss improved from 420.40213 to 419.08713, saving model to ./models/lstm_model_aug.keras\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10ms/step - loss: 486.1750 - mae: 11.6988 - val_loss: 419.0871 - val_mae: 11.2181\n",
      "Epoch 11/50\n",
      "\u001b[1m2865/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 786.9836 - mae: 14.1671\n",
      "Epoch 11: val_loss improved from 419.08713 to 418.98318, saving model to ./models/lstm_model_aug.keras\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10ms/step - loss: 787.0325 - mae: 14.1672 - val_loss: 418.9832 - val_mae: 11.4486\n",
      "Epoch 12/50\n",
      "\u001b[1m2867/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1089.0530 - mae: 14.2865\n",
      "Epoch 12: val_loss improved from 418.98318 to 417.77756, saving model to ./models/lstm_model_aug.keras\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10ms/step - loss: 1088.7780 - mae: 14.2856 - val_loss: 417.7776 - val_mae: 11.6291\n",
      "Epoch 13/50\n",
      "\u001b[1m2864/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 944.7024 - mae: 13.8911\n",
      "Epoch 13: val_loss did not improve from 417.77756\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10ms/step - loss: 944.2866 - mae: 13.8895 - val_loss: 419.8051 - val_mae: 11.2767\n",
      "Epoch 14/50\n",
      "\u001b[1m2865/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6550.5229 - mae: 18.8834\n",
      "Epoch 14: val_loss did not improve from 417.77756\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10ms/step - loss: 6559.4136 - mae: 18.8941 - val_loss: 4512.3198 - val_mae: 36.1603\n",
      "Epoch 15/50\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 19412.6387 - mae: 41.9680\n",
      "Epoch 15: val_loss did not improve from 417.77756\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10ms/step - loss: 19408.7617 - mae: 41.9623 - val_loss: 419.7005 - val_mae: 11.1486\n",
      "Epoch 16/50\n",
      "\u001b[1m2865/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2669.2756 - mae: 19.0058\n",
      "Epoch 16: val_loss did not improve from 417.77756\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10ms/step - loss: 2675.8398 - mae: 19.0131 - val_loss: 457.6008 - val_mae: 13.0228\n",
      "Epoch 17/50\n",
      "\u001b[1m2866/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7208.3325 - mae: 17.8950\n",
      "Epoch 17: val_loss did not improve from 417.77756\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10ms/step - loss: 7240.7700 - mae: 17.9060 - val_loss: 767.1656 - val_mae: 22.4809\n",
      "Epoch 17: early stopping\n",
      "\u001b[1m9895/9895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 718.7498 - mae: 22.0435\n",
      "Test Mean Absolute Error: 22.216716766357422\n",
      "\u001b[1m9895/9895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1ms/step\n",
      "Mean Absolute Error for each column:\n",
      "[23.852533 18.822742 23.676437 20.576822 24.151722]\n"
     ]
    }
   ],
   "source": [
    "# Defining callbacks\n",
    "checkpoint_aug = ModelCheckpoint(\"./models/lstm_model_aug.keras\", monitor='val_loss', save_best_only=True, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define LSTM model\n",
    "# Up to 2 layers of LSTM and number of hidden units were hand tuned to determine this as the optimum model\n",
    "lstm_model_aug = Sequential([\n",
    "    Input(shape=(X_train_aug.shape[1], 1)),\n",
    "    LSTM(units=64, activation='relu', recurrent_dropout=0.2),\n",
    "    Dense(5)\n",
    "])\n",
    "\n",
    "# Use MSE for loss because we want to emphasize the \"wrongest\" guesses the most. MAE is an interpretable metric\n",
    "lstm_model_aug.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model w/ early stopping\n",
    "# Batch size is the average number of flights per day\n",
    "history = lstm_model_aug.fit(X_train_aug, Y_train_aug, epochs=50, batch_size=265, validation_split=0.2, callbacks=[checkpoint_aug, early_stopping])\n",
    "\n",
    "\n",
    "loss, mae = lstm_model_aug.evaluate(X_test_aug, Y_test_aug)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred_aug = lstm_model_aug.predict(X_test_aug)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test_aug, Y_pred_aug, multioutput='raw_values')\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2875/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 455.6948 - mae: 11.7923\n",
      "Epoch 1: val_loss improved from inf to 397.93814, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 455.6567 - mae: 11.7917 - val_loss: 397.9381 - val_mae: 10.2813\n",
      "Epoch 2/50\n",
      "\u001b[1m2874/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 374.7997 - mae: 10.2537\n",
      "Epoch 2: val_loss improved from 397.93814 to 390.64362, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - loss: 374.7983 - mae: 10.2537 - val_loss: 390.6436 - val_mae: 10.2638\n",
      "Epoch 3/50\n",
      "\u001b[1m2873/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 372.4715 - mae: 10.1862\n",
      "Epoch 3: val_loss improved from 390.64362 to 388.11856, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - loss: 372.4691 - mae: 10.1861 - val_loss: 388.1186 - val_mae: 10.3847\n",
      "Epoch 4/50\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 368.8278 - mae: 10.1202\n",
      "Epoch 4: val_loss improved from 388.11856 to 386.07428, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - loss: 368.8276 - mae: 10.1202 - val_loss: 386.0743 - val_mae: 10.4351\n",
      "Epoch 5/50\n",
      "\u001b[1m2868/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 367.4667 - mae: 10.0924\n",
      "Epoch 5: val_loss improved from 386.07428 to 385.67960, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 367.4658 - mae: 10.0924 - val_loss: 385.6796 - val_mae: 10.5338\n",
      "Epoch 6/50\n",
      "\u001b[1m2869/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 366.3682 - mae: 10.0709\n",
      "Epoch 6: val_loss improved from 385.67960 to 384.92255, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 366.3678 - mae: 10.0709 - val_loss: 384.9225 - val_mae: 10.5306\n",
      "Epoch 7/50\n",
      "\u001b[1m2873/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 365.4733 - mae: 10.0498\n",
      "Epoch 7: val_loss improved from 384.92255 to 384.52234, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 365.4731 - mae: 10.0498 - val_loss: 384.5223 - val_mae: 10.6265\n",
      "Epoch 8/50\n",
      "\u001b[1m2870/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 364.9459 - mae: 10.0391\n",
      "Epoch 8: val_loss improved from 384.52234 to 383.90930, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 364.9453 - mae: 10.0391 - val_loss: 383.9093 - val_mae: 10.5663\n",
      "Epoch 9/50\n",
      "\u001b[1m2875/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 363.8372 - mae: 10.0129\n",
      "Epoch 9: val_loss improved from 383.90930 to 380.89240, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 363.8369 - mae: 10.0128 - val_loss: 380.8924 - val_mae: 10.5196\n",
      "Epoch 10/50\n",
      "\u001b[1m2868/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 361.4774 - mae: 9.9517\n",
      "Epoch 10: val_loss improved from 380.89240 to 378.18304, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 361.4756 - mae: 9.9517 - val_loss: 378.1830 - val_mae: 10.3975\n",
      "Epoch 11/50\n",
      "\u001b[1m2873/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 359.5475 - mae: 9.8986\n",
      "Epoch 11: val_loss improved from 378.18304 to 377.75827, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 359.5469 - mae: 9.8986 - val_loss: 377.7583 - val_mae: 10.2827\n",
      "Epoch 12/50\n",
      "\u001b[1m2867/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 358.5292 - mae: 9.8723\n",
      "Epoch 12: val_loss improved from 377.75827 to 376.61792, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 358.5280 - mae: 9.8723 - val_loss: 376.6179 - val_mae: 10.3417\n",
      "Epoch 13/50\n",
      "\u001b[1m2875/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 357.8050 - mae: 9.8513\n",
      "Epoch 13: val_loss improved from 376.61792 to 376.46152, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 357.8048 - mae: 9.8513 - val_loss: 376.4615 - val_mae: 10.3554\n",
      "Epoch 14/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 357.2658 - mae: 9.8360\n",
      "Epoch 14: val_loss did not improve from 376.46152\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 357.2650 - mae: 9.8360 - val_loss: 376.6728 - val_mae: 10.3160\n",
      "Epoch 15/50\n",
      "\u001b[1m2875/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 356.5691 - mae: 9.8167\n",
      "Epoch 15: val_loss improved from 376.46152 to 375.94461, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 356.5690 - mae: 9.8167 - val_loss: 375.9446 - val_mae: 10.3350\n",
      "Epoch 16/50\n",
      "\u001b[1m2866/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 356.7306 - mae: 9.8192\n",
      "Epoch 16: val_loss improved from 375.94461 to 375.46365, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 356.7290 - mae: 9.8192 - val_loss: 375.4637 - val_mae: 10.2924\n",
      "Epoch 17/50\n",
      "\u001b[1m2870/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 355.7003 - mae: 9.7948\n",
      "Epoch 17: val_loss improved from 375.46365 to 375.00089, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 355.6996 - mae: 9.7948 - val_loss: 375.0009 - val_mae: 10.2953\n",
      "Epoch 18/50\n",
      "\u001b[1m2869/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 355.4839 - mae: 9.7852\n",
      "Epoch 18: val_loss did not improve from 375.00089\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 355.4830 - mae: 9.7852 - val_loss: 375.1294 - val_mae: 10.3491\n",
      "Epoch 19/50\n",
      "\u001b[1m2875/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 355.1055 - mae: 9.7736\n",
      "Epoch 19: val_loss improved from 375.00089 to 374.71445, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 355.1053 - mae: 9.7736 - val_loss: 374.7144 - val_mae: 10.2675\n",
      "Epoch 20/50\n",
      "\u001b[1m2874/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 354.6518 - mae: 9.7661\n",
      "Epoch 20: val_loss improved from 374.71445 to 374.66321, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 354.6516 - mae: 9.7661 - val_loss: 374.6632 - val_mae: 10.3085\n",
      "Epoch 21/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 354.4628 - mae: 9.7605\n",
      "Epoch 21: val_loss did not improve from 374.66321\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 354.4625 - mae: 9.7605 - val_loss: 375.1556 - val_mae: 10.3350\n",
      "Epoch 22/50\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 354.0827 - mae: 9.7506\n",
      "Epoch 22: val_loss improved from 374.66321 to 374.17316, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 354.0826 - mae: 9.7506 - val_loss: 374.1732 - val_mae: 10.2910\n",
      "Epoch 23/50\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 353.8649 - mae: 9.7435\n",
      "Epoch 23: val_loss did not improve from 374.17316\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 353.8648 - mae: 9.7435 - val_loss: 378.4059 - val_mae: 10.4675\n",
      "Epoch 24/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 353.8615 - mae: 9.7433\n",
      "Epoch 24: val_loss did not improve from 374.17316\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 353.8608 - mae: 9.7432 - val_loss: 375.2765 - val_mae: 10.3628\n",
      "Epoch 25/50\n",
      "\u001b[1m2870/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 353.5185 - mae: 9.7342\n",
      "Epoch 25: val_loss did not improve from 374.17316\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 353.5182 - mae: 9.7342 - val_loss: 375.0940 - val_mae: 10.2969\n",
      "Epoch 26/50\n",
      "\u001b[1m2866/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 353.5075 - mae: 9.7339\n",
      "Epoch 26: val_loss did not improve from 374.17316\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - loss: 353.5062 - mae: 9.7338 - val_loss: 374.3860 - val_mae: 10.3315\n",
      "Epoch 27/50\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 353.2813 - mae: 9.7308\n",
      "Epoch 27: val_loss improved from 374.17316 to 373.69177, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 353.2812 - mae: 9.7308 - val_loss: 373.6918 - val_mae: 10.2710\n",
      "Epoch 28/50\n",
      "\u001b[1m2866/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 353.0300 - mae: 9.7242\n",
      "Epoch 28: val_loss did not improve from 373.69177\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - loss: 353.0295 - mae: 9.7242 - val_loss: 374.8831 - val_mae: 10.3918\n",
      "Epoch 29/50\n",
      "\u001b[1m2866/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 352.9172 - mae: 9.7203\n",
      "Epoch 29: val_loss did not improve from 373.69177\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 352.9165 - mae: 9.7203 - val_loss: 374.0208 - val_mae: 10.3630\n",
      "Epoch 30/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 353.4101 - mae: 9.7345\n",
      "Epoch 30: val_loss did not improve from 373.69177\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 353.4093 - mae: 9.7345 - val_loss: 374.1241 - val_mae: 10.3416\n",
      "Epoch 31/50\n",
      "\u001b[1m2870/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 352.6857 - mae: 9.7146\n",
      "Epoch 31: val_loss improved from 373.69177 to 373.57666, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 352.6852 - mae: 9.7146 - val_loss: 373.5767 - val_mae: 10.2287\n",
      "Epoch 32/50\n",
      "\u001b[1m2869/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 353.6850 - mae: 9.7400\n",
      "Epoch 32: val_loss did not improve from 373.57666\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 353.6831 - mae: 9.7399 - val_loss: 374.3490 - val_mae: 10.3297\n",
      "Epoch 33/50\n",
      "\u001b[1m2870/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 352.7348 - mae: 9.7185\n",
      "Epoch 33: val_loss did not improve from 373.57666\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 352.7340 - mae: 9.7184 - val_loss: 373.8202 - val_mae: 10.3144\n",
      "Epoch 34/50\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 352.7793 - mae: 9.7223\n",
      "Epoch 34: val_loss improved from 373.57666 to 373.57101, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 352.7791 - mae: 9.7223 - val_loss: 373.5710 - val_mae: 10.3321\n",
      "Epoch 35/50\n",
      "\u001b[1m2875/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 353.8685 - mae: 9.7441\n",
      "Epoch 35: val_loss did not improve from 373.57101\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 353.8676 - mae: 9.7441 - val_loss: 375.1987 - val_mae: 10.4170\n",
      "Epoch 36/50\n",
      "\u001b[1m2874/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 352.2674 - mae: 9.7080\n",
      "Epoch 36: val_loss did not improve from 373.57101\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 352.2670 - mae: 9.7080 - val_loss: 374.3434 - val_mae: 10.3564\n",
      "Epoch 37/50\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 352.2180 - mae: 9.7078\n",
      "Epoch 37: val_loss did not improve from 373.57101\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 352.2179 - mae: 9.7077 - val_loss: 374.0021 - val_mae: 10.3844\n",
      "Epoch 38/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 351.9550 - mae: 9.7011\n",
      "Epoch 38: val_loss improved from 373.57101 to 373.45877, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 351.9543 - mae: 9.7011 - val_loss: 373.4588 - val_mae: 10.3449\n",
      "Epoch 39/50\n",
      "\u001b[1m2869/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 352.2145 - mae: 9.7101\n",
      "Epoch 39: val_loss did not improve from 373.45877\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 352.2128 - mae: 9.7100 - val_loss: 373.8117 - val_mae: 10.3602\n",
      "Epoch 40/50\n",
      "\u001b[1m2867/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 351.8188 - mae: 9.6975\n",
      "Epoch 40: val_loss improved from 373.45877 to 372.84106, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 351.8174 - mae: 9.6975 - val_loss: 372.8411 - val_mae: 10.2815\n",
      "Epoch 41/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 351.6225 - mae: 9.6913\n",
      "Epoch 41: val_loss did not improve from 372.84106\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 351.6222 - mae: 9.6913 - val_loss: 374.7498 - val_mae: 10.3616\n",
      "Epoch 42/50\n",
      "\u001b[1m2869/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 351.9036 - mae: 9.6974\n",
      "Epoch 42: val_loss did not improve from 372.84106\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 351.9023 - mae: 9.6973 - val_loss: 373.2191 - val_mae: 10.3070\n",
      "Epoch 43/50\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 351.6201 - mae: 9.6951\n",
      "Epoch 43: val_loss did not improve from 372.84106\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 351.6201 - mae: 9.6951 - val_loss: 373.8151 - val_mae: 10.3428\n",
      "Epoch 44/50\n",
      "\u001b[1m2875/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 351.4898 - mae: 9.6905\n",
      "Epoch 44: val_loss did not improve from 372.84106\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 351.4897 - mae: 9.6905 - val_loss: 373.5281 - val_mae: 10.3577\n",
      "Epoch 45/50\n",
      "\u001b[1m2872/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 351.1923 - mae: 9.6858\n",
      "Epoch 45: val_loss did not improve from 372.84106\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 351.1918 - mae: 9.6858 - val_loss: 373.2668 - val_mae: 10.2778\n",
      "Epoch 45: early stopping\n",
      "\u001b[1m9924/9924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1ms/step - loss: 342.0641 - mae: 10.0107\n",
      "Test Mean Absolute Error: 9.833145141601562\n",
      "\u001b[1m9924/9924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 995us/step\n",
      "Mean Absolute Error for each column:\n",
      "[16.68961     4.058853    0.34435886  7.8688283  20.203745  ]\n"
     ]
    }
   ],
   "source": [
    "# Defining callbacks\n",
    "checkpoint = ModelCheckpoint(\"./models/lstm_model.keras\", monitor='val_loss', save_best_only=True, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define LSTM model\n",
    "# Up to 2 layers of LSTM and number of hidden units were hand tuned to determine this as the optimum model\n",
    "lstm_model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], 1)),\n",
    "    LSTM(units=64, activation='relu', recurrent_dropout=0.2),\n",
    "    Dense(5)\n",
    "])\n",
    "\n",
    "# Use MSE for loss because we want to emphasize the \"wrongest\" guesses the most. MAE is an interpretable metric\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model w/ early stopping\n",
    "# Batch size is the average number of flights per day\n",
    "history = lstm_model.fit(X_train, Y_train, epochs=50, batch_size=265, validation_split=0.2, callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "\n",
    "loss, mae = lstm_model.evaluate(X_test, Y_test)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred = lstm_model.predict(X_test)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test, Y_pred, multioutput='raw_values')\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2865/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1161.1936 - mae: 18.2545\n",
      "Epoch 1: val_loss improved from inf to 381.04468, saving model to ./models/bilstm_model_aug.keras\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 16ms/step - loss: 1160.6000 - mae: 18.2496 - val_loss: 381.0447 - val_mae: 10.2252\n",
      "Epoch 2/50\n",
      "\u001b[1m2865/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 560.8406 - mae: 12.5182\n",
      "Epoch 2: val_loss improved from 381.04468 to 371.49557, saving model to ./models/bilstm_model_aug.keras\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 16ms/step - loss: 561.1428 - mae: 12.5194 - val_loss: 371.4956 - val_mae: 9.7406\n",
      "Epoch 3/50\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 443.5334 - mae: 11.5017\n",
      "Epoch 3: val_loss improved from 371.49557 to 370.59045, saving model to ./models/bilstm_model_aug.keras\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 16ms/step - loss: 443.5423 - mae: 11.5019 - val_loss: 370.5905 - val_mae: 9.9939\n",
      "Epoch 4/50\n",
      "\u001b[1m2867/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 451.5786 - mae: 11.8089\n",
      "Epoch 4: val_loss improved from 370.59045 to 367.86896, saving model to ./models/bilstm_model_aug.keras\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 16ms/step - loss: 451.5832 - mae: 11.8090 - val_loss: 367.8690 - val_mae: 10.1072\n",
      "Epoch 5/50\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 927.9946 - mae: 11.5641\n",
      "Epoch 5: val_loss did not improve from 367.86896\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 16ms/step - loss: 928.1840 - mae: 11.5646 - val_loss: 367.8803 - val_mae: 9.8821\n",
      "Epoch 6/50\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 463.1062 - mae: 10.8681\n",
      "Epoch 6: val_loss improved from 367.86896 to 367.56226, saving model to ./models/bilstm_model_aug.keras\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 16ms/step - loss: 463.1679 - mae: 10.8683 - val_loss: 367.5623 - val_mae: 10.0242\n",
      "Epoch 7/50\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 563.6530 - mae: 12.1634\n",
      "Epoch 7: val_loss did not improve from 367.56226\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 16ms/step - loss: 563.6696 - mae: 12.1634 - val_loss: 375.6484 - val_mae: 10.8014\n",
      "Epoch 8/50\n",
      "\u001b[1m2867/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1057.7859 - mae: 14.1023\n",
      "Epoch 8: val_loss improved from 367.56226 to 366.61548, saving model to ./models/bilstm_model_aug.keras\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 16ms/step - loss: 1058.2614 - mae: 14.1037 - val_loss: 366.6155 - val_mae: 10.2062\n",
      "Epoch 9/50\n",
      "\u001b[1m2867/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 476.4719 - mae: 10.9688\n",
      "Epoch 9: val_loss improved from 366.61548 to 366.46283, saving model to ./models/bilstm_model_aug.keras\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 16ms/step - loss: 476.5540 - mae: 10.9692 - val_loss: 366.4628 - val_mae: 9.8737\n",
      "Epoch 10/50\n",
      "\u001b[1m2865/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 638.7050 - mae: 11.3510\n",
      "Epoch 10: val_loss did not improve from 366.46283\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 16ms/step - loss: 640.4929 - mae: 11.3536 - val_loss: 371.4445 - val_mae: 10.0961\n",
      "Epoch 11/50\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 418.9782 - mae: 10.8505\n",
      "Epoch 11: val_loss improved from 366.46283 to 364.78030, saving model to ./models/bilstm_model_aug.keras\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 16ms/step - loss: 418.9794 - mae: 10.8505 - val_loss: 364.7803 - val_mae: 9.9340\n",
      "Epoch 12/50\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 430.0539 - mae: 10.7840\n",
      "Epoch 12: val_loss improved from 364.78030 to 364.55844, saving model to ./models/bilstm_model_aug.keras\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 16ms/step - loss: 430.0581 - mae: 10.7841 - val_loss: 364.5584 - val_mae: 9.9718\n",
      "Epoch 13/50\n",
      "\u001b[1m2866/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1862.8839 - mae: 12.0928\n",
      "Epoch 13: val_loss did not improve from 364.55844\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 16ms/step - loss: 1866.3579 - mae: 12.0949 - val_loss: 366.2263 - val_mae: 10.1336\n",
      "Epoch 14/50\n",
      "\u001b[1m2865/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 528.6806 - mae: 11.6406\n",
      "Epoch 14: val_loss did not improve from 364.55844\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 16ms/step - loss: 529.0810 - mae: 11.6421 - val_loss: 365.1295 - val_mae: 10.0445\n",
      "Epoch 15/50\n",
      "\u001b[1m2866/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5320.2461 - mae: 16.0481\n",
      "Epoch 15: val_loss did not improve from 364.55844\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 16ms/step - loss: 5317.7168 - mae: 16.0457 - val_loss: 374.2272 - val_mae: 11.0571\n",
      "Epoch 16/50\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4443.7773 - mae: 17.1121\n",
      "Epoch 16: val_loss did not improve from 364.55844\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 16ms/step - loss: 4443.3203 - mae: 17.1115 - val_loss: 565.0464 - val_mae: 16.5544\n",
      "Epoch 17/50\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9398.6172 - mae: 21.3084\n",
      "Epoch 17: val_loss did not improve from 364.55844\n",
      "\u001b[1m2868/2868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 16ms/step - loss: 9398.2305 - mae: 21.3077 - val_loss: 382.1994 - val_mae: 11.0552\n",
      "Epoch 17: early stopping\n",
      "\u001b[1m9895/9895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - loss: 348.3074 - mae: 10.6103\n",
      "Test Mean Absolute Error: 10.744625091552734\n",
      "\u001b[1m9895/9895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step\n",
      "Mean Absolute Error for each column:\n",
      "[21.239563    5.187404    0.35823965  7.5562344  19.380827  ]\n"
     ]
    }
   ],
   "source": [
    "# Defining callbacks\n",
    "checkpoint_aug = ModelCheckpoint(\"./models/bilstm_model_aug.keras\", monitor='val_loss', save_best_only=True, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define LSTM model\n",
    "# Up to 2 layers of LSTM and number of hidden units were hand tuned to determine this as the optimum model\n",
    "bilstm_model_aug = Sequential([\n",
    "    Input(shape=(X_train.shape[1], 1)),\n",
    "    Bidirectional(\n",
    "        LSTM(units=64, activation='relu', recurrent_dropout=0.2)\n",
    "    ),\n",
    "    Dense(5)\n",
    "])\n",
    "\n",
    "# Use MSE for loss because we want to emphasize the \"wrongest\" guesses the most. MAE is an interpretable metric\n",
    "bilstm_model_aug.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model w/ early stopping\n",
    "# Batch size is the average number of flights per day\n",
    "history = bilstm_model_aug.fit(X_train_aug, Y_train_aug, epochs=50, batch_size=265, validation_split=0.2, callbacks=[checkpoint_aug, early_stopping])\n",
    "\n",
    "\n",
    "loss, mae = bilstm_model_aug.evaluate(X_test_aug, Y_test_aug)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred_aug = bilstm_model_aug.predict(X_test_aug)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test_aug, Y_pred_aug, multioutput='raw_values')\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2873/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 688.5925 - mae: 15.7021\n",
      "Epoch 1: val_loss improved from inf to 394.46857, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9ms/step - loss: 688.2634 - mae: 15.6969 - val_loss: 394.4686 - val_mae: 10.4897\n",
      "Epoch 2/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 373.3499 - mae: 10.3713\n",
      "Epoch 2: val_loss improved from 394.46857 to 391.52213, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 373.3479 - mae: 10.3712 - val_loss: 391.5221 - val_mae: 10.3779\n",
      "Epoch 3/50\n",
      "\u001b[1m2874/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 370.1465 - mae: 10.2578\n",
      "Epoch 3: val_loss improved from 391.52213 to 388.73044, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 370.1461 - mae: 10.2577 - val_loss: 388.7304 - val_mae: 10.2749\n",
      "Epoch 4/50\n",
      "\u001b[1m2874/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 368.1943 - mae: 10.1849\n",
      "Epoch 4: val_loss improved from 388.73044 to 387.65872, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 368.1941 - mae: 10.1849 - val_loss: 387.6587 - val_mae: 10.3326\n",
      "Epoch 5/50\n",
      "\u001b[1m2875/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 367.3248 - mae: 10.1543\n",
      "Epoch 5: val_loss improved from 387.65872 to 386.34100, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 367.3245 - mae: 10.1543 - val_loss: 386.3410 - val_mae: 10.4844\n",
      "Epoch 6/50\n",
      "\u001b[1m2873/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 366.7675 - mae: 10.1336\n",
      "Epoch 6: val_loss improved from 386.34100 to 385.13205, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 366.7674 - mae: 10.1336 - val_loss: 385.1320 - val_mae: 10.4026\n",
      "Epoch 7/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 366.1917 - mae: 10.1153\n",
      "Epoch 7: val_loss did not improve from 385.13205\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - loss: 366.1920 - mae: 10.1153 - val_loss: 386.1089 - val_mae: 10.1988\n",
      "Epoch 8/50\n",
      "\u001b[1m2872/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 366.9864 - mae: 10.1520\n",
      "Epoch 8: val_loss improved from 385.13205 to 384.34317, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 366.9850 - mae: 10.1519 - val_loss: 384.3432 - val_mae: 10.1838\n",
      "Epoch 9/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 365.3224 - mae: 10.0817\n",
      "Epoch 9: val_loss improved from 384.34317 to 384.09750, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - loss: 365.3262 - mae: 10.0818 - val_loss: 384.0975 - val_mae: 10.5063\n",
      "Epoch 10/50\n",
      "\u001b[1m2873/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 366.7012 - mae: 10.1153\n",
      "Epoch 10: val_loss improved from 384.09750 to 383.72894, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 366.7039 - mae: 10.1154 - val_loss: 383.7289 - val_mae: 10.3337\n",
      "Epoch 11/50\n",
      "\u001b[1m2873/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 367.8284 - mae: 10.1420\n",
      "Epoch 11: val_loss improved from 383.72894 to 382.83881, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 367.8295 - mae: 10.1421 - val_loss: 382.8388 - val_mae: 10.3879\n",
      "Epoch 12/50\n",
      "\u001b[1m2874/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 365.1811 - mae: 10.0884\n",
      "Epoch 12: val_loss did not improve from 382.83881\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 365.1815 - mae: 10.0884 - val_loss: 388.1347 - val_mae: 10.9019\n",
      "Epoch 13/50\n",
      "\u001b[1m2875/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 371.8855 - mae: 10.3247\n",
      "Epoch 13: val_loss improved from 382.83881 to 382.47076, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - loss: 371.8837 - mae: 10.3246 - val_loss: 382.4708 - val_mae: 10.2188\n",
      "Epoch 14/50\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 366.2822 - mae: 10.1467\n",
      "Epoch 14: val_loss improved from 382.47076 to 380.96884, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 366.2820 - mae: 10.1467 - val_loss: 380.9688 - val_mae: 10.2652\n",
      "Epoch 15/50\n",
      "\u001b[1m2874/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 364.7318 - mae: 10.1000\n",
      "Epoch 15: val_loss improved from 380.96884 to 380.47397, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - loss: 364.7334 - mae: 10.1001 - val_loss: 380.4740 - val_mae: 10.2079\n",
      "Epoch 16/50\n",
      "\u001b[1m2872/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 363.3489 - mae: 10.0602\n",
      "Epoch 16: val_loss improved from 380.47397 to 380.00598, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 363.3491 - mae: 10.0602 - val_loss: 380.0060 - val_mae: 10.1862\n",
      "Epoch 17/50\n",
      "\u001b[1m2870/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 376.2893 - mae: 10.4097\n",
      "Epoch 17: val_loss did not improve from 380.00598\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - loss: 376.3020 - mae: 10.4098 - val_loss: 380.5334 - val_mae: 10.3969\n",
      "Epoch 18/50\n",
      "\u001b[1m2872/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 367.6866 - mae: 10.1650\n",
      "Epoch 18: val_loss did not improve from 380.00598\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 367.6904 - mae: 10.1652 - val_loss: 382.3242 - val_mae: 10.1317\n",
      "Epoch 19/50\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 370.6341 - mae: 10.3225\n",
      "Epoch 19: val_loss improved from 380.00598 to 377.37982, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - loss: 370.6333 - mae: 10.3225 - val_loss: 377.3798 - val_mae: 10.1134\n",
      "Epoch 20/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 361.8727 - mae: 10.0314\n",
      "Epoch 20: val_loss did not improve from 377.37982\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 361.8722 - mae: 10.0314 - val_loss: 387.6001 - val_mae: 10.4616\n",
      "Epoch 21/50\n",
      "\u001b[1m2874/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 362.4616 - mae: 10.0836\n",
      "Epoch 21: val_loss did not improve from 377.37982\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 362.4608 - mae: 10.0835 - val_loss: 379.1719 - val_mae: 10.1466\n",
      "Epoch 22/50\n",
      "\u001b[1m2874/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 361.7968 - mae: 10.0595\n",
      "Epoch 22: val_loss improved from 377.37982 to 376.89182, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10ms/step - loss: 361.7961 - mae: 10.0594 - val_loss: 376.8918 - val_mae: 10.2680\n",
      "Epoch 23/50\n",
      "\u001b[1m2873/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 360.7397 - mae: 10.0182\n",
      "Epoch 23: val_loss did not improve from 376.89182\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - loss: 360.7395 - mae: 10.0182 - val_loss: 379.2889 - val_mae: 10.3835\n",
      "Epoch 24/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 361.2622 - mae: 10.0034\n",
      "Epoch 24: val_loss did not improve from 376.89182\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 361.2647 - mae: 10.0035 - val_loss: 381.2879 - val_mae: 10.3576\n",
      "Epoch 25/50\n",
      "\u001b[1m2875/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 375.1034 - mae: 10.1798\n",
      "Epoch 25: val_loss improved from 376.89182 to 375.81744, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 375.1033 - mae: 10.1798 - val_loss: 375.8174 - val_mae: 10.3443\n",
      "Epoch 26/50\n",
      "\u001b[1m2873/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 360.8451 - mae: 9.9543\n",
      "Epoch 26: val_loss did not improve from 375.81744\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 360.8479 - mae: 9.9544 - val_loss: 385.4972 - val_mae: 10.9011\n",
      "Epoch 27/50\n",
      "\u001b[1m2870/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 362.7079 - mae: 10.0574\n",
      "Epoch 27: val_loss did not improve from 375.81744\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 362.7033 - mae: 10.0573 - val_loss: 375.9123 - val_mae: 10.2500\n",
      "Epoch 28/50\n",
      "\u001b[1m2873/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 362.3346 - mae: 10.0328\n",
      "Epoch 28: val_loss did not improve from 375.81744\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 362.3358 - mae: 10.0328 - val_loss: 378.2432 - val_mae: 9.9999\n",
      "Epoch 29/50\n",
      "\u001b[1m2875/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 365.3590 - mae: 10.0653\n",
      "Epoch 29: val_loss improved from 375.81744 to 375.13556, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 365.3616 - mae: 10.0654 - val_loss: 375.1356 - val_mae: 10.0438\n",
      "Epoch 30/50\n",
      "\u001b[1m2874/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 367.9617 - mae: 10.1174\n",
      "Epoch 30: val_loss did not improve from 375.13556\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 367.9668 - mae: 10.1175 - val_loss: 375.7191 - val_mae: 10.2358\n",
      "Epoch 31/50\n",
      "\u001b[1m2872/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 383.7756 - mae: 10.5422\n",
      "Epoch 31: val_loss did not improve from 375.13556\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 383.7723 - mae: 10.5421 - val_loss: 376.2931 - val_mae: 10.2710\n",
      "Epoch 32/50\n",
      "\u001b[1m2870/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 358.4059 - mae: 9.8962\n",
      "Epoch 32: val_loss did not improve from 375.13556\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 358.4076 - mae: 9.8963 - val_loss: 376.2912 - val_mae: 10.2346\n",
      "Epoch 33/50\n",
      "\u001b[1m2872/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 361.0532 - mae: 9.9623\n",
      "Epoch 33: val_loss did not improve from 375.13556\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 361.0508 - mae: 9.9623 - val_loss: 375.8452 - val_mae: 10.0266\n",
      "Epoch 34/50\n",
      "\u001b[1m2872/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 360.4576 - mae: 9.9411\n",
      "Epoch 34: val_loss improved from 375.13556 to 375.00430, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 360.4664 - mae: 9.9413 - val_loss: 375.0043 - val_mae: 10.2473\n",
      "Epoch 35/50\n",
      "\u001b[1m2875/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 361.6706 - mae: 9.9909\n",
      "Epoch 35: val_loss did not improve from 375.00430\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 361.6697 - mae: 9.9909 - val_loss: 375.0657 - val_mae: 10.2249\n",
      "Epoch 36/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 361.2205 - mae: 10.0237\n",
      "Epoch 36: val_loss did not improve from 375.00430\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 361.2188 - mae: 10.0237 - val_loss: 375.3562 - val_mae: 10.2225\n",
      "Epoch 37/50\n",
      "\u001b[1m2871/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 362.2063 - mae: 10.0639\n",
      "Epoch 37: val_loss did not improve from 375.00430\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 362.2072 - mae: 10.0639 - val_loss: 378.7737 - val_mae: 10.6367\n",
      "Epoch 38/50\n",
      "\u001b[1m2872/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 359.1751 - mae: 9.9496\n",
      "Epoch 38: val_loss did not improve from 375.00430\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - loss: 359.1813 - mae: 9.9497 - val_loss: 377.9796 - val_mae: 10.5909\n",
      "Epoch 39/50\n",
      "\u001b[1m2872/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 359.3049 - mae: 9.9126\n",
      "Epoch 39: val_loss did not improve from 375.00430\n",
      "\u001b[1m2876/2876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - loss: 359.3085 - mae: 9.9127 - val_loss: 375.0139 - val_mae: 10.1457\n",
      "Epoch 39: early stopping\n",
      "\u001b[1m9924/9924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 340.4415 - mae: 9.8000\n",
      "Test Mean Absolute Error: 9.620270729064941\n",
      "\u001b[1m9924/9924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1ms/step\n",
      "Mean Absolute Error for each column:\n",
      "[16.327892   3.0245533  0.7545036  8.037462  19.956173 ]\n"
     ]
    }
   ],
   "source": [
    "# Defining callbacks\n",
    "checkpoint = ModelCheckpoint(\"./models/bilstm_model.keras\", monitor='val_loss', save_best_only=True, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define LSTM model\n",
    "# Up to 2 layers of LSTM and number of hidden units were hand tuned to determine this as the optimum model\n",
    "bilstm_model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], 1)),\n",
    "    Bidirectional(\n",
    "        LSTM(units=64, activation='relu', recurrent_dropout=0.2)\n",
    "    ),\n",
    "    Dense(5)\n",
    "])\n",
    "\n",
    "# Use MSE for loss because we want to emphasize the \"wrongest\" guesses the most. MAE is an interpretable metric\n",
    "bilstm_model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model w/ early stopping\n",
    "# Batch size is the average number of flights per day\n",
    "history = bilstm_model.fit(X_train, Y_train, epochs=50, batch_size=265, validation_split=0.2, callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "\n",
    "loss, mae = bilstm_model.evaluate(X_test, Y_test)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred = bilstm_model.predict(X_test)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test, Y_pred, multioutput='raw_values')\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN + LSTM Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 477.1819 - mae: 12.7312\n",
      "Epoch 1: val_loss improved from inf to 374.61707, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 21ms/step - loss: 477.0248 - mae: 12.7280 - val_loss: 374.6171 - val_mae: 10.5223\n",
      "Epoch 2/50\n",
      "\u001b[1m1482/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 378.4031 - mae: 10.5702\n",
      "Epoch 2: val_loss improved from 374.61707 to 372.81757, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 22ms/step - loss: 378.3961 - mae: 10.5699 - val_loss: 372.8176 - val_mae: 10.1336\n",
      "Epoch 3/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 368.5249 - mae: 10.2117\n",
      "Epoch 3: val_loss improved from 372.81757 to 371.32379, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - loss: 368.5264 - mae: 10.2118 - val_loss: 371.3238 - val_mae: 10.0474\n",
      "Epoch 4/50\n",
      "\u001b[1m1482/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 367.1640 - mae: 10.1659\n",
      "Epoch 4: val_loss improved from 371.32379 to 370.30838, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 23ms/step - loss: 367.1663 - mae: 10.1659 - val_loss: 370.3084 - val_mae: 10.0052\n",
      "Epoch 5/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 366.0695 - mae: 10.1250\n",
      "Epoch 5: val_loss improved from 370.30838 to 369.14462, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 22ms/step - loss: 366.0712 - mae: 10.1250 - val_loss: 369.1446 - val_mae: 9.9782\n",
      "Epoch 6/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 365.0858 - mae: 10.0968\n",
      "Epoch 6: val_loss improved from 369.14462 to 368.45947, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 22ms/step - loss: 365.0878 - mae: 10.0968 - val_loss: 368.4595 - val_mae: 9.9909\n",
      "Epoch 7/50\n",
      "\u001b[1m1482/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 364.3918 - mae: 10.0715\n",
      "Epoch 7: val_loss improved from 368.45947 to 368.29968, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 22ms/step - loss: 364.3943 - mae: 10.0715 - val_loss: 368.2997 - val_mae: 10.0814\n",
      "Epoch 8/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 363.8324 - mae: 10.0497\n",
      "Epoch 8: val_loss improved from 368.29968 to 367.60129, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 22ms/step - loss: 363.8343 - mae: 10.0497 - val_loss: 367.6013 - val_mae: 10.1091\n",
      "Epoch 9/50\n",
      "\u001b[1m1484/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 363.5324 - mae: 10.0350\n",
      "Epoch 9: val_loss did not improve from 367.60129\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 22ms/step - loss: 363.5338 - mae: 10.0350 - val_loss: 367.7028 - val_mae: 10.2170\n",
      "Epoch 10/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 362.9882 - mae: 10.0150\n",
      "Epoch 10: val_loss improved from 367.60129 to 367.27835, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - loss: 362.9901 - mae: 10.0150 - val_loss: 367.2784 - val_mae: 10.1992\n",
      "Epoch 11/50\n",
      "\u001b[1m1482/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 362.3654 - mae: 9.9973\n",
      "Epoch 11: val_loss did not improve from 367.27835\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 22ms/step - loss: 362.3681 - mae: 9.9973 - val_loss: 367.3480 - val_mae: 10.2490\n",
      "Epoch 12/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 361.9371 - mae: 9.9835\n",
      "Epoch 12: val_loss improved from 367.27835 to 367.07303, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - loss: 361.9391 - mae: 9.9835 - val_loss: 367.0730 - val_mae: 10.2403\n",
      "Epoch 13/50\n",
      "\u001b[1m1484/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 361.6487 - mae: 9.9735\n",
      "Epoch 13: val_loss improved from 367.07303 to 366.86636, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 22ms/step - loss: 361.6499 - mae: 9.9735 - val_loss: 366.8664 - val_mae: 10.2727\n",
      "Epoch 14/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 361.1404 - mae: 9.9596\n",
      "Epoch 14: val_loss improved from 366.86636 to 366.78604, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 22ms/step - loss: 361.1423 - mae: 9.9596 - val_loss: 366.7860 - val_mae: 10.2291\n",
      "Epoch 15/50\n",
      "\u001b[1m1482/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 360.3942 - mae: 9.9421\n",
      "Epoch 15: val_loss improved from 366.78604 to 365.89197, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - loss: 360.3962 - mae: 9.9421 - val_loss: 365.8920 - val_mae: 10.2360\n",
      "Epoch 16/50\n",
      "\u001b[1m1482/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 357.8905 - mae: 9.8984\n",
      "Epoch 16: val_loss did not improve from 365.89197\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - loss: 357.8951 - mae: 9.8984 - val_loss: 366.7747 - val_mae: 10.2152\n",
      "Epoch 17/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 359.3675 - mae: 9.9148\n",
      "Epoch 17: val_loss improved from 365.89197 to 360.07513, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - loss: 359.3657 - mae: 9.9148 - val_loss: 360.0751 - val_mae: 10.0948\n",
      "Epoch 18/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 352.6049 - mae: 9.7373\n",
      "Epoch 18: val_loss improved from 360.07513 to 358.59540, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - loss: 352.6065 - mae: 9.7373 - val_loss: 358.5954 - val_mae: 10.0211\n",
      "Epoch 19/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 351.0693 - mae: 9.6934\n",
      "Epoch 19: val_loss improved from 358.59540 to 358.35904, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 22ms/step - loss: 351.0710 - mae: 9.6934 - val_loss: 358.3590 - val_mae: 10.0293\n",
      "Epoch 20/50\n",
      "\u001b[1m1482/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 352.6315 - mae: 9.7350\n",
      "Epoch 20: val_loss did not improve from 358.35904\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - loss: 352.6419 - mae: 9.7352 - val_loss: 365.6234 - val_mae: 10.1273\n",
      "Epoch 21/50\n",
      "\u001b[1m1482/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 359.2357 - mae: 9.9097\n",
      "Epoch 21: val_loss did not improve from 358.35904\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 22ms/step - loss: 359.2376 - mae: 9.9097 - val_loss: 365.7406 - val_mae: 10.1869\n",
      "Epoch 22/50\n",
      "\u001b[1m1484/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 356.9683 - mae: 9.8530\n",
      "Epoch 22: val_loss did not improve from 358.35904\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 22ms/step - loss: 356.9668 - mae: 9.8530 - val_loss: 358.8032 - val_mae: 10.0052\n",
      "Epoch 23/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 350.9991 - mae: 9.6934\n",
      "Epoch 23: val_loss did not improve from 358.35904\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 22ms/step - loss: 350.9999 - mae: 9.6934 - val_loss: 358.5579 - val_mae: 9.9712\n",
      "Epoch 24/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 349.2690 - mae: 9.6500\n",
      "Epoch 24: val_loss improved from 358.35904 to 357.54663, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 22ms/step - loss: 349.2724 - mae: 9.6501 - val_loss: 357.5466 - val_mae: 9.9145\n",
      "Epoch 25/50\n",
      "\u001b[1m1484/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 347.6618 - mae: 9.6101\n",
      "Epoch 25: val_loss did not improve from 357.54663\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 22ms/step - loss: 347.6630 - mae: 9.6101 - val_loss: 357.5721 - val_mae: 9.9015\n",
      "Epoch 26/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 346.7373 - mae: 9.5892\n",
      "Epoch 26: val_loss did not improve from 357.54663\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 22ms/step - loss: 346.7394 - mae: 9.5892 - val_loss: 358.3925 - val_mae: 9.9131\n",
      "Epoch 27/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 346.3698 - mae: 9.5827\n",
      "Epoch 27: val_loss improved from 357.54663 to 357.03549, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 22ms/step - loss: 346.3714 - mae: 9.5827 - val_loss: 357.0355 - val_mae: 9.7583\n",
      "Epoch 28/50\n",
      "\u001b[1m1484/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 347.5279 - mae: 9.6159\n",
      "Epoch 28: val_loss did not improve from 357.03549\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 22ms/step - loss: 347.5325 - mae: 9.6160 - val_loss: 358.6852 - val_mae: 9.9371\n",
      "Epoch 29/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 349.1431 - mae: 9.6560\n",
      "Epoch 29: val_loss did not improve from 357.03549\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - loss: 349.1443 - mae: 9.6559 - val_loss: 357.2991 - val_mae: 9.8365\n",
      "Epoch 30/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 347.0549 - mae: 9.6047\n",
      "Epoch 30: val_loss improved from 357.03549 to 356.74506, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - loss: 347.0564 - mae: 9.6047 - val_loss: 356.7451 - val_mae: 9.8220\n",
      "Epoch 31/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 345.9240 - mae: 9.5788\n",
      "Epoch 31: val_loss improved from 356.74506 to 356.06516, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 22ms/step - loss: 345.9259 - mae: 9.5788 - val_loss: 356.0652 - val_mae: 9.8271\n",
      "Epoch 32/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 345.9571 - mae: 9.5835\n",
      "Epoch 32: val_loss did not improve from 356.06516\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 22ms/step - loss: 345.9615 - mae: 9.5835 - val_loss: 357.7085 - val_mae: 9.9090\n",
      "Epoch 33/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 346.5263 - mae: 9.6066\n",
      "Epoch 33: val_loss improved from 356.06516 to 354.86954, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 21ms/step - loss: 346.5272 - mae: 9.6065 - val_loss: 354.8695 - val_mae: 9.7848\n",
      "Epoch 34/50\n",
      "\u001b[1m1482/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 346.0724 - mae: 9.5896\n",
      "Epoch 34: val_loss improved from 354.86954 to 354.41409, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - loss: 346.0747 - mae: 9.5896 - val_loss: 354.4141 - val_mae: 9.7495\n",
      "Epoch 35/50\n",
      "\u001b[1m1482/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 347.8008 - mae: 9.6297\n",
      "Epoch 35: val_loss did not improve from 354.41409\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - loss: 347.8102 - mae: 9.6299 - val_loss: 357.6819 - val_mae: 9.8363\n",
      "Epoch 36/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 348.8672 - mae: 9.6498\n",
      "Epoch 36: val_loss did not improve from 354.41409\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - loss: 348.8683 - mae: 9.6498 - val_loss: 356.0440 - val_mae: 9.7716\n",
      "Epoch 37/50\n",
      "\u001b[1m1482/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 346.9764 - mae: 9.6017\n",
      "Epoch 37: val_loss did not improve from 354.41409\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step - loss: 346.9788 - mae: 9.6017 - val_loss: 356.1771 - val_mae: 9.7935\n",
      "Epoch 38/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 346.3963 - mae: 9.5877\n",
      "Epoch 38: val_loss did not improve from 354.41409\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 22ms/step - loss: 346.3979 - mae: 9.5877 - val_loss: 355.3997 - val_mae: 9.7326\n",
      "Epoch 39/50\n",
      "\u001b[1m1483/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 345.2555 - mae: 9.5595\n",
      "Epoch 39: val_loss did not improve from 354.41409\n",
      "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 22ms/step - loss: 345.2577 - mae: 9.5595 - val_loss: 355.4748 - val_mae: 9.7388\n",
      "Epoch 39: early stopping\n",
      "\u001b[1m9895/9895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - loss: 316.8660 - mae: 9.0074\n",
      "Test Mean Absolute Error: 9.206757545471191\n",
      "\u001b[1m9895/9895\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step\n",
      "Mean Absolute Error for each column:\n",
      "[17.409521    3.3328285   0.20278177  7.0646586  18.02365   ]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_aug = ModelCheckpoint(\"./models/hybrid_model_aug.keras\", monitor='val_loss', save_best_only=True, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define input layer\n",
    "input_layer_aug = Input(shape=(X_train_aug.shape[1], 1))\n",
    "\n",
    "# CNN model\n",
    "conv_layer_aug = Conv1D(filters=32, kernel_size=3, activation='relu')(input_layer_aug)\n",
    "maxpool_layer_aug = MaxPooling1D(pool_size=2)(conv_layer_aug)\n",
    "flatten_layer_aug = Flatten()(maxpool_layer_aug)\n",
    "dense_cnn_aug = Dense(32, activation='relu')(flatten_layer_aug)\n",
    "\n",
    "# BiLSTM model\n",
    "lstm_layer_aug = LSTM(64, activation='relu')(input_layer_aug)\n",
    "# lstm_layer2 = LSTM(32, activation='relu', return_sequences=False)(lstm_layer)\n",
    "dense_lstm_aug = Dense(32, activation='relu')(lstm_layer_aug)\n",
    "\n",
    "# Concatenate CNN and BiLSTM outputs\n",
    "concatenated_aug = Concatenate()([dense_cnn_aug, dense_lstm_aug])\n",
    "\n",
    "# Output layer\n",
    "output_layer_aug = Dense(5)(concatenated_aug)\n",
    "\n",
    "# Create the ensemble model\n",
    "hybrid_model_aug = Model(inputs=input_layer_aug, outputs=output_layer_aug)\n",
    "\n",
    "hybrid_model_aug.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history_aug = hybrid_model_aug.fit(\n",
    "    X_train_aug,\n",
    "    Y_train_aug,\n",
    "    epochs=50,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint_aug, early_stopping]\n",
    ")\n",
    "\n",
    "loss, mae = hybrid_model_aug.evaluate(X_test_aug, Y_test_aug)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred_aug = hybrid_model_aug.predict(X_test_aug)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test_aug, Y_pred_aug, multioutput='raw_values')\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1486/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 440.2392 - mae: 11.9459\n",
      "Epoch 1: val_loss improved from inf to 404.43927, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 440.1000 - mae: 11.9428 - val_loss: 404.4393 - val_mae: 10.4216\n",
      "Epoch 2/50\n",
      "\u001b[1m1488/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 371.1723 - mae: 10.3173\n",
      "Epoch 2: val_loss improved from 404.43927 to 391.91068, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 371.1712 - mae: 10.3172 - val_loss: 391.9107 - val_mae: 10.3450\n",
      "Epoch 3/50\n",
      "\u001b[1m1485/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 368.9981 - mae: 10.2475\n",
      "Epoch 3: val_loss improved from 391.91068 to 390.98267, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 368.9974 - mae: 10.2475 - val_loss: 390.9827 - val_mae: 10.5796\n",
      "Epoch 4/50\n",
      "\u001b[1m1485/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 367.4153 - mae: 10.1983\n",
      "Epoch 4: val_loss improved from 390.98267 to 388.45041, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 367.4143 - mae: 10.1983 - val_loss: 388.4504 - val_mae: 10.5422\n",
      "Epoch 5/50\n",
      "\u001b[1m1486/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 366.0706 - mae: 10.1537\n",
      "Epoch 5: val_loss improved from 388.45041 to 387.47495, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 366.0695 - mae: 10.1537 - val_loss: 387.4749 - val_mae: 10.5390\n",
      "Epoch 6/50\n",
      "\u001b[1m1484/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 364.5831 - mae: 10.1045\n",
      "Epoch 6: val_loss improved from 387.47495 to 383.81320, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 364.5800 - mae: 10.1044 - val_loss: 383.8132 - val_mae: 10.5546\n",
      "Epoch 7/50\n",
      "\u001b[1m1485/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 358.5017 - mae: 9.9551\n",
      "Epoch 7: val_loss improved from 383.81320 to 377.57495, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 358.4958 - mae: 9.9549 - val_loss: 377.5750 - val_mae: 10.3175\n",
      "Epoch 8/50\n",
      "\u001b[1m1485/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 354.7321 - mae: 9.8161\n",
      "Epoch 8: val_loss improved from 377.57495 to 376.41852, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 354.7298 - mae: 9.8160 - val_loss: 376.4185 - val_mae: 10.4585\n",
      "Epoch 9/50\n",
      "\u001b[1m1484/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 353.4142 - mae: 9.7750\n",
      "Epoch 9: val_loss improved from 376.41852 to 375.22522, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 353.4120 - mae: 9.7749 - val_loss: 375.2252 - val_mae: 10.1367\n",
      "Epoch 10/50\n",
      "\u001b[1m1485/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 352.8455 - mae: 9.7497\n",
      "Epoch 10: val_loss did not improve from 375.22522\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 352.8541 - mae: 9.7499 - val_loss: 386.4406 - val_mae: 10.4332\n",
      "Epoch 11/50\n",
      "\u001b[1m1485/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 364.2766 - mae: 10.0468\n",
      "Epoch 11: val_loss did not improve from 375.22522\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 364.2736 - mae: 10.0467 - val_loss: 384.0623 - val_mae: 10.4180\n",
      "Epoch 12/50\n",
      "\u001b[1m1486/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 358.2391 - mae: 9.8902\n",
      "Epoch 12: val_loss did not improve from 375.22522\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 358.2326 - mae: 9.8900 - val_loss: 376.4855 - val_mae: 10.1632\n",
      "Epoch 13/50\n",
      "\u001b[1m1484/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 353.0846 - mae: 9.7432\n",
      "Epoch 13: val_loss did not improve from 375.22522\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 353.0816 - mae: 9.7431 - val_loss: 376.7954 - val_mae: 10.2217\n",
      "Epoch 14/50\n",
      "\u001b[1m1488/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 351.7344 - mae: 9.7075\n",
      "Epoch 14: val_loss improved from 375.22522 to 374.84940, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 351.7337 - mae: 9.7074 - val_loss: 374.8494 - val_mae: 10.2066\n",
      "Epoch 15/50\n",
      "\u001b[1m1488/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 350.9687 - mae: 9.6854\n",
      "Epoch 15: val_loss did not improve from 374.84940\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 350.9678 - mae: 9.6854 - val_loss: 375.3769 - val_mae: 10.2498\n",
      "Epoch 16/50\n",
      "\u001b[1m1487/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 350.1807 - mae: 9.6658\n",
      "Epoch 16: val_loss improved from 374.84940 to 373.69800, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 350.1796 - mae: 9.6658 - val_loss: 373.6980 - val_mae: 10.2695\n",
      "Epoch 17/50\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 349.5353 - mae: 9.6525\n",
      "Epoch 17: val_loss improved from 373.69800 to 373.67822, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - loss: 349.5349 - mae: 9.6525 - val_loss: 373.6782 - val_mae: 10.2502\n",
      "Epoch 18/50\n",
      "\u001b[1m1486/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 349.0919 - mae: 9.6445\n",
      "Epoch 18: val_loss improved from 373.67822 to 373.34863, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 349.0903 - mae: 9.6444 - val_loss: 373.3486 - val_mae: 10.2964\n",
      "Epoch 19/50\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 348.6859 - mae: 9.6358\n",
      "Epoch 19: val_loss did not improve from 373.34863\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - loss: 348.6856 - mae: 9.6358 - val_loss: 373.7459 - val_mae: 10.3031\n",
      "Epoch 20/50\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 348.1870 - mae: 9.6242\n",
      "Epoch 20: val_loss improved from 373.34863 to 372.85471, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 348.1866 - mae: 9.6242 - val_loss: 372.8547 - val_mae: 10.2829\n",
      "Epoch 21/50\n",
      "\u001b[1m1487/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 347.9442 - mae: 9.6204\n",
      "Epoch 21: val_loss did not improve from 372.85471\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 347.9430 - mae: 9.6204 - val_loss: 373.5818 - val_mae: 10.3461\n",
      "Epoch 22/50\n",
      "\u001b[1m1485/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 347.5180 - mae: 9.6107\n",
      "Epoch 22: val_loss improved from 372.85471 to 372.82175, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 347.5161 - mae: 9.6107 - val_loss: 372.8217 - val_mae: 10.3060\n",
      "Epoch 23/50\n",
      "\u001b[1m1488/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 347.2531 - mae: 9.6048\n",
      "Epoch 23: val_loss did not improve from 372.82175\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 347.2524 - mae: 9.6047 - val_loss: 373.2059 - val_mae: 10.3303\n",
      "Epoch 24/50\n",
      "\u001b[1m1486/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 346.9042 - mae: 9.5968\n",
      "Epoch 24: val_loss did not improve from 372.82175\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - loss: 346.9026 - mae: 9.5967 - val_loss: 372.9239 - val_mae: 10.3404\n",
      "Epoch 25/50\n",
      "\u001b[1m1487/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 346.7529 - mae: 9.5969\n",
      "Epoch 25: val_loss improved from 372.82175 to 372.71222, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 346.7519 - mae: 9.5969 - val_loss: 372.7122 - val_mae: 10.3079\n",
      "Epoch 26/50\n",
      "\u001b[1m1487/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 346.4504 - mae: 9.5903\n",
      "Epoch 26: val_loss improved from 372.71222 to 372.70059, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 346.4494 - mae: 9.5903 - val_loss: 372.7006 - val_mae: 10.3291\n",
      "Epoch 27/50\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 346.1527 - mae: 9.5848\n",
      "Epoch 27: val_loss improved from 372.70059 to 372.62961, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 346.1523 - mae: 9.5847 - val_loss: 372.6296 - val_mae: 10.3406\n",
      "Epoch 28/50\n",
      "\u001b[1m1487/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 345.9082 - mae: 9.5800\n",
      "Epoch 28: val_loss improved from 372.62961 to 372.57059, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 345.9073 - mae: 9.5800 - val_loss: 372.5706 - val_mae: 10.3054\n",
      "Epoch 29/50\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 345.6891 - mae: 9.5771\n",
      "Epoch 29: val_loss improved from 372.57059 to 372.38266, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 345.6888 - mae: 9.5771 - val_loss: 372.3827 - val_mae: 10.1168\n",
      "Epoch 30/50\n",
      "\u001b[1m1486/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 346.8860 - mae: 9.5972\n",
      "Epoch 30: val_loss did not improve from 372.38266\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - loss: 346.8852 - mae: 9.5972 - val_loss: 374.8732 - val_mae: 10.3562\n",
      "Epoch 31/50\n",
      "\u001b[1m1488/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 346.5036 - mae: 9.5835\n",
      "Epoch 31: val_loss improved from 372.38266 to 372.28366, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - loss: 346.5024 - mae: 9.5834 - val_loss: 372.2837 - val_mae: 10.2694\n",
      "Epoch 32/50\n",
      "\u001b[1m1484/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 345.3079 - mae: 9.5692\n",
      "Epoch 32: val_loss did not improve from 372.28366\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - loss: 345.3054 - mae: 9.5691 - val_loss: 372.6505 - val_mae: 10.3154\n",
      "Epoch 33/50\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 345.1200 - mae: 9.5666\n",
      "Epoch 33: val_loss did not improve from 372.28366\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - loss: 345.1197 - mae: 9.5666 - val_loss: 372.8798 - val_mae: 10.3382\n",
      "Epoch 34/50\n",
      "\u001b[1m1488/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 344.9017 - mae: 9.5625\n",
      "Epoch 34: val_loss did not improve from 372.28366\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 344.9011 - mae: 9.5625 - val_loss: 372.6556 - val_mae: 10.3256\n",
      "Epoch 35/50\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 345.0730 - mae: 9.5639\n",
      "Epoch 35: val_loss did not improve from 372.28366\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 345.0725 - mae: 9.5639 - val_loss: 372.8480 - val_mae: 10.3150\n",
      "Epoch 36/50\n",
      "\u001b[1m1487/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 344.7428 - mae: 9.5592\n",
      "Epoch 36: val_loss did not improve from 372.28366\n",
      "\u001b[1m1489/1489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - loss: 344.7417 - mae: 9.5592 - val_loss: 372.6928 - val_mae: 10.3049\n",
      "Epoch 36: early stopping\n",
      "\u001b[1m9924/9924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1ms/step - loss: 341.8192 - mae: 9.9869\n",
      "Test Mean Absolute Error: 9.846953392028809\n",
      "\u001b[1m9924/9924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1ms/step\n",
      "Mean Absolute Error for each column:\n",
      "[16.781525    3.9432178   0.31193873  8.035642   20.162024  ]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"./models/hybrid_model.keras\", monitor='val_loss', save_best_only=True, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define input layer\n",
    "input_layer = Input(shape=(X_train.shape[1], 1))\n",
    "\n",
    "# CNN model\n",
    "conv_layer = Conv1D(filters=32, kernel_size=3, activation='relu')(input_layer)\n",
    "maxpool_layer = MaxPooling1D(pool_size=2)(conv_layer)\n",
    "flatten_layer = Flatten()(maxpool_layer)\n",
    "dense_cnn = Dense(32, activation='relu')(flatten_layer)\n",
    "\n",
    "# BiLSTM model\n",
    "lstm_layer = LSTM(64, activation='relu')(input_layer)\n",
    "# lstm_layer2 = LSTM(32, activation='relu', return_sequences=False)(lstm_layer)\n",
    "dense_lstm = Dense(32, activation='relu')(lstm_layer)\n",
    "\n",
    "# Concatenate CNN and BiLSTM outputs\n",
    "concatenated = Concatenate()([dense_cnn, dense_lstm])\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(5)(concatenated)\n",
    "\n",
    "# Create the ensemble model\n",
    "hybrid_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "hybrid_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history = hybrid_model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=50,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint, early_stopping]\n",
    ")\n",
    "\n",
    "loss, mae = hybrid_model.evaluate(X_test, Y_test)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred = hybrid_model.predict(X_test)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test, Y_pred, multioutput='raw_values')\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN + LSTM Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix shape: (19213, 19213)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────\n",
    "MASTER_COORD = \"./Datasets/T_MASTER_CORD.csv\"\n",
    "K_NEIGHBORS  = 5      # connect each airport to its 5 closest neighbors\n",
    "EARTH_R      = 6371.0 # km\n",
    "\n",
    "# ─── 1) load airport coordinates ────────────────────────────────\n",
    "mc = (\n",
    "    pd.read_csv(MASTER_COORD, dtype=str)\n",
    "      .set_index(\"AIRPORT_SEQ_ID\")[[\"LATITUDE\",\"LONGITUDE\"]]\n",
    "      .astype(float)\n",
    ")\n",
    "# list of IDs and arrays of lat/lon in radians\n",
    "ids  = mc.index.to_list()\n",
    "lats = np.deg2rad(mc[\"LATITUDE\"].values)\n",
    "lons = np.deg2rad(mc[\"LONGITUDE\"].values)\n",
    "N    = len(ids)\n",
    "\n",
    "# ─── 2) compute pairwise haversine distances ────────────────────\n",
    "# using the vectorized formula\n",
    "lat1 = lats[:, None]\n",
    "lat2 = lats[None, :]\n",
    "dlon = lons[None, :] - lons[:, None]\n",
    "\n",
    "dlat = lat2 - lat1\n",
    "a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "c = 2 * np.arcsin(np.sqrt(a))\n",
    "dist_km = EARTH_R * c  # shape (N, N)\n",
    "\n",
    "# ─── 3) build adjacency by k-nearest neighbors ───────────────────\n",
    "A = np.zeros((N, N), dtype=np.float32)\n",
    "\n",
    "for i in range(N):\n",
    "    # argsort returns i itself at position 0, so skip it\n",
    "    neighbors = np.argsort(dist_km[i])[1 : K_NEIGHBORS+1]\n",
    "    A[i, neighbors] = 1.0\n",
    "\n",
    "# symmetrize: if i→j or j→i, keep edge both ways\n",
    "A = np.maximum(A, A.T)\n",
    "\n",
    "# ─── 4) (Optional) row-normalize adjacency ──────────────────────\n",
    "# so each row sums to 1\n",
    "row_sums = A.sum(axis=1, keepdims=True)\n",
    "A_norm   = A / np.where(row_sums>0, row_sums, 1.0)\n",
    "\n",
    "# Now `A_norm` is your adjacency matrix to feed into the GNN\n",
    "print(\"Adjacency matrix shape:\", A_norm.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable(package=\"Custom\", name=\"SimpleGraphConv\")\n",
    "class SimpleGraphConv(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape = [(batch, N, F), (batch, N, N)]\n",
    "        F = input_shape[0][-1]\n",
    "        self.w = self.add_weight(\n",
    "            shape=(F, self.units),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            name=\"kernel\")\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X, A = inputs    # X: (batch, N, F), A: (batch, N, N)\n",
    "        return tf.matmul(A, tf.matmul(X, self.w))\n",
    "\n",
    "\n",
    "@register_keras_serializable(package=\"Custom\", name=\"GCNTimeDistributed\")\n",
    "class GCNTimeDistributed(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        # we’ll reuse this conv at each time slice\n",
    "        self.gcn = SimpleGraphConv(units)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape = [(batch, T, N, F), (batch, N, N)]\n",
    "        # we need to build the inner GCN on a flattened time‐slice shape:\n",
    "        # pretend batch' = None and time = 1 so shape = (None, N, F) & (None, N, N)\n",
    "        _, T, N, F = input_shape[0]\n",
    "        # call inner build:\n",
    "        self.gcn.build([(None, N, F), (None, N, N)])\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X, A = inputs\n",
    "        batch = tf.shape(X)[0]\n",
    "        T     = tf.shape(X)[1]\n",
    "        N     = tf.shape(X)[2]\n",
    "        F     = tf.shape(X)[3]\n",
    "\n",
    "        # 1) collapse time\n",
    "        Xr = tf.reshape(X, (batch * T, N, F))\n",
    "        # 2) tile adjacency\n",
    "        Aexp = tf.expand_dims(A, 1)               # (batch, 1, N, N)\n",
    "        Atil = tf.tile(Aexp, [1, T, 1, 1])         # (batch, T, N, N)\n",
    "        Ar   = tf.reshape(Atil, (batch * T, N, N))\n",
    "        # 3) apply GCN\n",
    "        Yr = self.gcn([Xr, Ar])                    # (batch*T, N, units)\n",
    "        # 4) restore time axis\n",
    "        return tf.reshape(Yr, (batch, T, N, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_mae\",\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build GNN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(952611, 1, 2, 5)\n"
     ]
    }
   ],
   "source": [
    "# 1) decide T, N, F\n",
    "T = 1       # one snapshot per flight\n",
    "N = 2       # origin + destination nodes\n",
    "F = 5       # the five weather vars you listed\n",
    "\n",
    "A_np = A_norm\n",
    "gcn_units = 32\n",
    "\n",
    "X_in = Input(shape=(T, N, F), name=\"node_features\")       # dynamic nodal time‐series\n",
    "A_in = Input(shape=(N, N),    name=\"adjacency_matrix\")    # can be broadcast\n",
    "\n",
    "weather_cols = X_train.columns[: N * F]  # first 10 columns\n",
    "assert len(weather_cols) == N * F\n",
    "\n",
    "# ─── C) extract and reshape ───────────────────────────────────────────────────\n",
    "weather_train = X_train[weather_cols].to_numpy()  # shape (n_samples, 10)\n",
    "weather_test  = X_test[ weather_cols].to_numpy()\n",
    "\n",
    "# reshape into (batch, T, N, F)\n",
    "X_train_gnn = weather_train.reshape(-1, T, N, F)\n",
    "X_test_gnn  = weather_test.reshape(-1, T, N, F)\n",
    "\n",
    "print(X_train_gnn.shape)  # → (952611, 1, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 2a) GCN‐over‐time layer 1\n",
    "g = GCNTimeDistributed(gcn_units, name=\"time_gcn\")([X_in, A_in])\n",
    "\n",
    "# ─── (optional) 2nd GCN‐over‐time\n",
    "g = GCNTimeDistributed(gcn_units, name=\"time_gcn2\")([g, A_in])\n",
    "\n",
    "# ─── 2b) flatten per‐time‐step but keep T\n",
    "\n",
    "g_flat = TimeDistributed(Flatten(), name=\"flatten_nodes\")(g)  \n",
    "# shape = (batch, T, N * gcn_units)\n",
    "\n",
    "# ─── 2c) LSTM\n",
    "h = LSTM(64, name=\"temporal_lstm\")(g_flat)  # now g_flat is 3D\n",
    "\n",
    "# ─── 2d) final Dense (make sure units=Y_train.shape[1])\n",
    "Y_dim = Y_train.shape[1]\n",
    "out = Dense(Y_dim, name=\"output\")(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"GNN_LSTM_Hybrid\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"GNN_LSTM_Hybrid\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ node_features       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ adjacency_matrix    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_gcn            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │ node_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GCNTimeDistribute…</span> │                   │            │ adjacency_matrix… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_gcn2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ time_gcn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GCNTimeDistribute…</span> │                   │            │ adjacency_matrix… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_nodes       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_gcn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ temporal_lstm       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ flatten_nodes[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)              │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │ temporal_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ node_features       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ adjacency_matrix    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_gcn            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │        \u001b[38;5;34m160\u001b[0m │ node_features[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mGCNTimeDistribute…\u001b[0m │                   │            │ adjacency_matrix… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_gcn2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │      \u001b[38;5;34m1,024\u001b[0m │ time_gcn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│ (\u001b[38;5;33mGCNTimeDistribute…\u001b[0m │                   │            │ adjacency_matrix… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_nodes       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ time_gcn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ temporal_lstm       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m33,024\u001b[0m │ flatten_nodes[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mLSTM\u001b[0m)              │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │        \u001b[38;5;34m325\u001b[0m │ temporal_lstm[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,533</span> (134.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,533\u001b[0m (134.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,533</span> (134.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,533\u001b[0m (134.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ─── 3) Compile, train & evaluate ────────────────────────────────────────────\n",
    "\n",
    "gnn_lstm = Model([X_in, A_in], out, name=\"GNN_LSTM_Hybrid\")\n",
    "gnn_lstm.summary()\n",
    "gnn_lstm.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=\"./models/gnn_lstm.keras\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_train shape: (952611, 2, 2)\n",
      "Epoch 1/50\n",
      "\u001b[1m23813/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 429.5786 - mae: 11.1594\n",
      "Epoch 1: val_loss improved from inf to 437.54355, saving model to ./models/gnn_lstm.keras\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2ms/step - loss: 429.5766 - mae: 11.1595 - val_loss: 437.5435 - val_mae: 11.7251 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m23799/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5328 - mae: 11.4718\n",
      "Epoch 2: val_loss did not improve from 437.54355\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2ms/step - loss: 414.5326 - mae: 11.4718 - val_loss: 437.5435 - val_mae: 11.7251 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m23810/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5327 - mae: 11.4718\n",
      "Epoch 3: val_loss did not improve from 437.54355\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2ms/step - loss: 414.5326 - mae: 11.4718 - val_loss: 437.5435 - val_mae: 11.7251 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m23793/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5329 - mae: 11.4718\n",
      "Epoch 4: val_loss did not improve from 437.54355\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2ms/step - loss: 414.5326 - mae: 11.4718 - val_loss: 437.5435 - val_mae: 11.7251 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m23787/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5221 - mae: 11.4709\n",
      "Epoch 5: val_loss improved from 437.54355 to 437.45560, saving model to ./models/gnn_lstm.keras\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2ms/step - loss: 414.5218 - mae: 11.4709 - val_loss: 437.4556 - val_mae: 11.7281 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m23805/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5215 - mae: 11.4716\n",
      "Epoch 6: val_loss did not improve from 437.45560\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2ms/step - loss: 414.5214 - mae: 11.4716 - val_loss: 437.4556 - val_mae: 11.7281 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m23788/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5217 - mae: 11.4716\n",
      "Epoch 7: val_loss did not improve from 437.45560\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2ms/step - loss: 414.5214 - mae: 11.4716 - val_loss: 437.4556 - val_mae: 11.7281 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m23786/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5164 - mae: 11.4707\n",
      "Epoch 8: val_loss improved from 437.45560 to 437.40884, saving model to ./models/gnn_lstm.keras\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2ms/step - loss: 414.5161 - mae: 11.4707 - val_loss: 437.4088 - val_mae: 11.7325 - learning_rate: 2.5000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m23814/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5162 - mae: 11.4719\n",
      "Epoch 9: val_loss did not improve from 437.40884\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2ms/step - loss: 414.5162 - mae: 11.4719 - val_loss: 437.4088 - val_mae: 11.7325 - learning_rate: 2.5000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m23804/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5163 - mae: 11.4719\n",
      "Epoch 10: val_loss did not improve from 437.40884\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2ms/step - loss: 414.5162 - mae: 11.4719 - val_loss: 437.4088 - val_mae: 11.7325 - learning_rate: 2.5000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m23802/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5141 - mae: 11.4715\n",
      "Epoch 11: val_loss did not improve from 437.40884\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2ms/step - loss: 414.5139 - mae: 11.4715 - val_loss: 437.4165 - val_mae: 11.7334 - learning_rate: 1.2500e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m23809/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5135 - mae: 11.4723\n",
      "Epoch 12: val_loss did not improve from 437.40884\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2ms/step - loss: 414.5134 - mae: 11.4723 - val_loss: 437.4165 - val_mae: 11.7334 - learning_rate: 1.2500e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m23794/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 414.5136 - mae: 11.4723\n",
      "Epoch 13: val_loss did not improve from 437.40884\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m23816/23816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2ms/step - loss: 414.5134 - mae: 11.4723 - val_loss: 437.4165 - val_mae: 11.7334 - learning_rate: 1.2500e-04\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Test loss, MAE: [365.7720947265625, 10.890806198120117]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# For origin ↔ dest only:\n",
    "A2 = np.array([[0., 1.],\n",
    "               [1., 0.]], dtype=np.float32)\n",
    "\n",
    "# Tile it for every sample in the train/test set:\n",
    "A_train = np.tile(A2[None], (X_train_gnn.shape[0], 1, 1))  # (952611,2,2)\n",
    "A_test  = np.tile(A2[None], (X_test_gnn.shape[0],  1, 1))  # (… likewise)\n",
    "\n",
    "print(\"A_train shape:\", A_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Fit just like you did your LSTM models:\n",
    "history = gnn_lstm.fit(\n",
    "    [X_train_gnn, A_train],\n",
    "    Y_train,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr],\n",
    ")\n",
    "\n",
    "test_results = gnn_lstm.evaluate(\n",
    "    [X_test_gnn, A_test],\n",
    "    Y_test,\n",
    "    verbose=0,\n",
    ")\n",
    "print(\"Test loss, MAE:\", test_results)\n",
    "test_results.to_csv(\"/models/results/gnn_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GNN training data → ./data/\n"
     ]
    }
   ],
   "source": [
    "history.history.to_csv(\"/models/results/gnn_test.csv\")\n",
    "np.save(\"./data/X_train_gnn.npy\", X_train_gnn)\n",
    "np.save(\"./data/A_train.npy\",     A_train)\n",
    "np.save(\"./data/Y_train.npy\",     Y_train)\n",
    "\n",
    "np.save(\"./data/X_test_gnn.npy\", X_test_gnn)\n",
    "np.save(\"./data/A_test.npy\",     A_test)\n",
    "np.save(\"./data/Y_test.npy\",     Y_test)\n",
    "\n",
    "print(\"Saved GNN training data → ./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(949836, 1, 2, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) decide T, N, F\n",
    "T = 1       # one snapshot per flight\n",
    "N = 2       # origin + destination nodes\n",
    "F = 5       # the five weather vars you listed\n",
    "\n",
    "\n",
    "A_np_aug = A_norm\n",
    "gcn_units = 32\n",
    "\n",
    "# Inputs\n",
    "X_in_aug = Input(shape=(T, N, F), name=\"node_features\")       # dynamic nodal time‐series\n",
    "A_in_aug = Input(shape=(N, N),    name=\"adjacency_matrix\")    # can be broadcast\n",
    "\n",
    "\n",
    "# 2) pull out just the 10 weather columns from your 11\n",
    "#    (assumes X_train[:, :10] are [orig_var0..4, dest_var0..4])\n",
    "weather_cols_aug = X_train_aug.columns[: N * F]  # first 10 columns\n",
    "assert len(weather_cols_aug) == N * F\n",
    "\n",
    "# ─── C) extract and reshape ───────────────────────────────────────────────────\n",
    "weather_train_aug = X_train_aug[weather_cols_aug].to_numpy()  # shape (n_samples, 10)\n",
    "weather_test_aug  = X_test_aug[ weather_cols_aug].to_numpy()\n",
    "\n",
    "# reshape into (batch, T, N, F)\n",
    "X_train_gnn_aug = weather_train_aug.reshape(-1, T, N, F)\n",
    "X_test_gnn_aug  = weather_test_aug.reshape(-1, T, N, F)\n",
    "\n",
    "print(X_train_gnn_aug.shape)  # → (952611, 1, 2, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 2) Build the Graph-LSTM hybrid ───────────────────────────────────────────\n",
    "\n",
    "# ─── 2a) GCN‐over‐time layer 1\n",
    "g_aug = GCNTimeDistributed(gcn_units, name=\"time_gcn\")([X_in_aug, A_in_aug])\n",
    "\n",
    "# ─── (optional) 2nd GCN‐over‐time\n",
    "#g_aug = GCNTimeDistributed(gcn_units, name=\"time_gcn2\")([g_aug, A_in_aug])\n",
    "\n",
    "# ─── 2b) flatten per‐time‐step but keep T\n",
    "\n",
    "g_flat_aug = TimeDistributed(Flatten(), name=\"flatten_nodes\")(g_aug)  \n",
    "# shape = (batch, T, N * gcn_units)\n",
    "\n",
    "# ─── 2c) LSTM\n",
    "h_aug = LSTM(64, name=\"temporal_lstm\")(g_flat_aug)  # now g_flat is 3D\n",
    "\n",
    "# ─── 2d) final Dense (make sure units=Y_train.shape[1])\n",
    "Y_dim = Y_train_aug.shape[1]\n",
    "out_aug = Dense(Y_dim, name=\"output\")(h_aug)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"GNN_LSTM_Hybrid\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"GNN_LSTM_Hybrid\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ node_features       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ adjacency_matrix    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_gcn            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │ node_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GCNTimeDistribute…</span> │                   │            │ adjacency_matrix… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_nodes       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_gcn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ temporal_lstm       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ flatten_nodes[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)              │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │ temporal_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ node_features       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ adjacency_matrix    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_gcn            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │        \u001b[38;5;34m160\u001b[0m │ node_features[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mGCNTimeDistribute…\u001b[0m │                   │            │ adjacency_matrix… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_nodes       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ time_gcn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ temporal_lstm       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m33,024\u001b[0m │ flatten_nodes[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mLSTM\u001b[0m)              │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │        \u001b[38;5;34m325\u001b[0m │ temporal_lstm[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,509</span> (130.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m33,509\u001b[0m (130.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,509</span> (130.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m33,509\u001b[0m (130.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ─── 3) Compile, train & evaluate ────────────────────────────────────────────\n",
    "\n",
    "# Assemble\n",
    "gnn_lstm_aug = Model(inputs=[X_in_aug, A_in_aug], outputs=out_aug, name=\"GNN_LSTM_Hybrid\")\n",
    "gnn_lstm_aug.summary()\n",
    "\n",
    "\n",
    "gnn_lstm_aug.compile(optimizer=\"adam\",loss=\"mse\",metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=\"./models/gnn_lstm_aug.keras\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_train_aug shape: (949836, 2, 2)\n",
      "Epoch 1/50\n",
      "\u001b[1m23717/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 422.3806 - mae: 11.1153 - mse: 422.3806\n",
      "Epoch 1: val_loss improved from inf to 417.75284, saving model to ./models/gnn_lstm_aug.keras\n",
      "\u001b[1m23746/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2ms/step - loss: 422.3701 - mae: 11.1155 - mse: 422.3701 - val_loss: 417.7528 - val_mae: 11.4811 - val_mse: 417.7528 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m23726/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 410.2379 - mae: 11.3897 - mse: 410.2379\n",
      "Epoch 2: val_loss did not improve from 417.75284\n",
      "\u001b[1m23746/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2ms/step - loss: 410.2389 - mae: 11.3897 - mse: 410.2389 - val_loss: 417.7528 - val_mae: 11.4811 - val_mse: 417.7528 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m23734/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 410.2383 - mae: 11.3897 - mse: 410.2383\n",
      "Epoch 3: val_loss did not improve from 417.75284\n",
      "\u001b[1m23746/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2ms/step - loss: 410.2389 - mae: 11.3897 - mse: 410.2389 - val_loss: 417.7528 - val_mae: 11.4811 - val_mse: 417.7528 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m23746/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 410.2388 - mae: 11.3897 - mse: 410.2388\n",
      "Epoch 4: val_loss did not improve from 417.75284\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m23746/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2ms/step - loss: 410.2389 - mae: 11.3897 - mse: 410.2389 - val_loss: 417.7528 - val_mae: 11.4811 - val_mse: 417.7528 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m23720/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 410.2266 - mae: 11.3891 - mse: 410.2266\n",
      "Epoch 5: val_loss improved from 417.75284 to 417.60925, saving model to ./models/gnn_lstm_aug.keras\n",
      "\u001b[1m23746/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2ms/step - loss: 410.2280 - mae: 11.3892 - mse: 410.2280 - val_loss: 417.6093 - val_mae: 11.4908 - val_mse: 417.6093 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m23725/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 410.2251 - mae: 11.3898 - mse: 410.2251\n",
      "Epoch 6: val_loss did not improve from 417.60925\n",
      "\u001b[1m23746/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2ms/step - loss: 410.2262 - mae: 11.3898 - mse: 410.2262 - val_loss: 417.6093 - val_mae: 11.4908 - val_mse: 417.6093 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m23745/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 410.2261 - mae: 11.3898 - mse: 410.2261\n",
      "Epoch 7: val_loss did not improve from 417.60925\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m23746/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2ms/step - loss: 410.2262 - mae: 11.3898 - mse: 410.2262 - val_loss: 417.6093 - val_mae: 11.4908 - val_mse: 417.6093 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m23736/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 410.2192 - mae: 11.3894 - mse: 410.2192\n",
      "Epoch 8: val_loss improved from 417.60925 to 417.53690, saving model to ./models/gnn_lstm_aug.keras\n",
      "\u001b[1m23746/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2ms/step - loss: 410.2198 - mae: 11.3894 - mse: 410.2198 - val_loss: 417.5369 - val_mae: 11.4979 - val_mse: 417.5369 - learning_rate: 2.5000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m23743/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 410.2191 - mae: 11.3898 - mse: 410.2191\n",
      "Epoch 9: val_loss did not improve from 417.53690\n",
      "\u001b[1m23746/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2ms/step - loss: 410.2193 - mae: 11.3898 - mse: 410.2193 - val_loss: 417.5369 - val_mae: 11.4979 - val_mse: 417.5369 - learning_rate: 2.5000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m23737/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 410.2188 - mae: 11.3898 - mse: 410.2188\n",
      "Epoch 10: val_loss did not improve from 417.53690\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m23746/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - loss: 410.2193 - mae: 11.3898 - mse: 410.2193 - val_loss: 417.5369 - val_mae: 11.4979 - val_mse: 417.5369 - learning_rate: 2.5000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m23745/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 410.2165 - mae: 11.3895 - mse: 410.2165\n",
      "Epoch 11: val_loss improved from 417.53690 to 417.51971, saving model to ./models/gnn_lstm_aug.keras\n",
      "\u001b[1m23746/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - loss: 410.2166 - mae: 11.3895 - mse: 410.2166 - val_loss: 417.5197 - val_mae: 11.5036 - val_mse: 417.5197 - learning_rate: 1.2500e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m23737/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 410.2169 - mae: 11.3903 - mse: 410.2169\n",
      "Epoch 12: val_loss did not improve from 417.51971\n",
      "\u001b[1m23746/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - loss: 410.2173 - mae: 11.3903 - mse: 410.2173 - val_loss: 417.5197 - val_mae: 11.5036 - val_mse: 417.5197 - learning_rate: 1.2500e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m23744/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 410.2172 - mae: 11.3903 - mse: 410.2172\n",
      "Epoch 13: val_loss improved from 417.51971 to 417.51965, saving model to ./models/gnn_lstm_aug.keras\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m23746/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - loss: 410.2173 - mae: 11.3903 - mse: 410.2173 - val_loss: 417.5197 - val_mae: 11.5036 - val_mse: 417.5197 - learning_rate: 1.2500e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m23719/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 410.2155 - mae: 11.3905 - mse: 410.2155\n",
      "Epoch 14: val_loss did not improve from 417.51965\n",
      "\u001b[1m23746/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2ms/step - loss: 410.2168 - mae: 11.3905 - mse: 410.2168 - val_loss: 417.5296 - val_mae: 11.5073 - val_mse: 417.5296 - learning_rate: 6.2500e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m23722/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 410.2151 - mae: 11.3917 - mse: 410.2151\n",
      "Epoch 15: val_loss did not improve from 417.51965\n",
      "\u001b[1m23746/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 410.2163 - mae: 11.3917 - mse: 410.2163 - val_loss: 417.5298 - val_mae: 11.5073 - val_mse: 417.5298 - learning_rate: 6.2500e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m23715/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 410.2148 - mae: 11.3917 - mse: 410.2148\n",
      "Epoch 16: val_loss did not improve from 417.51965\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m23746/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - loss: 410.2163 - mae: 11.3917 - mse: 410.2163 - val_loss: 417.5298 - val_mae: 11.5073 - val_mse: 417.5298 - learning_rate: 6.2500e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m23740/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 410.2158 - mae: 11.3927 - mse: 410.2158\n",
      "Epoch 17: val_loss did not improve from 417.51965\n",
      "\u001b[1m23746/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 410.2161 - mae: 11.3927 - mse: 410.2161 - val_loss: 417.5425 - val_mae: 11.5074 - val_mse: 417.5425 - learning_rate: 3.1250e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m23730/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 410.2145 - mae: 11.3928 - mse: 410.2145\n",
      "Epoch 18: val_loss did not improve from 417.51965\n",
      "\u001b[1m23746/23746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 410.2153 - mae: 11.3929 - mse: 410.2153 - val_loss: 417.5431 - val_mae: 11.5076 - val_mse: 417.5431 - learning_rate: 3.1250e-05\n",
      "Epoch 18: early stopping\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "Aug Test loss, MAE: [384.61846923828125, 11.09146785736084, 384.61846923828125]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# For origin ↔ dest only:\n",
    "A2 = np.array([[0., 1.],\n",
    "               [1., 0.]], dtype=np.float32)\n",
    "\n",
    "# Tile it for every sample in the train/test set:\n",
    "A_train_aug = np.tile(A2[None], (X_train_gnn_aug.shape[0], 1, 1))  # (952611,2,2)\n",
    "A_test_aug  = np.tile(A2[None], (X_test_gnn_aug.shape[0],  1, 1))  # (… likewise)\n",
    "\n",
    "print(\"A_train_aug shape:\", A_train_aug.shape)\n",
    "\n",
    "\n",
    "# Fit just like you did your LSTM models:\n",
    "history_aug = gnn_lstm_aug.fit(\n",
    "    [X_train_gnn_aug, A_train_aug],\n",
    "    Y_train_aug,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr],\n",
    ")\n",
    "\n",
    "# Evaluate on test:\n",
    "test_results_aug = gnn_lstm_aug.evaluate(\n",
    "    [X_test_gnn_aug, A_test_aug],\n",
    "    Y_test_aug,\n",
    "    verbose=0,\n",
    ")\n",
    "print(\"Aug Test loss, MAE:\", test_results_aug)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GNN aug training data → ./data/\n"
     ]
    }
   ],
   "source": [
    "history_aug.history.to_csv(\"/models/results/gnn_test_aug.csv\")\n",
    "\n",
    "np.save(\"./data/X_train_gnn_aug.npy\", X_train_gnn_aug)\n",
    "np.save(\"./data/A_train_aug.npy\",     A_train_aug)\n",
    "np.save(\"./data/Y_train_aug.npy\",     Y_train_aug)\n",
    "\n",
    "np.save(\"./data/X_test_gnn_aug.npy\", X_test_gnn_aug)\n",
    "np.save(\"./data/A_test_aug.npy\",     A_test_aug)\n",
    "np.save(\"./data/Y_test_aug.npy\",     Y_test_aug)\n",
    "\n",
    "print(\"Saved GNN aug training data → ./data/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
