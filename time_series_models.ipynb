{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Bidirectional, Conv1D, MaxPooling1D, Flatten, Concatenate\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"./data/X_train_lstm.csv\")\n",
    "X_test = pd.read_csv(\"./data/X_test_lstm.csv\")\n",
    "\n",
    "Y_train = pd.read_csv(\"./data/Y_train_lstm.csv\")\n",
    "Y_test = pd.read_csv(\"./data/Y_test_lstm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 401.9368 - mae: 10.8933\n",
      "Epoch 1: val_loss improved from inf to 378.16812, saving model to ../models\\lstm_model.keras\n",
      "1110/1110 [==============================] - 46s 40ms/step - loss: 401.9368 - mae: 10.8933 - val_loss: 378.1681 - val_mae: 10.1630\n",
      "Epoch 2/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 380.4790 - mae: 10.3173\n",
      "Epoch 2: val_loss improved from 378.16812 to 374.56149, saving model to ../models\\lstm_model.keras\n",
      "1110/1110 [==============================] - 44s 40ms/step - loss: 380.4741 - mae: 10.3172 - val_loss: 374.5615 - val_mae: 9.9608\n",
      "Epoch 3/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 377.6657 - mae: 10.2380\n",
      "Epoch 3: val_loss improved from 374.56149 to 372.89502, saving model to ../models\\lstm_model.keras\n",
      "1110/1110 [==============================] - 44s 40ms/step - loss: 377.6783 - mae: 10.2381 - val_loss: 372.8950 - val_mae: 10.2404\n",
      "Epoch 4/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 376.1225 - mae: 10.1909\n",
      "Epoch 4: val_loss improved from 372.89502 to 372.48251, saving model to ../models\\lstm_model.keras\n",
      "1110/1110 [==============================] - 44s 39ms/step - loss: 376.1383 - mae: 10.1911 - val_loss: 372.4825 - val_mae: 10.0072\n",
      "Epoch 5/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 375.1248 - mae: 10.1545\n",
      "Epoch 5: val_loss improved from 372.48251 to 369.26303, saving model to ../models\\lstm_model.keras\n",
      "1110/1110 [==============================] - 44s 39ms/step - loss: 375.1184 - mae: 10.1545 - val_loss: 369.2630 - val_mae: 10.1609\n",
      "Epoch 6/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 374.2487 - mae: 10.1324\n",
      "Epoch 6: val_loss did not improve from 369.26303\n",
      "1110/1110 [==============================] - 44s 40ms/step - loss: 374.2590 - mae: 10.1325 - val_loss: 372.0356 - val_mae: 10.1161\n",
      "Epoch 7/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 373.6910 - mae: 10.1170\n",
      "Epoch 7: val_loss improved from 369.26303 to 368.16092, saving model to ../models\\lstm_model.keras\n",
      "1110/1110 [==============================] - 44s 39ms/step - loss: 373.6871 - mae: 10.1169 - val_loss: 368.1609 - val_mae: 10.0448\n",
      "Epoch 8/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 373.1266 - mae: 10.1000\n",
      "Epoch 8: val_loss improved from 368.16092 to 368.10052, saving model to ../models\\lstm_model.keras\n",
      "1110/1110 [==============================] - 44s 40ms/step - loss: 373.1238 - mae: 10.1000 - val_loss: 368.1005 - val_mae: 9.9579\n",
      "Epoch 9/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 372.6132 - mae: 10.0889\n",
      "Epoch 9: val_loss improved from 368.10052 to 367.97589, saving model to ../models\\lstm_model.keras\n",
      "1110/1110 [==============================] - 44s 40ms/step - loss: 372.6110 - mae: 10.0888 - val_loss: 367.9759 - val_mae: 9.8390\n",
      "Epoch 10/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 372.5141 - mae: 10.0818\n",
      "Epoch 10: val_loss improved from 367.97589 to 367.61368, saving model to ../models\\lstm_model.keras\n",
      "1110/1110 [==============================] - 44s 40ms/step - loss: 372.5253 - mae: 10.0819 - val_loss: 367.6137 - val_mae: 10.1031\n",
      "Epoch 11/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 371.8716 - mae: 10.0713\n",
      "Epoch 11: val_loss did not improve from 367.61368\n",
      "1110/1110 [==============================] - 45s 40ms/step - loss: 371.8840 - mae: 10.0714 - val_loss: 369.3571 - val_mae: 10.2995\n",
      "Epoch 12/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 371.6692 - mae: 10.0648\n",
      "Epoch 12: val_loss did not improve from 367.61368\n",
      "1110/1110 [==============================] - 45s 41ms/step - loss: 371.6636 - mae: 10.0647 - val_loss: 367.9030 - val_mae: 9.9121\n",
      "Epoch 13/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 371.3475 - mae: 10.0547\n",
      "Epoch 13: val_loss did not improve from 367.61368\n",
      "1110/1110 [==============================] - 45s 40ms/step - loss: 371.3389 - mae: 10.0545 - val_loss: 367.9212 - val_mae: 9.9724\n",
      "Epoch 14/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 371.2003 - mae: 10.0518\n",
      "Epoch 14: val_loss improved from 367.61368 to 367.42044, saving model to ../models\\lstm_model.keras\n",
      "1110/1110 [==============================] - 45s 40ms/step - loss: 371.1908 - mae: 10.0517 - val_loss: 367.4204 - val_mae: 9.8532\n",
      "Epoch 15/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 370.8776 - mae: 10.0481\n",
      "Epoch 15: val_loss improved from 367.42044 to 367.33292, saving model to ../models\\lstm_model.keras\n",
      "1110/1110 [==============================] - 45s 40ms/step - loss: 370.8798 - mae: 10.0481 - val_loss: 367.3329 - val_mae: 9.9207\n",
      "Epoch 16/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 370.6474 - mae: 10.0403\n",
      "Epoch 16: val_loss improved from 367.33292 to 367.09677, saving model to ../models\\lstm_model.keras\n",
      "1110/1110 [==============================] - 45s 40ms/step - loss: 370.6528 - mae: 10.0403 - val_loss: 367.0968 - val_mae: 9.9725\n",
      "Epoch 17/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 370.4977 - mae: 10.0384\n",
      "Epoch 17: val_loss improved from 367.09677 to 366.47040, saving model to ../models\\lstm_model.keras\n",
      "1110/1110 [==============================] - 45s 40ms/step - loss: 370.5154 - mae: 10.0386 - val_loss: 366.4704 - val_mae: 9.8984\n",
      "Epoch 18/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 370.3959 - mae: 10.0356\n",
      "Epoch 18: val_loss improved from 366.47040 to 365.86539, saving model to ../models\\lstm_model.keras\n",
      "1110/1110 [==============================] - 45s 41ms/step - loss: 370.3911 - mae: 10.0356 - val_loss: 365.8654 - val_mae: 9.8742\n",
      "Epoch 19/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 370.1316 - mae: 10.0269\n",
      "Epoch 19: val_loss did not improve from 365.86539\n",
      "1110/1110 [==============================] - 45s 41ms/step - loss: 370.1316 - mae: 10.0269 - val_loss: 366.3896 - val_mae: 9.9473\n",
      "Epoch 20/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 369.8618 - mae: 10.0249\n",
      "Epoch 20: val_loss did not improve from 365.86539\n",
      "1110/1110 [==============================] - 45s 41ms/step - loss: 369.8560 - mae: 10.0248 - val_loss: 367.2119 - val_mae: 10.0117\n",
      "Epoch 21/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 369.6571 - mae: 10.0177\n",
      "Epoch 21: val_loss did not improve from 365.86539\n",
      "1110/1110 [==============================] - 45s 41ms/step - loss: 369.6609 - mae: 10.0177 - val_loss: 367.2299 - val_mae: 9.9569\n",
      "Epoch 22/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 369.4588 - mae: 10.0149\n",
      "Epoch 22: val_loss improved from 365.86539 to 365.69608, saving model to ../models\\lstm_model.keras\n",
      "1110/1110 [==============================] - 45s 40ms/step - loss: 369.4691 - mae: 10.0150 - val_loss: 365.6961 - val_mae: 9.9783\n",
      "Epoch 23/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 369.2550 - mae: 10.0123\n",
      "Epoch 23: val_loss did not improve from 365.69608\n",
      "1110/1110 [==============================] - 45s 41ms/step - loss: 369.2574 - mae: 10.0123 - val_loss: 365.9354 - val_mae: 10.0468\n",
      "Epoch 24/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 369.0624 - mae: 10.0077\n",
      "Epoch 24: val_loss did not improve from 365.69608\n",
      "1110/1110 [==============================] - 45s 41ms/step - loss: 369.0590 - mae: 10.0077 - val_loss: 366.6899 - val_mae: 10.1552\n",
      "Epoch 25/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 368.9948 - mae: 10.0040\n",
      "Epoch 25: val_loss improved from 365.69608 to 365.57724, saving model to ../models\\lstm_model.keras\n",
      "1110/1110 [==============================] - 45s 40ms/step - loss: 369.0044 - mae: 10.0041 - val_loss: 365.5772 - val_mae: 10.2068\n",
      "Epoch 26/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 368.7549 - mae: 10.0010\n",
      "Epoch 26: val_loss did not improve from 365.57724\n",
      "1110/1110 [==============================] - 44s 40ms/step - loss: 368.7689 - mae: 10.0012 - val_loss: 365.7110 - val_mae: 10.0959\n",
      "Epoch 27/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 443.6952 - mae: 10.5968\n",
      "Epoch 27: val_loss did not improve from 365.57724\n",
      "1110/1110 [==============================] - 44s 40ms/step - loss: 443.6828 - mae: 10.5967 - val_loss: 429.8193 - val_mae: 9.9249\n",
      "Epoch 28/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 428.2635 - mae: 10.6787\n",
      "Epoch 28: val_loss did not improve from 365.57724\n",
      "1110/1110 [==============================] - 44s 40ms/step - loss: 428.2765 - mae: 10.6788 - val_loss: 404.8988 - val_mae: 10.1952\n",
      "Epoch 29/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 406.2882 - mae: 10.4940\n",
      "Epoch 29: val_loss did not improve from 365.57724\n",
      "1110/1110 [==============================] - 44s 40ms/step - loss: 406.2839 - mae: 10.4940 - val_loss: 384.2252 - val_mae: 10.1399\n",
      "Epoch 30/50\n",
      "1109/1110 [============================>.] - ETA: 0s - loss: 390.0034 - mae: 10.3864\n",
      "Epoch 30: val_loss did not improve from 365.57724\n",
      "1110/1110 [==============================] - 45s 40ms/step - loss: 390.0063 - mae: 10.3864 - val_loss: 378.9536 - val_mae: 10.0825\n",
      "Epoch 30: early stopping\n",
      "3827/3827 [==============================] - 26s 7ms/step - loss: 374.2926 - mae: 10.0319\n",
      "Test Mean Absolute Error: 10.031909942626953\n",
      "3827/3827 [==============================] - 23s 6ms/step\n",
      "Mean Absolute Error for each column:\n",
      "[16.87730588  3.68045628  0.19225073  9.67583174 19.73372496]\n"
     ]
    }
   ],
   "source": [
    "# Defining callbacks\n",
    "checkpoint = ModelCheckpoint(\"./models/lstm_model.keras\", monitor='val_loss', save_best_only=True, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define LSTM model\n",
    "# Up to 2 layers of LSTM and number of hidden units were hand tuned to determine this as the optimum model\n",
    "lstm_model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], 1)),\n",
    "    LSTM(units=64, activation='relu', recurrent_dropout=0.2),\n",
    "    Dense(5)\n",
    "])\n",
    "\n",
    "# Use MSE for loss because we want to emphasize the \"wrongest\" guesses the most. MAE is an interpretable metric\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model w/ early stopping\n",
    "# Batch size is the average number of flights per day\n",
    "history = lstm_model.fit(X_train, Y_train, epochs=50, batch_size=265, validation_split=0.2, callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "\n",
    "loss, mae = lstm_model.evaluate(X_test, Y_test)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred = lstm_model.predict(X_test)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test, Y_pred, multioutput='raw_values')\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 445.2512 - mae: 11.9186\n",
      "Epoch 1: val_loss improved from inf to 378.54678, saving model to ../models\\bilstm_model.keras\n",
      "1110/1110 [==============================] - 86s 76ms/step - loss: 445.2512 - mae: 11.9186 - val_loss: 378.5468 - val_mae: 10.2095\n",
      "Epoch 2/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 381.6242 - mae: 10.4476\n",
      "Epoch 2: val_loss improved from 378.54678 to 374.90466, saving model to ../models\\bilstm_model.keras\n",
      "1110/1110 [==============================] - 84s 76ms/step - loss: 381.6242 - mae: 10.4476 - val_loss: 374.9047 - val_mae: 10.0181\n",
      "Epoch 3/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 378.9670 - mae: 10.3367\n",
      "Epoch 3: val_loss improved from 374.90466 to 374.22491, saving model to ../models\\bilstm_model.keras\n",
      "1110/1110 [==============================] - 84s 76ms/step - loss: 378.9670 - mae: 10.3367 - val_loss: 374.2249 - val_mae: 10.8192\n",
      "Epoch 4/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 377.2368 - mae: 10.2736\n",
      "Epoch 4: val_loss did not improve from 374.22491\n",
      "1110/1110 [==============================] - 85s 76ms/step - loss: 377.2368 - mae: 10.2736 - val_loss: 376.0947 - val_mae: 10.1358\n",
      "Epoch 5/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 376.1689 - mae: 10.2351\n",
      "Epoch 5: val_loss improved from 374.22491 to 371.45572, saving model to ../models\\bilstm_model.keras\n",
      "1110/1110 [==============================] - 84s 76ms/step - loss: 376.1689 - mae: 10.2351 - val_loss: 371.4557 - val_mae: 10.1757\n",
      "Epoch 6/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 375.2820 - mae: 10.2009\n",
      "Epoch 6: val_loss did not improve from 371.45572\n",
      "1110/1110 [==============================] - 84s 76ms/step - loss: 375.2820 - mae: 10.2009 - val_loss: 373.1274 - val_mae: 10.2453\n",
      "Epoch 7/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 374.5452 - mae: 10.1799\n",
      "Epoch 7: val_loss improved from 371.45572 to 369.03610, saving model to ../models\\bilstm_model.keras\n",
      "1110/1110 [==============================] - 84s 76ms/step - loss: 374.5452 - mae: 10.1799 - val_loss: 369.0361 - val_mae: 10.0069\n",
      "Epoch 8/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 374.2165 - mae: 10.1719\n",
      "Epoch 8: val_loss improved from 369.03610 to 368.71072, saving model to ../models\\bilstm_model.keras\n",
      "1110/1110 [==============================] - 84s 76ms/step - loss: 374.2165 - mae: 10.1719 - val_loss: 368.7107 - val_mae: 9.9117\n",
      "Epoch 9/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 373.5285 - mae: 10.1571\n",
      "Epoch 9: val_loss did not improve from 368.71072\n",
      "1110/1110 [==============================] - 84s 76ms/step - loss: 373.5285 - mae: 10.1571 - val_loss: 368.9948 - val_mae: 9.6788\n",
      "Epoch 10/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 373.1938 - mae: 10.1514\n",
      "Epoch 10: val_loss did not improve from 368.71072\n",
      "1110/1110 [==============================] - 83s 75ms/step - loss: 373.1938 - mae: 10.1514 - val_loss: 369.3811 - val_mae: 10.5541\n",
      "Epoch 11/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 372.8665 - mae: 10.1421\n",
      "Epoch 11: val_loss did not improve from 368.71072\n",
      "1110/1110 [==============================] - 83s 75ms/step - loss: 372.8665 - mae: 10.1421 - val_loss: 371.3800 - val_mae: 10.5910\n",
      "Epoch 12/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 372.3563 - mae: 10.1272\n",
      "Epoch 12: val_loss improved from 368.71072 to 367.94159, saving model to ../models\\bilstm_model.keras\n",
      "1110/1110 [==============================] - 83s 75ms/step - loss: 372.3563 - mae: 10.1272 - val_loss: 367.9416 - val_mae: 9.8475\n",
      "Epoch 13/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 372.2718 - mae: 10.1104\n",
      "Epoch 13: val_loss did not improve from 367.94159\n",
      "1110/1110 [==============================] - 83s 75ms/step - loss: 372.2718 - mae: 10.1104 - val_loss: 369.6774 - val_mae: 10.0274\n",
      "Epoch 14/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 372.1303 - mae: 10.1062\n",
      "Epoch 14: val_loss did not improve from 367.94159\n",
      "1110/1110 [==============================] - 83s 75ms/step - loss: 372.1303 - mae: 10.1062 - val_loss: 368.2519 - val_mae: 9.7954\n",
      "Epoch 15/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 371.8383 - mae: 10.0945\n",
      "Epoch 15: val_loss did not improve from 367.94159\n",
      "1110/1110 [==============================] - 83s 75ms/step - loss: 371.8383 - mae: 10.0945 - val_loss: 369.8653 - val_mae: 9.9821\n",
      "Epoch 16/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 371.6135 - mae: 10.0859\n",
      "Epoch 16: val_loss did not improve from 367.94159\n",
      "1110/1110 [==============================] - 83s 75ms/step - loss: 371.6135 - mae: 10.0859 - val_loss: 369.1604 - val_mae: 10.1838\n",
      "Epoch 17/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 371.3730 - mae: 10.0760\n",
      "Epoch 17: val_loss improved from 367.94159 to 366.54404, saving model to ../models\\bilstm_model.keras\n",
      "1110/1110 [==============================] - 83s 75ms/step - loss: 371.3730 - mae: 10.0760 - val_loss: 366.5440 - val_mae: 10.0425\n",
      "Epoch 18/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 371.2036 - mae: 10.0711\n",
      "Epoch 18: val_loss did not improve from 366.54404\n",
      "1110/1110 [==============================] - 83s 75ms/step - loss: 371.2036 - mae: 10.0711 - val_loss: 366.7523 - val_mae: 9.9826\n",
      "Epoch 19/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 371.0390 - mae: 10.0633\n",
      "Epoch 19: val_loss did not improve from 366.54404\n",
      "1110/1110 [==============================] - 83s 75ms/step - loss: 371.0390 - mae: 10.0633 - val_loss: 368.0311 - val_mae: 10.2258\n",
      "Epoch 20/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 370.8433 - mae: 10.0607\n",
      "Epoch 20: val_loss did not improve from 366.54404\n",
      "1110/1110 [==============================] - 83s 75ms/step - loss: 370.8433 - mae: 10.0607 - val_loss: 369.2222 - val_mae: 10.0977\n",
      "Epoch 21/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 370.6979 - mae: 10.0517\n",
      "Epoch 21: val_loss did not improve from 366.54404\n",
      "1110/1110 [==============================] - 83s 75ms/step - loss: 370.6979 - mae: 10.0517 - val_loss: 366.9940 - val_mae: 9.9167\n",
      "Epoch 22/50\n",
      "1110/1110 [==============================] - ETA: 0s - loss: 370.5795 - mae: 10.0557\n",
      "Epoch 22: val_loss did not improve from 366.54404\n",
      "1110/1110 [==============================] - 83s 75ms/step - loss: 370.5795 - mae: 10.0557 - val_loss: 367.4127 - val_mae: 10.1371\n",
      "Epoch 22: early stopping\n",
      "3827/3827 [==============================] - 46s 12ms/step - loss: 362.0834 - mae: 10.0498\n",
      "Test Mean Absolute Error: 10.049848556518555\n",
      "3827/3827 [==============================] - 43s 11ms/step\n",
      "Mean Absolute Error for each column:\n",
      "[16.621918    3.93657331  0.51713517  9.63077188 19.54279866]\n"
     ]
    }
   ],
   "source": [
    "# Defining callbacks\n",
    "checkpoint = ModelCheckpoint(\"./models/bilstm_model.keras\", monitor='val_loss', save_best_only=True, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define LSTM model\n",
    "# Up to 2 layers of LSTM and number of hidden units were hand tuned to determine this as the optimum model\n",
    "bilstm_model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], 1)),\n",
    "    Bidirectional(\n",
    "        LSTM(units=64, activation='relu', recurrent_dropout=0.2)\n",
    "    ),\n",
    "    Dense(5)\n",
    "])\n",
    "\n",
    "# Use MSE for loss because we want to emphasize the \"wrongest\" guesses the most. MAE is an interpretable metric\n",
    "bilstm_model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model w/ early stopping\n",
    "# Batch size is the average number of flights per day\n",
    "history = bilstm_model.fit(X_train, Y_train, epochs=50, batch_size=265, validation_split=0.2, callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "\n",
    "loss, mae = bilstm_model.evaluate(X_test, Y_test)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred = bilstm_model.predict(X_test)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test, Y_pred, multioutput='raw_values')\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN + LSTM Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "575/575 [==============================] - ETA: 0s - loss: 409.1539 - mae: 11.3165\n",
      "Epoch 1: val_loss improved from inf to 387.31973, saving model to ../models\\hybrid_model.keras\n",
      "575/575 [==============================] - 11s 18ms/step - loss: 409.1539 - mae: 11.3165 - val_loss: 387.3197 - val_mae: 9.8553\n",
      "Epoch 2/50\n",
      "574/575 [============================>.] - ETA: 0s - loss: 379.9135 - mae: 10.3883\n",
      "Epoch 2: val_loss improved from 387.31973 to 384.78098, saving model to ../models\\hybrid_model.keras\n",
      "575/575 [==============================] - 10s 17ms/step - loss: 379.9100 - mae: 10.3883 - val_loss: 384.7810 - val_mae: 10.0259\n",
      "Epoch 3/50\n",
      "575/575 [==============================] - ETA: 0s - loss: 378.9606 - mae: 10.3539\n",
      "Epoch 3: val_loss did not improve from 384.78098\n",
      "575/575 [==============================] - 10s 17ms/step - loss: 378.9606 - mae: 10.3539 - val_loss: 386.5429 - val_mae: 11.1886\n",
      "Epoch 4/50\n",
      "573/575 [============================>.] - ETA: 0s - loss: 377.7383 - mae: 10.3115\n",
      "Epoch 4: val_loss improved from 384.78098 to 377.85553, saving model to ../models\\hybrid_model.keras\n",
      "575/575 [==============================] - 10s 17ms/step - loss: 377.6660 - mae: 10.3110 - val_loss: 377.8555 - val_mae: 10.6097\n",
      "Epoch 5/50\n",
      "572/575 [============================>.] - ETA: 0s - loss: 1021.5801 - mae: 13.7509\n",
      "Epoch 5: val_loss did not improve from 377.85553\n",
      "575/575 [==============================] - 10s 17ms/step - loss: 1019.5332 - mae: 13.7451 - val_loss: 411.7723 - val_mae: 11.4080\n",
      "Epoch 6/50\n",
      "574/575 [============================>.] - ETA: 0s - loss: 413.1315 - mae: 11.5248\n",
      "Epoch 6: val_loss did not improve from 377.85553\n",
      "575/575 [==============================] - 10s 17ms/step - loss: 413.1331 - mae: 11.5248 - val_loss: 411.1298 - val_mae: 11.4512\n",
      "Epoch 7/50\n",
      "574/575 [============================>.] - ETA: 0s - loss: 412.7240 - mae: 11.5051\n",
      "Epoch 7: val_loss did not improve from 377.85553\n",
      "575/575 [==============================] - 10s 17ms/step - loss: 412.7240 - mae: 11.5050 - val_loss: 404.6491 - val_mae: 11.4560\n",
      "Epoch 8/50\n",
      "572/575 [============================>.] - ETA: 0s - loss: 411.6535 - mae: 11.4524\n",
      "Epoch 8: val_loss did not improve from 377.85553\n",
      "575/575 [==============================] - 10s 17ms/step - loss: 411.5759 - mae: 11.4516 - val_loss: 409.1925 - val_mae: 10.8293\n",
      "Epoch 9/50\n",
      "575/575 [==============================] - ETA: 0s - loss: 411.5976 - mae: 11.4499\n",
      "Epoch 9: val_loss did not improve from 377.85553\n",
      "575/575 [==============================] - 10s 17ms/step - loss: 411.5976 - mae: 11.4499 - val_loss: 421.4854 - val_mae: 10.5446\n",
      "Epoch 9: early stopping\n",
      "3827/3827 [==============================] - 17s 4ms/step - loss: 423.7206 - mae: 10.6719\n",
      "Test Mean Absolute Error: 10.67187786102295\n",
      "3827/3827 [==============================] - 13s 3ms/step\n",
      "Mean Absolute Error for each column:\n",
      "[19.45525618  3.15689241  0.8698911  11.05302003 18.82440156]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"./models/hybrid_model.keras\", monitor='val_loss', save_best_only=True, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define input layer\n",
    "input_layer = Input(shape=(X_train.shape[1], 1))\n",
    "\n",
    "# CNN model\n",
    "conv_layer = Conv1D(filters=32, kernel_size=3, activation='relu')(input_layer)\n",
    "maxpool_layer = MaxPooling1D(pool_size=2)(conv_layer)\n",
    "flatten_layer = Flatten()(maxpool_layer)\n",
    "dense_cnn = Dense(32, activation='relu')(flatten_layer)\n",
    "\n",
    "# BiLSTM model\n",
    "lstm_layer = LSTM(64, activation='relu')(input_layer)\n",
    "# lstm_layer2 = LSTM(32, activation='relu', return_sequences=False)(lstm_layer)\n",
    "dense_lstm = Dense(32, activation='relu')(lstm_layer)\n",
    "\n",
    "# Concatenate CNN and BiLSTM outputs\n",
    "concatenated = Concatenate()([dense_cnn, dense_lstm])\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(5)(concatenated)\n",
    "\n",
    "# Create the ensemble model\n",
    "hybrid_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "hybrid_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history = hybrid_model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=50,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint, early_stopping]\n",
    ")\n",
    "\n",
    "loss, mae = hybrid_model.evaluate(X_test, Y_test)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred = hybrid_model.predict(X_test)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test, Y_pred, multioutput='raw_values')\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
