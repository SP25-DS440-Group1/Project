{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Bidirectional, Conv1D, MaxPooling1D, Flatten, Concatenate, TimeDistributed, Reshape, Activation, Lambda, Layer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug = pd.read_csv(\"./data/X_train_lstm.csv\")\n",
    "X_test_aug = pd.read_csv(\"./data/X_test_lstm.csv\")\n",
    "\n",
    "Y_train_aug = pd.read_csv(\"./data/Y_train_lstm.csv\")\n",
    "Y_test_aug = pd.read_csv(\"./data/Y_test_lstm.csv\")\n",
    "\n",
    "# Only using total delay\n",
    "Y_train_aug = Y_train_aug[['ARR_DELAY']]\n",
    "Y_test_aug = Y_test_aug[['ARR_DELAY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"./data/old_data/X_train_lstm.csv\")\n",
    "X_test = pd.read_csv(\"./data/old_data/X_test_lstm.csv\")\n",
    "\n",
    "Y_train = pd.read_csv(\"./data/old_data/Y_train_lstm.csv\")\n",
    "Y_test = pd.read_csv(\"./data/old_data/Y_test_lstm.csv\")\n",
    "\n",
    "# Only using total delay\n",
    "Y_train = Y_train[['ARR_DELAY']]\n",
    "Y_test = Y_test[['ARR_DELAY']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1099.3448 - mae: 19.4959\n",
      "Epoch 1: val_loss improved from inf to 530.66980, saving model to ./models/lstm_model_aug.keras\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 14ms/step - loss: 1099.3519 - mae: 19.4958 - val_loss: 530.6698 - val_mae: 16.2410\n",
      "Epoch 2/50\n",
      "\u001b[1m10880/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1238.8148 - mae: 19.0135\n",
      "Epoch 2: val_loss improved from 530.66980 to 495.81778, saving model to ./models/lstm_model_aug.keras\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 13ms/step - loss: 1238.7844 - mae: 19.0134 - val_loss: 495.8178 - val_mae: 16.5178\n",
      "Epoch 3/50\n",
      "\u001b[1m10880/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5637.0264 - mae: 21.4801\n",
      "Epoch 3: val_loss improved from 495.81778 to 480.63504, saving model to ./models/lstm_model_aug.keras\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 14ms/step - loss: 5636.7104 - mae: 21.4801 - val_loss: 480.6350 - val_mae: 17.2940\n",
      "Epoch 4/50\n",
      "\u001b[1m10879/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1521.2450 - mae: 19.0101\n",
      "Epoch 4: val_loss did not improve from 480.63504\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 14ms/step - loss: 1521.3999 - mae: 19.0105 - val_loss: 553.0258 - val_mae: 16.3441\n",
      "Epoch 5/50\n",
      "\u001b[1m10882/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 13726.9590 - mae: 28.0864\n",
      "Epoch 5: val_loss did not improve from 480.63504\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 14ms/step - loss: 13726.9980 - mae: 28.0863 - val_loss: 512.9430 - val_mae: 16.3293\n",
      "Epoch 6/50\n",
      "\u001b[1m10881/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 126537.8750 - mae: 56.3508\n",
      "Epoch 6: val_loss did not improve from 480.63504\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 13ms/step - loss: 126526.6875 - mae: 56.3480 - val_loss: 501.4048 - val_mae: 16.8826\n",
      "Epoch 7/50\n",
      "\u001b[1m10880/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 211883.3750 - mae: 59.2583\n",
      "Epoch 7: val_loss did not improve from 480.63504\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 14ms/step - loss: 211886.8438 - mae: 59.2601 - val_loss: 504.5187 - val_mae: 16.8238\n",
      "Epoch 8/50\n",
      "\u001b[1m10881/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 39588.4297 - mae: 34.3436\n",
      "Epoch 8: val_loss did not improve from 480.63504\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 13ms/step - loss: 39591.9023 - mae: 34.3443 - val_loss: 499.7030 - val_mae: 16.4911\n",
      "Epoch 8: early stopping\n",
      "\u001b[1m37549/37549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2ms/step - loss: 519.2325 - mae: 16.8253\n",
      "Test Mean Absolute Error: 16.439659118652344\n",
      "\u001b[1m37549/37549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 2ms/step\n",
      "Mean Absolute Error for each column:\n",
      "16.439664840698242\n"
     ]
    }
   ],
   "source": [
    "# Defining callbacks\n",
    "checkpoint_aug = ModelCheckpoint(\"./models/lstm_model_aug.keras\", monitor='val_loss', save_best_only=True, verbose=1, save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define LSTM model\n",
    "# 2 layers of LSTM and number of hidden units were hand tuned\n",
    "lstm_model_aug = Sequential([\n",
    "    Input(shape=(X_train_aug.shape[1], 1)),\n",
    "    LSTM(units=64, activation='relu', recurrent_dropout=0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Use MSE for loss because we want to emphasize the \"wrongest\" guesses the most. MAE is an interpretable metric\n",
    "lstm_model_aug.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model w/ early stopping\n",
    "history_aug = lstm_model_aug.fit(X_train_aug, Y_train_aug, epochs=50, batch_size=265, validation_split=0.2, callbacks=[checkpoint_aug, early_stopping])\n",
    "\n",
    "\n",
    "loss, mae = lstm_model_aug.evaluate(X_test_aug, Y_test_aug)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred_aug = lstm_model_aug.predict(X_test_aug)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test_aug, Y_pred_aug)\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)\n",
    "\n",
    "pd.DataFrame(history_aug.history).to_csv(\"./models/results/lstm_test_aug.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m10881/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 447.4342 - mae: 15.9438\n",
      "Epoch 1: val_loss improved from inf to 452.75455, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 7ms/step - loss: 447.4272 - mae: 15.9437 - val_loss: 452.7545 - val_mae: 16.0383\n",
      "Epoch 2/50\n",
      "\u001b[1m10877/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 408.8911 - mae: 15.3260\n",
      "Epoch 2: val_loss improved from 452.75455 to 448.77139, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 6ms/step - loss: 408.8907 - mae: 15.3260 - val_loss: 448.7714 - val_mae: 16.1321\n",
      "Epoch 3/50\n",
      "\u001b[1m10878/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 406.0383 - mae: 15.2436\n",
      "Epoch 3: val_loss improved from 448.77139 to 447.69293, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 6ms/step - loss: 406.0380 - mae: 15.2436 - val_loss: 447.6929 - val_mae: 16.0712\n",
      "Epoch 4/50\n",
      "\u001b[1m10881/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 404.0822 - mae: 15.1866\n",
      "Epoch 4: val_loss improved from 447.69293 to 446.91568, saving model to ./models/lstm_model.keras\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 7ms/step - loss: 404.0822 - mae: 15.1866 - val_loss: 446.9157 - val_mae: 16.0342\n",
      "Epoch 5/50\n",
      "\u001b[1m10880/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 402.9167 - mae: 15.1533\n",
      "Epoch 5: val_loss did not improve from 446.91568\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 7ms/step - loss: 402.9165 - mae: 15.1533 - val_loss: 450.1833 - val_mae: 15.8631\n",
      "Epoch 6/50\n",
      "\u001b[1m10878/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 401.0998 - mae: 15.1053\n",
      "Epoch 6: val_loss did not improve from 446.91568\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 7ms/step - loss: 401.0996 - mae: 15.1053 - val_loss: 454.5027 - val_mae: 15.7165\n",
      "Epoch 7/50\n",
      "\u001b[1m10881/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 399.9431 - mae: 15.0776\n",
      "Epoch 7: val_loss did not improve from 446.91568\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 7ms/step - loss: 399.9430 - mae: 15.0776 - val_loss: 455.4205 - val_mae: 15.7061\n",
      "Epoch 8/50\n",
      "\u001b[1m10876/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 399.1539 - mae: 15.0586\n",
      "Epoch 8: val_loss did not improve from 446.91568\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 7ms/step - loss: 399.1538 - mae: 15.0586 - val_loss: 453.5523 - val_mae: 15.7240\n",
      "Epoch 9/50\n",
      "\u001b[1m10877/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 398.6909 - mae: 15.0468\n",
      "Epoch 9: val_loss did not improve from 446.91568\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 7ms/step - loss: 398.6908 - mae: 15.0468 - val_loss: 453.6461 - val_mae: 15.7377\n",
      "Epoch 9: early stopping\n",
      "\u001b[1m37549/37549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1ms/step - loss: 459.3535 - mae: 16.3588\n",
      "Test Mean Absolute Error: 15.809830665588379\n",
      "\u001b[1m37549/37549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1ms/step\n",
      "Mean Absolute Error for each column:\n",
      "15.809826850891113\n"
     ]
    }
   ],
   "source": [
    "# Defining callbacks\n",
    "checkpoint = ModelCheckpoint(\"./models/lstm_model.keras\", monitor='val_loss', save_best_only=True, verbose=1, save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define LSTM model\n",
    "# Up to 2 layers of LSTM and number of hidden units were hand tuned to determine this as the optimum model\n",
    "lstm_model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], 1)),\n",
    "    LSTM(units=64, activation='relu', recurrent_dropout=0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Use MSE for loss because we want to emphasize the \"wrongest\" guesses the most. MAE is an interpretable metric\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model w/ early stopping\n",
    "# Batch size is the average number of flights per day\n",
    "history = lstm_model.fit(X_train, Y_train, epochs=50, batch_size=265, validation_split=0.2, callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "\n",
    "loss, mae = lstm_model.evaluate(X_test, Y_test)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred = lstm_model.predict(X_test)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test, Y_pred)#, multioutput='raw_values')\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)\n",
    "\n",
    "pd.DataFrame(history.history).to_csv(\"./models/results/lstm_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1162.8428 - mae: 19.1464\n",
      "Epoch 1: val_loss improved from inf to 445.05237, saving model to ./models/bilstm_model_aug.keras\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 18ms/step - loss: 1162.7882 - mae: 19.1462 - val_loss: 445.0524 - val_mae: 15.9454\n",
      "Epoch 2/50\n",
      "\u001b[1m10881/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4869.1328 - mae: 20.0206\n",
      "Epoch 2: val_loss did not improve from 445.05237\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 17ms/step - loss: 4868.9946 - mae: 20.0206 - val_loss: 453.4020 - val_mae: 15.9955\n",
      "Epoch 3/50\n",
      "\u001b[1m10881/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1320.5222 - mae: 18.0874\n",
      "Epoch 3: val_loss did not improve from 445.05237\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 18ms/step - loss: 1320.4877 - mae: 18.0875 - val_loss: 469.0385 - val_mae: 15.7362\n",
      "Epoch 4/50\n",
      "\u001b[1m10881/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3096.0034 - mae: 19.3865\n",
      "Epoch 4: val_loss did not improve from 445.05237\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 18ms/step - loss: 3096.0520 - mae: 19.3867 - val_loss: 468.3565 - val_mae: 15.9774\n",
      "Epoch 5/50\n",
      "\u001b[1m10881/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5839.0371 - mae: 19.5363\n",
      "Epoch 5: val_loss did not improve from 445.05237\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 18ms/step - loss: 5839.1821 - mae: 19.5364 - val_loss: 509.2194 - val_mae: 15.4102\n",
      "Epoch 6/50\n",
      "\u001b[1m10880/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 985.9104 - mae: 17.8776\n",
      "Epoch 6: val_loss did not improve from 445.05237\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 18ms/step - loss: 986.1119 - mae: 17.8779 - val_loss: 445.8631 - val_mae: 15.7455\n",
      "Epoch 6: early stopping\n",
      "\u001b[1m37549/37549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 3ms/step - loss: 456.3234 - mae: 16.2628\n",
      "Test Mean Absolute Error: 15.816459655761719\n",
      "\u001b[1m37549/37549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 3ms/step\n",
      "Mean Absolute Error for each column:\n",
      "15.816433906555176\n"
     ]
    }
   ],
   "source": [
    "# Defining callbacks\n",
    "checkpoint_aug = ModelCheckpoint(\"./models/bilstm_model_aug.keras\", monitor='val_loss', save_best_only=True, verbose=1, save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define LSTM model\n",
    "# Up to 2 layers of LSTM and number of hidden units were hand tuned to determine this as the optimum model\n",
    "bilstm_model_aug = Sequential([\n",
    "    Input(shape=(X_train.shape[1], 1)),\n",
    "    Bidirectional(\n",
    "        LSTM(units=64, activation='relu', recurrent_dropout=0.2)\n",
    "    ),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Use MSE for loss because we want to emphasize the \"wrongest\" guesses the most. MAE is an interpretable metric\n",
    "bilstm_model_aug.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model w/ early stopping\n",
    "# Batch size is the average number of flights per day\n",
    "history_aug = bilstm_model_aug.fit(X_train_aug, Y_train_aug, epochs=50, batch_size=265, validation_split=0.2, callbacks=[checkpoint_aug, early_stopping])\n",
    "\n",
    "\n",
    "loss, mae = bilstm_model_aug.evaluate(X_test_aug, Y_test_aug)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred_aug = bilstm_model_aug.predict(X_test_aug)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test_aug, Y_pred_aug)#, multioutput='raw_values')\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)\n",
    "\n",
    "pd.DataFrame(history_aug.history).to_csv(\"./models/results/bilstm_test_aug.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m10878/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 570.9128 - mae: 17.2346\n",
      "Epoch 1: val_loss improved from inf to 448.05511, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 10ms/step - loss: 570.8456 - mae: 17.2339 - val_loss: 448.0551 - val_mae: 16.1207\n",
      "Epoch 2/50\n",
      "\u001b[1m10880/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 408.1736 - mae: 15.3034\n",
      "Epoch 2: val_loss improved from 448.05511 to 446.09976, saving model to ./models/bilstm_model.keras\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 10ms/step - loss: 408.1734 - mae: 15.3034 - val_loss: 446.0998 - val_mae: 16.1436\n",
      "Epoch 3/50\n",
      "\u001b[1m10879/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 409.7234 - mae: 15.3038\n",
      "Epoch 3: val_loss did not improve from 446.09976\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 10ms/step - loss: 409.7234 - mae: 15.3037 - val_loss: 447.4204 - val_mae: 15.9597\n",
      "Epoch 4/50\n",
      "\u001b[1m10878/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 411.1529 - mae: 15.3210\n",
      "Epoch 4: val_loss did not improve from 446.09976\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 10ms/step - loss: 411.1559 - mae: 15.3210 - val_loss: 448.2432 - val_mae: 16.0389\n",
      "Epoch 5/50\n",
      "\u001b[1m10879/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 454.1749 - mae: 15.5753\n",
      "Epoch 5: val_loss did not improve from 446.09976\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 10ms/step - loss: 454.1663 - mae: 15.5752 - val_loss: 446.2239 - val_mae: 16.0231\n",
      "Epoch 6/50\n",
      "\u001b[1m10879/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 414.4403 - mae: 15.2894\n",
      "Epoch 6: val_loss did not improve from 446.09976\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 10ms/step - loss: 414.4443 - mae: 15.2894 - val_loss: 486.9553 - val_mae: 15.4097\n",
      "Epoch 7/50\n",
      "\u001b[1m10880/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 413.3778 - mae: 15.3066\n",
      "Epoch 7: val_loss did not improve from 446.09976\n",
      "\u001b[1m10883/10883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 10ms/step - loss: 413.3774 - mae: 15.3066 - val_loss: 449.8404 - val_mae: 15.9505\n",
      "Epoch 7: early stopping\n",
      "\u001b[1m37549/37549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 2ms/step - loss: 461.4931 - mae: 16.4168\n",
      "Test Mean Absolute Error: 16.01453399658203\n",
      "\u001b[1m37549/37549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 2ms/step\n",
      "Mean Absolute Error for each column:\n",
      "16.014511108398438\n"
     ]
    }
   ],
   "source": [
    "# Defining callbacks\n",
    "checkpoint = ModelCheckpoint(\"./models/bilstm_model.keras\", monitor='val_loss', save_best_only=True, verbose=1, save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define LSTM model\n",
    "# Up to 2 layers of LSTM and number of hidden units were hand tuned to determine this as the optimum model\n",
    "bilstm_model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], 1)),\n",
    "    Bidirectional(\n",
    "        LSTM(units=64, activation='relu', recurrent_dropout=0.2)\n",
    "    ),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Use MSE for loss because we want to emphasize the \"wrongest\" guesses the most. MAE is an interpretable metric\n",
    "bilstm_model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model w/ early stopping\n",
    "# Batch size is the average number of flights per day\n",
    "history = bilstm_model.fit(X_train, Y_train, epochs=50, batch_size=265, validation_split=0.2, callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "\n",
    "loss, mae = bilstm_model.evaluate(X_test, Y_test)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred = bilstm_model.predict(X_test)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test, Y_pred)#, multioutput='raw_values')\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)\n",
    "\n",
    "pd.DataFrame(history.history).to_csv(\"./models/results/bilstm_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN + LSTM Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m5632/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 562.8344 - mae: 16.7462\n",
      "Epoch 1: val_loss improved from inf to 446.44348, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 22ms/step - loss: 562.7859 - mae: 16.7458 - val_loss: 446.4435 - val_mae: 16.1197\n",
      "Epoch 2/50\n",
      "\u001b[1m5631/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 402.5555 - mae: 15.1362\n",
      "Epoch 2: val_loss did not improve from 446.44348\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 22ms/step - loss: 402.5577 - mae: 15.1363 - val_loss: 450.8263 - val_mae: 15.8314\n",
      "Epoch 3/50\n",
      "\u001b[1m5632/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 403.4681 - mae: 15.1895\n",
      "Epoch 3: val_loss did not improve from 446.44348\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 22ms/step - loss: 403.4678 - mae: 15.1895 - val_loss: 447.8056 - val_mae: 15.7480\n",
      "Epoch 4/50\n",
      "\u001b[1m5631/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 401.0988 - mae: 15.1244\n",
      "Epoch 4: val_loss did not improve from 446.44348\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 22ms/step - loss: 401.0986 - mae: 15.1244 - val_loss: 447.2274 - val_mae: 15.7332\n",
      "Epoch 5/50\n",
      "\u001b[1m5632/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 399.6125 - mae: 15.0858\n",
      "Epoch 5: val_loss improved from 446.44348 to 446.04401, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 22ms/step - loss: 399.6126 - mae: 15.0858 - val_loss: 446.0440 - val_mae: 15.8766\n",
      "Epoch 6/50\n",
      "\u001b[1m5632/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 400.1434 - mae: 15.1033\n",
      "Epoch 6: val_loss did not improve from 446.04401\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 22ms/step - loss: 400.1433 - mae: 15.1033 - val_loss: 446.0879 - val_mae: 15.8890\n",
      "Epoch 7/50\n",
      "\u001b[1m5630/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 399.0440 - mae: 15.0780\n",
      "Epoch 7: val_loss improved from 446.04401 to 445.87732, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 22ms/step - loss: 399.0440 - mae: 15.0780 - val_loss: 445.8773 - val_mae: 15.8570\n",
      "Epoch 8/50\n",
      "\u001b[1m5631/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 398.6183 - mae: 15.0674\n",
      "Epoch 8: val_loss improved from 445.87732 to 445.54520, saving model to ./models/hybrid_model_aug.keras\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 22ms/step - loss: 398.6182 - mae: 15.0674 - val_loss: 445.5452 - val_mae: 15.8259\n",
      "Epoch 9/50\n",
      "\u001b[1m5631/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 398.3152 - mae: 15.0594\n",
      "Epoch 9: val_loss did not improve from 445.54520\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 22ms/step - loss: 398.3152 - mae: 15.0594 - val_loss: 446.7434 - val_mae: 15.7587\n",
      "Epoch 10/50\n",
      "\u001b[1m5632/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 397.9996 - mae: 15.0508\n",
      "Epoch 10: val_loss did not improve from 445.54520\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 22ms/step - loss: 397.9996 - mae: 15.0508 - val_loss: 446.3217 - val_mae: 15.7808\n",
      "Epoch 11/50\n",
      "\u001b[1m5632/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 397.7465 - mae: 15.0449\n",
      "Epoch 11: val_loss did not improve from 445.54520\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 22ms/step - loss: 397.7464 - mae: 15.0449 - val_loss: 447.0797 - val_mae: 15.7501\n",
      "Epoch 12/50\n",
      "\u001b[1m5630/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 397.4832 - mae: 15.0372\n",
      "Epoch 12: val_loss did not improve from 445.54520\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 22ms/step - loss: 397.4832 - mae: 15.0372 - val_loss: 447.0922 - val_mae: 15.7600\n",
      "Epoch 13/50\n",
      "\u001b[1m5632/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 397.2519 - mae: 15.0312\n",
      "Epoch 13: val_loss did not improve from 445.54520\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 21ms/step - loss: 397.2519 - mae: 15.0312 - val_loss: 447.2519 - val_mae: 15.7168\n",
      "Epoch 13: early stopping\n",
      "\u001b[1m37549/37549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 2ms/step - loss: 458.2950 - mae: 16.1250\n",
      "Test Mean Absolute Error: 15.690813064575195\n",
      "\u001b[1m37549/37549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 2ms/step\n",
      "Mean Absolute Error for each column:\n",
      "15.69083309173584\n"
     ]
    }
   ],
   "source": [
    "checkpoint_aug = ModelCheckpoint(\"./models/hybrid_model_aug.keras\", monitor='val_loss', save_best_only=True, verbose=1, save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define input layer\n",
    "input_layer_aug = Input(shape=(X_train_aug.shape[1], 1))\n",
    "\n",
    "# CNN model\n",
    "conv_layer_aug = Conv1D(filters=32, kernel_size=3, activation='relu')(input_layer_aug)\n",
    "maxpool_layer_aug = MaxPooling1D(pool_size=2)(conv_layer_aug)\n",
    "flatten_layer_aug = Flatten()(maxpool_layer_aug)\n",
    "dense_cnn_aug = Dense(32, activation='relu')(flatten_layer_aug)\n",
    "\n",
    "# BiLSTM model\n",
    "lstm_layer_aug = LSTM(64, activation='relu')(input_layer_aug)\n",
    "# lstm_layer2 = LSTM(32, activation='relu', return_sequences=False)(lstm_layer)\n",
    "dense_lstm_aug = Dense(32, activation='relu')(lstm_layer_aug)\n",
    "\n",
    "# Concatenate CNN and BiLSTM outputs\n",
    "concatenated_aug = Concatenate()([dense_cnn_aug, dense_lstm_aug])\n",
    "\n",
    "# Output layer\n",
    "output_layer_aug = Dense(1)(concatenated_aug)\n",
    "\n",
    "# Create the ensemble model\n",
    "hybrid_model_aug = Model(inputs=input_layer_aug, outputs=output_layer_aug)\n",
    "\n",
    "hybrid_model_aug.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history_aug = hybrid_model_aug.fit(\n",
    "    X_train_aug,\n",
    "    Y_train_aug,\n",
    "    epochs=50,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint_aug, early_stopping]\n",
    ")\n",
    "\n",
    "loss, mae = hybrid_model_aug.evaluate(X_test_aug, Y_test_aug)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred_aug = hybrid_model_aug.predict(X_test_aug)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test_aug, Y_pred_aug)#, multioutput='raw_values')\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)\n",
    "\n",
    "pd.DataFrame(history_aug.history).to_csv(\"./models/results/cnn_lstm_test_aug.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 432.5814 - mae: 15.9155\n",
      "Epoch 1: val_loss improved from inf to 454.90015, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 14ms/step - loss: 432.5790 - mae: 15.9155 - val_loss: 454.9001 - val_mae: 16.0124\n",
      "Epoch 2/50\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 410.9279 - mae: 15.4097\n",
      "Epoch 2: val_loss improved from 454.90015 to 450.82748, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 14ms/step - loss: 410.9276 - mae: 15.4096 - val_loss: 450.8275 - val_mae: 15.9566\n",
      "Epoch 3/50\n",
      "\u001b[1m5631/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 406.5569 - mae: 15.2816\n",
      "Epoch 3: val_loss improved from 450.82748 to 450.23837, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 13ms/step - loss: 406.5568 - mae: 15.2816 - val_loss: 450.2384 - val_mae: 15.9224\n",
      "Epoch 4/50\n",
      "\u001b[1m5632/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 404.7415 - mae: 15.2279\n",
      "Epoch 4: val_loss improved from 450.23837 to 449.89819, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 13ms/step - loss: 404.7412 - mae: 15.2279 - val_loss: 449.8982 - val_mae: 15.8468\n",
      "Epoch 5/50\n",
      "\u001b[1m5630/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 401.7538 - mae: 15.1389\n",
      "Epoch 5: val_loss improved from 449.89819 to 449.08850, saving model to ./models/hybrid_model.keras\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 14ms/step - loss: 401.7533 - mae: 15.1389 - val_loss: 449.0885 - val_mae: 15.7321\n",
      "Epoch 6/50\n",
      "\u001b[1m5629/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 400.1366 - mae: 15.0916\n",
      "Epoch 6: val_loss did not improve from 449.08850\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 13ms/step - loss: 400.1373 - mae: 15.0916 - val_loss: 452.4727 - val_mae: 15.7383\n",
      "Epoch 7/50\n",
      "\u001b[1m5631/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 399.4848 - mae: 15.0742\n",
      "Epoch 7: val_loss did not improve from 449.08850\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 13ms/step - loss: 399.4843 - mae: 15.0742 - val_loss: 455.3235 - val_mae: 15.5992\n",
      "Epoch 8/50\n",
      "\u001b[1m5630/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 395.9697 - mae: 14.9780\n",
      "Epoch 8: val_loss did not improve from 449.08850\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 13ms/step - loss: 395.9691 - mae: 14.9780 - val_loss: 452.5480 - val_mae: 15.5525\n",
      "Epoch 9/50\n",
      "\u001b[1m5630/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 393.3106 - mae: 14.9053\n",
      "Epoch 9: val_loss did not improve from 449.08850\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 13ms/step - loss: 393.3103 - mae: 14.9053 - val_loss: 451.6372 - val_mae: 15.5002\n",
      "Epoch 10/50\n",
      "\u001b[1m5629/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 392.0115 - mae: 14.8687\n",
      "Epoch 10: val_loss did not improve from 449.08850\n",
      "\u001b[1m5633/5633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 13ms/step - loss: 392.0114 - mae: 14.8687 - val_loss: 450.5518 - val_mae: 15.4839\n",
      "Epoch 10: early stopping\n",
      "\u001b[1m37549/37549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 1ms/step - loss: 455.1441 - mae: 16.0883\n",
      "Test Mean Absolute Error: 15.553506851196289\n",
      "\u001b[1m37549/37549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1ms/step\n",
      "Mean Absolute Error for each column:\n",
      "15.553540229797363\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"./models/hybrid_model.keras\", monitor='val_loss', save_best_only=True, verbose=1, save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Define input layer\n",
    "input_layer = Input(shape=(X_train.shape[1], 1))\n",
    "\n",
    "# CNN model\n",
    "conv_layer = Conv1D(filters=32, kernel_size=3, activation='relu')(input_layer)\n",
    "maxpool_layer = MaxPooling1D(pool_size=2)(conv_layer)\n",
    "flatten_layer = Flatten()(maxpool_layer)\n",
    "dense_cnn = Dense(32, activation='relu')(flatten_layer)\n",
    "\n",
    "# BiLSTM model\n",
    "lstm_layer = LSTM(64, activation='relu')(input_layer)\n",
    "# lstm_layer2 = LSTM(32, activation='relu', return_sequences=False)(lstm_layer)\n",
    "dense_lstm = Dense(32, activation='relu')(lstm_layer)\n",
    "\n",
    "# Concatenate CNN and BiLSTM outputs\n",
    "concatenated = Concatenate()([dense_cnn, dense_lstm])\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(1)(concatenated)\n",
    "\n",
    "# Create the ensemble model\n",
    "hybrid_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "hybrid_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history = hybrid_model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=50,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint, early_stopping]\n",
    ")\n",
    "\n",
    "loss, mae = hybrid_model.evaluate(X_test, Y_test)\n",
    "print(\"Test Mean Absolute Error:\", mae)\n",
    "\n",
    "Y_pred = hybrid_model.predict(X_test)\n",
    "\n",
    "mae_columns = mean_absolute_error(Y_test, Y_pred)#, multioutput='raw_values')\n",
    "print(\"Mean Absolute Error for each column:\")\n",
    "print(mae_columns)\n",
    "\n",
    "pd.DataFrame(history.history).to_csv(\"./models/results/cnn_lstm_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN + LSTM Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix shape: (19213, 19213)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────\n",
    "MASTER_COORD = \"./Datasets/T_MASTER_CORD.csv\"\n",
    "K_NEIGHBORS  = 5      # connect each airport to its 5 closest neighbors\n",
    "EARTH_R      = 6371.0 # km\n",
    "\n",
    "# ─── 1) load airport coordinates ────────────────────────────────\n",
    "mc = (\n",
    "    pd.read_csv(MASTER_COORD, dtype=str)\n",
    "      .set_index(\"AIRPORT_SEQ_ID\")[[\"LATITUDE\",\"LONGITUDE\"]]\n",
    "      .astype(float)\n",
    ")\n",
    "# list of IDs and arrays of lat/lon in radians\n",
    "ids  = mc.index.to_list()\n",
    "lats = np.deg2rad(mc[\"LATITUDE\"].values)\n",
    "lons = np.deg2rad(mc[\"LONGITUDE\"].values)\n",
    "N    = len(ids)\n",
    "\n",
    "# ─── 2) compute pairwise haversine distances ────────────────────\n",
    "# using the vectorized formula\n",
    "lat1 = lats[:, None]\n",
    "lat2 = lats[None, :]\n",
    "dlon = lons[None, :] - lons[:, None]\n",
    "\n",
    "dlat = lat2 - lat1\n",
    "a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "c = 2 * np.arcsin(np.sqrt(a))\n",
    "dist_km = EARTH_R * c  # shape (N, N)\n",
    "\n",
    "# ─── 3) build adjacency by k-nearest neighbors ───────────────────\n",
    "A = np.zeros((N, N), dtype=np.float32)\n",
    "\n",
    "for i in range(N):\n",
    "    # argsort returns i itself at position 0, so skip it\n",
    "    neighbors = np.argsort(dist_km[i])[1 : K_NEIGHBORS+1]\n",
    "    A[i, neighbors] = 1.0\n",
    "\n",
    "# symmetrize: if i→j or j→i, keep edge both ways\n",
    "A = np.maximum(A, A.T)\n",
    "\n",
    "# ─── 4) (Optional) row-normalize adjacency ──────────────────────\n",
    "# so each row sums to 1\n",
    "row_sums = A.sum(axis=1, keepdims=True)\n",
    "A_norm   = A / np.where(row_sums>0, row_sums, 1.0)\n",
    "\n",
    "# Now `A_norm` is your adjacency matrix to feed into the GNN\n",
    "print(\"Adjacency matrix shape:\", A_norm.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable(package=\"Custom\", name=\"SimpleGraphConv\")\n",
    "class SimpleGraphConv(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape = [(batch, N, F), (batch, N, N)]\n",
    "        F = input_shape[0][-1]\n",
    "        self.w = self.add_weight(\n",
    "            shape=(F, self.units),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            name=\"kernel\")\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X, A = inputs    # X: (batch, N, F), A: (batch, N, N)\n",
    "        return tf.matmul(A, tf.matmul(X, self.w))\n",
    "\n",
    "\n",
    "@register_keras_serializable(package=\"Custom\", name=\"GCNTimeDistributed\")\n",
    "class GCNTimeDistributed(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        # we’ll reuse this conv at each time slice\n",
    "        self.gcn = SimpleGraphConv(units)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape = [(batch, T, N, F), (batch, N, N)]\n",
    "        # we need to build the inner GCN on a flattened time‐slice shape:\n",
    "        # pretend batch' = None and time = 1 so shape = (None, N, F) & (None, N, N)\n",
    "        _, T, N, F = input_shape[0]\n",
    "        # call inner build:\n",
    "        self.gcn.build([(None, N, F), (None, N, N)])\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X, A = inputs\n",
    "        batch = tf.shape(X)[0]\n",
    "        T     = tf.shape(X)[1]\n",
    "        N     = tf.shape(X)[2]\n",
    "        F     = tf.shape(X)[3]\n",
    "\n",
    "        # 1) collapse time\n",
    "        Xr = tf.reshape(X, (batch * T, N, F))\n",
    "        # 2) tile adjacency\n",
    "        Aexp = tf.expand_dims(A, 1)               # (batch, 1, N, N)\n",
    "        Atil = tf.tile(Aexp, [1, T, 1, 1])         # (batch, T, N, N)\n",
    "        Ar   = tf.reshape(Atil, (batch * T, N, N))\n",
    "        # 3) apply GCN\n",
    "        Yr = self.gcn([Xr, Ar])                    # (batch*T, N, units)\n",
    "        # 4) restore time axis\n",
    "        return tf.reshape(Yr, (batch, T, N, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_mae\",\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build GNN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3604675, 1, 2, 5)\n"
     ]
    }
   ],
   "source": [
    "# 1) decide T, N, F\n",
    "T = 1       # one snapshot per flight\n",
    "N = 2       # origin + destination nodes\n",
    "F = 5       # the five weather vars you listed\n",
    "\n",
    "A_np = A_norm\n",
    "gcn_units = 32\n",
    "\n",
    "X_in = Input(shape=(T, N, F), name=\"node_features\")       # dynamic nodal time‐series\n",
    "A_in = Input(shape=(N, N),    name=\"adjacency_matrix\")    # can be broadcast\n",
    "\n",
    "weather_cols = X_train.columns[: N * F]  # first 10 columns\n",
    "assert len(weather_cols) == N * F\n",
    "\n",
    "# ─── C) extract and reshape ───────────────────────────────────────────────────\n",
    "weather_train = X_train[weather_cols].to_numpy()  # shape (n_samples, 10)\n",
    "weather_test  = X_test[ weather_cols].to_numpy()\n",
    "\n",
    "# reshape into (batch, T, N, F)\n",
    "X_train_gnn = weather_train.reshape(-1, T, N, F)\n",
    "X_test_gnn  = weather_test.reshape(-1, T, N, F)\n",
    "\n",
    "print(X_train_gnn.shape)  # → (952611, 1, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ebror\\OneDrive\\Documents\\GitHub\\Project\\.venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── 2a) GCN‐over‐time layer 1\n",
    "g = GCNTimeDistributed(gcn_units, name=\"time_gcn\")([X_in, A_in])\n",
    "\n",
    "# ─── (optional) 2nd GCN‐over‐time\n",
    "g = GCNTimeDistributed(gcn_units, name=\"time_gcn2\")([g, A_in])\n",
    "\n",
    "# ─── 2b) flatten per‐time‐step but keep T\n",
    "\n",
    "g_flat = TimeDistributed(Flatten(), name=\"flatten_nodes\")(g)  \n",
    "# shape = (batch, T, N * gcn_units)\n",
    "\n",
    "# ─── 2c) LSTM\n",
    "h = LSTM(64, name=\"temporal_lstm\")(g_flat)  # now g_flat is 3D\n",
    "\n",
    "# ─── 2d) final Dense (make sure units=Y_train.shape[1])\n",
    "Y_dim = Y_train.shape[1]\n",
    "out = Dense(Y_dim, name=\"output\")(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"GNN_LSTM_Hybrid\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"GNN_LSTM_Hybrid\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ node_features       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ adjacency_matrix    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_gcn            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │ node_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GCNTimeDistribute…</span> │                   │            │ adjacency_matrix… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_gcn2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ time_gcn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GCNTimeDistribute…</span> │                   │            │ adjacency_matrix… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_nodes       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_gcn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ temporal_lstm       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ flatten_nodes[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)              │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ temporal_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ node_features       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ adjacency_matrix    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_gcn            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │        \u001b[38;5;34m160\u001b[0m │ node_features[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mGCNTimeDistribute…\u001b[0m │                   │            │ adjacency_matrix… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_gcn2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │      \u001b[38;5;34m1,024\u001b[0m │ time_gcn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│ (\u001b[38;5;33mGCNTimeDistribute…\u001b[0m │                   │            │ adjacency_matrix… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_nodes       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ time_gcn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ temporal_lstm       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m33,024\u001b[0m │ flatten_nodes[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mLSTM\u001b[0m)              │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ temporal_lstm[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,273</span> (133.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,273\u001b[0m (133.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,273</span> (133.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,273\u001b[0m (133.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ─── 3) Compile, train & evaluate ────────────────────────────────────────────\n",
    "\n",
    "gnn_lstm = Model([X_in, A_in], out, name=\"GNN_LSTM_Hybrid\")\n",
    "gnn_lstm.summary()\n",
    "gnn_lstm.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=\"./models/gnn_lstm.keras\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    verbose=1, save_weights_only=False\n",
    ")\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_train shape: (3604675, 2, 2)\n",
      "Epoch 1/50\n",
      "\u001b[1m90111/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 448.6422 - mae: 16.1254\n",
      "Epoch 1: val_loss improved from inf to 486.62286, saving model to ./models/gnn_lstm.keras\n",
      "\u001b[1m90117/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 2ms/step - loss: 448.6415 - mae: 16.1254 - val_loss: 486.6229 - val_mae: 16.7691 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m90092/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 438.4288 - mae: 16.1840\n",
      "Epoch 2: val_loss did not improve from 486.62286\n",
      "\u001b[1m90117/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 2ms/step - loss: 438.4288 - mae: 16.1840 - val_loss: 486.6229 - val_mae: 16.7691 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m90104/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 438.4287 - mae: 16.1840\n",
      "Epoch 3: val_loss improved from 486.62286 to 486.62277, saving model to ./models/gnn_lstm.keras\n",
      "\u001b[1m90117/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 2ms/step - loss: 438.4287 - mae: 16.1840 - val_loss: 486.6228 - val_mae: 16.7691 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m90091/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 438.4287 - mae: 16.1840\n",
      "Epoch 4: val_loss did not improve from 486.62277\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m90117/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 2ms/step - loss: 438.4287 - mae: 16.1840 - val_loss: 486.6228 - val_mae: 16.7691 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m90106/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 438.4191 - mae: 16.1835\n",
      "Epoch 5: val_loss did not improve from 486.62277\n",
      "\u001b[1m90117/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 2ms/step - loss: 438.4190 - mae: 16.1835 - val_loss: 486.6923 - val_mae: 16.7654 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m90088/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 438.4193 - mae: 16.1834\n",
      "Epoch 6: val_loss did not improve from 486.62277\n",
      "\u001b[1m90117/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 2ms/step - loss: 438.4193 - mae: 16.1834 - val_loss: 486.6923 - val_mae: 16.7654 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m90095/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 438.4193 - mae: 16.1834\n",
      "Epoch 7: val_loss did not improve from 486.62277\n",
      "\u001b[1m90117/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 2ms/step - loss: 438.4193 - mae: 16.1834 - val_loss: 486.6922 - val_mae: 16.7654 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m90103/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 438.4193 - mae: 16.1834\n",
      "Epoch 8: val_loss did not improve from 486.62277\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m90117/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 2ms/step - loss: 438.4193 - mae: 16.1834 - val_loss: 486.6923 - val_mae: 16.7654 - learning_rate: 5.0000e-04\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Test loss, MAE: [483.0390625, 16.727325439453125]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# For origin ↔ dest only:\n",
    "A2 = np.array([[0., 1.],\n",
    "               [1., 0.]], dtype=np.float32)\n",
    "\n",
    "# Tile it for every sample in the train/test set:\n",
    "A_train = np.tile(A2[None], (X_train_gnn.shape[0], 1, 1))  # (952611,2,2)\n",
    "A_test  = np.tile(A2[None], (X_test_gnn.shape[0],  1, 1))  # (… likewise)\n",
    "\n",
    "print(\"A_train shape:\", A_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Fit just like you did your LSTM models:\n",
    "history = gnn_lstm.fit(\n",
    "    [X_train_gnn, A_train],\n",
    "    Y_train,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr],\n",
    ")\n",
    "\n",
    "test_results = gnn_lstm.evaluate(\n",
    "    [X_test_gnn, A_test],\n",
    "    Y_test,\n",
    "    verbose=0,\n",
    ")\n",
    "print(\"Test loss, MAE:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GNN training data → ./data/\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(history.history).to_csv(\"./models/results/gnn_test.csv\")\n",
    "np.save(\"./data/X_train_gnn.npy\", X_train_gnn)\n",
    "np.save(\"./data/A_train.npy\",     A_train)\n",
    "np.save(\"./data/Y_train.npy\",     Y_train)\n",
    "\n",
    "np.save(\"./data/X_test_gnn.npy\", X_test_gnn)\n",
    "np.save(\"./data/A_test.npy\",     A_test)\n",
    "np.save(\"./data/Y_test.npy\",     Y_test)\n",
    "\n",
    "print(\"Saved GNN training data → ./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3604675, 1, 2, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# decide T, N, F\n",
    "T = 1       # one snapshot per flight\n",
    "N = 2       # origin + destination nodes\n",
    "F = 5       # the five weather vars you listed\n",
    "\n",
    "\n",
    "A_np_aug = A_norm\n",
    "gcn_units = 32\n",
    "\n",
    "# Inputs\n",
    "X_in_aug = Input(shape=(T, N, F), name=\"node_features\")       # dynamic nodal time‐series\n",
    "A_in_aug = Input(shape=(N, N),    name=\"adjacency_matrix\")    # can be broadcast\n",
    "\n",
    "\n",
    "# pull out just the 10 weather columns from your 11\n",
    "weather_cols_aug = X_train_aug.columns[: N * F]  # first 10 columns\n",
    "assert len(weather_cols_aug) == N * F\n",
    "\n",
    "# extract and reshape\n",
    "weather_train_aug = X_train_aug[weather_cols_aug].to_numpy()  # shape (n_samples, 10)\n",
    "weather_test_aug  = X_test_aug[ weather_cols_aug].to_numpy()\n",
    "\n",
    "# reshape into (batch, T, N, F)\n",
    "X_train_gnn_aug = weather_train_aug.reshape(-1, T, N, F)\n",
    "X_test_gnn_aug  = weather_test_aug.reshape(-1, T, N, F)\n",
    "\n",
    "print(X_train_gnn_aug.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Graph-LSTM hybrid\n",
    "\n",
    "# GCN‐over‐time layer 1\n",
    "g_aug = GCNTimeDistributed(gcn_units, name=\"time_gcn\")([X_in_aug, A_in_aug])\n",
    "\n",
    "\n",
    "# flatten per‐time‐step but keep T\n",
    "g_flat_aug = TimeDistributed(Flatten(), name=\"flatten_nodes\")(g_aug)  \n",
    "\n",
    "# LSTM\n",
    "h_aug = LSTM(64, name=\"temporal_lstm\")(g_flat_aug)  # now g_flat is 3D\n",
    "\n",
    "# final Dense (make sure units=Y_train.shape[1])\n",
    "Y_dim = Y_train_aug.shape[1]\n",
    "out_aug = Dense(Y_dim, name=\"output\")(h_aug)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"GNN_LSTM_Hybrid\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"GNN_LSTM_Hybrid\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ node_features       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ adjacency_matrix    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_gcn            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │ node_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GCNTimeDistribute…</span> │                   │            │ adjacency_matrix… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_nodes       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_gcn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ temporal_lstm       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ flatten_nodes[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)              │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ temporal_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ node_features       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ adjacency_matrix    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_gcn            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │        \u001b[38;5;34m160\u001b[0m │ node_features[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mGCNTimeDistribute…\u001b[0m │                   │            │ adjacency_matrix… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_nodes       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ time_gcn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ temporal_lstm       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m33,024\u001b[0m │ flatten_nodes[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mLSTM\u001b[0m)              │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ temporal_lstm[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,249</span> (129.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m33,249\u001b[0m (129.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,249</span> (129.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m33,249\u001b[0m (129.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compile, train & evaluate\n",
    "\n",
    "# Assemble\n",
    "gnn_lstm_aug = Model(inputs=[X_in_aug, A_in_aug], outputs=out_aug, name=\"GNN_LSTM_Hybrid\")\n",
    "gnn_lstm_aug.summary()\n",
    "\n",
    "\n",
    "gnn_lstm_aug.compile(optimizer=\"adam\",loss=\"mse\",metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=\"./models/gnn_lstm_aug.keras\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    verbose=1, save_weights_only=False\n",
    ")\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_train_aug shape: (3604675, 2, 2)\n",
      "Epoch 1/50\n",
      "\u001b[1m90087/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 452.7466 - mae: 16.1462\n",
      "Epoch 1: val_loss improved from inf to 486.64337, saving model to ./models/gnn_lstm_aug.keras\n",
      "\u001b[1m90117/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 1ms/step - loss: 452.7426 - mae: 16.1462 - val_loss: 486.6434 - val_mae: 16.7682 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m90105/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 438.4277 - mae: 16.1839\n",
      "Epoch 2: val_loss did not improve from 486.64337\n",
      "\u001b[1m90117/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2ms/step - loss: 438.4277 - mae: 16.1839 - val_loss: 486.6434 - val_mae: 16.7682 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m90092/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 438.4277 - mae: 16.1839\n",
      "Epoch 3: val_loss did not improve from 486.64337\n",
      "\u001b[1m90117/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 1ms/step - loss: 438.4277 - mae: 16.1839 - val_loss: 486.6434 - val_mae: 16.7682 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m90084/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 438.4278 - mae: 16.1839\n",
      "Epoch 4: val_loss did not improve from 486.64337\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m90117/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 1ms/step - loss: 438.4277 - mae: 16.1839 - val_loss: 486.6434 - val_mae: 16.7682 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m90108/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 438.4179 - mae: 16.1835\n",
      "Epoch 5: val_loss did not improve from 486.64337\n",
      "\u001b[1m90117/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 1ms/step - loss: 438.4178 - mae: 16.1835 - val_loss: 486.6834 - val_mae: 16.7673 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m90090/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 438.4180 - mae: 16.1834\n",
      "Epoch 6: val_loss did not improve from 486.64337\n",
      "\u001b[1m90117/90117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 438.4179 - mae: 16.1834 - val_loss: 486.6834 - val_mae: 16.7673 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Aug Test loss, MAE: [483.055908203125, 16.725717544555664]\n"
     ]
    }
   ],
   "source": [
    "# For origin ↔ dest only:\n",
    "A2 = np.array([[0., 1.],\n",
    "               [1., 0.]], dtype=np.float32)\n",
    "\n",
    "# Tile it for every sample in the train/test set:\n",
    "A_train_aug = np.tile(A2[None], (X_train_gnn_aug.shape[0], 1, 1))  # (952611,2,2)\n",
    "A_test_aug  = np.tile(A2[None], (X_test_gnn_aug.shape[0],  1, 1))  # (… likewise)\n",
    "\n",
    "print(\"A_train_aug shape:\", A_train_aug.shape)\n",
    "\n",
    "\n",
    "# Fit just like you did your LSTM models:\n",
    "history_aug = gnn_lstm_aug.fit(\n",
    "    [X_train_gnn_aug, A_train_aug],\n",
    "    Y_train_aug,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr],\n",
    ")\n",
    "\n",
    "# Evaluate on test:\n",
    "test_results_aug = gnn_lstm_aug.evaluate(\n",
    "    [X_test_gnn_aug, A_test_aug],\n",
    "    Y_test_aug,\n",
    "    verbose=0,\n",
    ")\n",
    "print(\"Aug Test loss, MAE:\", test_results_aug)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GNN aug training data → ./data/\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(history_aug.history).to_csv(\"./models/results/gnn_test_aug.csv\")\n",
    "\n",
    "np.save(\"./data/X_train_gnn_aug.npy\", X_train_gnn_aug)\n",
    "np.save(\"./data/A_train_aug.npy\",     A_train_aug)\n",
    "np.save(\"./data/Y_train_aug.npy\",     Y_train_aug)\n",
    "\n",
    "np.save(\"./data/X_test_gnn_aug.npy\", X_test_gnn_aug)\n",
    "np.save(\"./data/A_test_aug.npy\",     A_test_aug)\n",
    "np.save(\"./data/Y_test_aug.npy\",     Y_test_aug)\n",
    "\n",
    "print(\"Saved GNN aug training data → ./data/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
